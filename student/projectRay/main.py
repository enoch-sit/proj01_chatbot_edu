from dotenv import load_dotenv

load_dotenv()
import os
import json
import uuid
import datetime
from typing import List, Dict, Any, Optional, TypedDict
from langchain_aws import ChatBedrock
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langgraph.graph import StateGraph, END
from langgraph.prebuilt import ToolNode
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage, ToolMessage
from tmdb_tools import (
    tmdb_search_movie,
    tmdb_discover_movies,
    tmdb_get_genres,
    tmdb_get_movie_videos,
)

# ====== é…ç½® ======
AWS_ACCESS_KEY = os.getenv("AWS_ACCESS_KEY_ID")
AWS_SECRET_KEY = os.getenv("AWS_SECRET_ACCESS_KEY")
AWS_REGION = os.getenv("AWS_REGION", "us-east-1")
BEDROCK_MODEL_ID = os.getenv("BEDROCK_MODEL_ID", "amazon.nova-lite-v1:0")
OPENROUTER_API_KEY = os.getenv("OPENROUTER_API_KEY")
TMDB_API_KEY = os.getenv("TMDB_API_KEY")
CHAT_HISTORY_DIR = "./movie_agent_chat_history"
os.makedirs(CHAT_HISTORY_DIR, exist_ok=True)

if not TMDB_API_KEY:
    raise ValueError("è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ TMDB_API_KEY")

# å¯ç”¨æ¨¡å‹é…ç½®
AVAILABLE_MODELS = {
    "1": {
        "name": "AWS Bedrock Nova Lite",
        "provider": "bedrock",
        "model_id": "amazon.nova-lite-v1:0",
    },
    "2": {"name": "GPT-4.1", "provider": "openrouter", "model_id": "openai/gpt-4.1"},
    # DeepSeek V2.5 åœ¨å·¥å…·è°ƒç”¨æ—¶ä¼šè¢«è·¯ç”±åˆ° Novita å¯¼è‡´é”™è¯¯,æš‚æ—¶ç¦ç”¨
    # "3": {"name": "DeepSeek Chat V2.5", "provider": "openrouter", "model_id": "deepseek/deepseek-chat"},
}

# ====== å·¥å…·æ³¨å†Œ ======
tools = [
    tmdb_search_movie,
    tmdb_discover_movies,
    tmdb_get_genres,
    tmdb_get_movie_videos,
]
tool_node = ToolNode(tools)

# Removed loading of external TMDB API markdown files and embedding into the system prompt

# ====== Agentæç¤ºè¯ ======
# è·å–å½“å‰æ—¥æœŸ,ä¾› LLM è®¡ç®—æ—¶é—´èŒƒå›´
current_date = datetime.date.today().strftime("%Y-%m-%d")

system_prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç”µå½±AIåŠ©æ‰‹,ä½¿ç”¨ TMDB API ä¸ºç”¨æˆ·æä¾›ç”µå½±æŸ¥è¯¢å’Œæ¨èæœåŠ¡ã€‚

â° **å½“å‰æ—¥æœŸ**: {current_date}
ğŸ’¡ å½“ç”¨æˆ·è¯¢é—®"æœ€è¿‘ä¸Šæ˜ "/"æ­£åœ¨ä¸Šæ˜ "/"æœ¬å‘¨æ–°ç‰‡"æ—¶,è¯·æ ¹æ®å½“å‰æ—¥æœŸè®¡ç®—æ—¶é—´èŒƒå›´,ä½¿ç”¨ primary_release_date_gte/lte å‚æ•°

# æ ¸å¿ƒè§„åˆ™
- æ•°æ®æ¥æº: æ‰€æœ‰ç”µå½±ä¿¡æ¯ï¼ˆæ ‡é¢˜ã€è¯„åˆ†ã€é¢„å‘Šç‰‡ç­‰ï¼‰å¿…é¡»æ¥è‡ª tmdb_search_movie/tmdb_discover_movies ç­‰å·¥å…·è¿”å›çš„ raw_data å­—æ®µ
- ç¦æ­¢è¡Œä¸º:
  1. ç¦æ­¢ç¼–é€ ç”µå½±ä¿¡æ¯ï¼ˆå¦‚è™šæ„è¯„åˆ†ã€ä¸Šæ˜ æ—¶é—´ï¼‰
  2. ç¦æ­¢è·³è¿‡å·¥å…·ç›´æ¥å›ç­”ï¼ˆå³ä½¿å·²çŸ¥ç”µå½±ä¿¡æ¯ï¼‰
  3. ç¦æ­¢è·³è¿‡æ€è€ƒæ­¥éª¤
  4. ç¦æ­¢ä¿®æ”¹å·¥å…·è¿”å›çš„æ ¸å¿ƒæ•°æ®ï¼ˆå¦‚è¯„åˆ†ã€é¢„å‘Šç‰‡é“¾æ¥ï¼‰

 
# TMDB å¯ç”¨å‚æ•°åˆ—è¡¨ï¼ˆå«æ ¼å¼ç¤ºä¾‹ï¼‰
- sort_by: **å¿…é¡»ä¿æŒé»˜è®¤ "popularity.desc"ï¼ˆæŒ‰çƒ­åº¦æ’åºï¼‰ã€‚å³ä½¿ç”¨æˆ·è¦æ±‚"é«˜åˆ†"æˆ–"è¯„åˆ†æœ€é«˜"ï¼Œä¹Ÿå¿…é¡»ç”¨çƒ­åº¦æ’åºï¼Œé€šè¿‡ vote_average.gte æ¥ç­›é€‰é«˜åˆ†ç”µå½±ï¼Œè€Œä¸æ˜¯æ”¹å˜æ’åºæ–¹å¼**
- with_genres: ç±»å‹IDï¼ˆçº¯æ•°å­—å­—ç¬¦ä¸²ï¼Œå®Œæ•´æ˜ å°„å¦‚ä¸‹ï¼‰
  - åŠ¨ä½œï¼š28 | å†’é™©ï¼š12 | åŠ¨ç”»ï¼š16 | å–œå‰§ï¼š35 | çŠ¯ç½ªï¼š80 | çºªå½•ç‰‡ï¼š99 | å‰§æƒ…ï¼š18 | å®¶åº­ï¼š10751 | å¥‡å¹»ï¼š14 | å†å²ï¼š36
  - ææ€–ï¼š27 | éŸ³ä¹ï¼š10402 | æ‚¬ç–‘ï¼š9648 | çˆ±æƒ…ï¼š10749 | ç§‘å¹»ï¼š878 | æƒŠæ‚šï¼š53 | æˆ˜äº‰ï¼š10752 | è¥¿éƒ¨ï¼š37 | ç”µè§†ç”µå½±ï¼š10770
- without_genres: æ’é™¤ç±»å‹ï¼ˆæ ¼å¼åŒ with_genresï¼‰
- vote_average.gte/lte: è¯„åˆ†èŒƒå›´ï¼ˆ0-10ï¼Œfloatï¼Œå¦‚ 7.5ï¼‰**ã€‚ç”¨è¿™ä¸ªå‚æ•°æ¥ç­›é€‰é«˜åˆ†ç”µå½±ï¼Œä¿æŒçƒ­åº¦æ’åº**
- vote_count.gte: æœ€ä½è¯„åˆ†äººæ•°ï¼ˆintï¼Œ**æ— é»˜è®¤å€¼ï¼Œä¸è¦è®¾ç½®æ­¤å‚æ•°é™¤éç”¨æˆ·æ˜ç¡®è¦æ±‚ï¼Œä¿æŒçµæ´»æ€§ä»¥æ”¯æŒå†·é—¨ç”µå½±æœç´¢**ï¼‰
- primary_release_year: é¦–æ˜ å¹´ä»½ï¼ˆintï¼Œå¦‚ 2023ï¼‰
- primary_release_date.gte/lte: æ—¥æœŸèŒƒå›´ï¼ˆå­—ç¬¦ä¸²ï¼Œå¦‚ "2024-01-01"ï¼‰
- with_origin_country: åˆ¶ä½œåœ°åŒºï¼ˆå›½å®¶ä»£ç ï¼Œå¦‚ CN=ä¸­å›½ã€JP=æ—¥æœ¬ã€KR=éŸ©å›½ã€TW=ä¸­å›½å°æ¹¾ã€HK=ä¸­å›½é¦™æ¸¯ï¼‰
- with_original_language: åŸå§‹è¯­è¨€ï¼ˆä»£ç ï¼Œå¦‚ zh=ä¸­æ–‡ã€en=è‹±æ–‡ã€ja=æ—¥æ–‡ã€ko=éŸ©æ–‡ï¼‰
- with_runtime.gte/lte: æ—¶é•¿èŒƒå›´ï¼ˆåˆ†é’Ÿï¼Œintï¼Œå¦‚ 90ï¼‰
- with_companies: åˆ¶ç‰‡å…¬å¸IDï¼ˆçº¯æ•°å­—å­—ç¬¦ä¸²ï¼Œå¤šå€¼ç”¨ | æˆ– , åˆ†éš”ï¼‰
- with_cast: æ¼”å‘˜IDï¼ˆçº¯æ•°å­—å­—ç¬¦ä¸²ï¼Œå¤šå€¼ç”¨ | æˆ– , åˆ†éš”ï¼‰
- with_release_type: å‘è¡ŒçŠ¶æ€ï¼ˆintï¼Œ3=é™¢çº¿ä¸Šæ˜ ä¸­ï¼Œ1=æœªä¸Šæ˜ ï¼Œ2=æ•°å­—å‘è¡Œï¼‰
- query: ç”µå½±æ ‡é¢˜ï¼ˆç²¾ç¡®æœç´¢ç”¨ï¼Œå­—ç¬¦ä¸²ï¼Œå¦‚ "æ— é—´é“"ï¼‰
- region: å‘è¡Œåœ°åŒºï¼ˆå›½å®¶ä»£ç ï¼Œå¦‚ CN/US/TWï¼‰
- page: é¡µç ï¼ˆintï¼Œé»˜è®¤ 1ï¼‰
- count: è¿”å›ç»“æœæ•°é‡ï¼ˆå·¥å…·å†…éƒ¨æˆªå–ï¼‰
- include_adult: æ˜¯å¦åŒ…å«æˆäººå†…å®¹ï¼ˆbooleanï¼Œé»˜è®¤ falseï¼‰

# å·¥å…·è°ƒç”¨æµç¨‹ï¼ˆæŒ‰åœºæ™¯é€‰æ‹©ï¼‰
1. ç²¾ç¡®æœç´¢ï¼ˆå·²çŸ¥ç”µå½±å/å¹´ä»½ï¼‰â†’ tmdb_search_movie
   - åœºæ™¯ç¤ºä¾‹ï¼š"æ‰¾ã€Šç›—æ¢¦ç©ºé—´ã€‹"ã€"2010å¹´çš„ã€Šinceptionã€‹"ã€"æœç´¢ã€Šæ— é—´é“ã€‹"
2. æ¡ä»¶æ¨è/ç­›é€‰ï¼ˆæŒ‰ç±»å‹/å¹´ä»½/åœ°åŒºç­‰ï¼‰â†’ tmdb_discover_movies
   - åœºæ™¯ç¤ºä¾‹ï¼š"Find popular action movies"ã€"æ¨èé«˜åˆ†ç§‘å¹»ç‰‡"ã€"2023å¹´ä¸­å›½å°æ¹¾çˆ±æƒ…ç‰‡"ã€"æ­£åœ¨ä¸Šæ˜ çš„å–œå‰§ç‰‡"
3. è·å–YouTubeé¢„å‘Šç‰‡ç›´æ¥é“¾æ¥ â†’ tmdb_get_movie_videos
   - åœºæ™¯ç¤ºä¾‹ï¼š"ã€Šç›—æ¢¦ç©ºé—´ã€‹çš„YouTubeé¢„å‘Šç‰‡"ã€"è¦èƒ½ç›´æ¥æ’­æ”¾çš„é¢„å‘Šç‰‡é“¾æ¥"
   - æ³¨æ„ï¼šæ¨èåˆ—è¡¨é»˜è®¤åŒ…å«TMDBé¢„å‘Šç‰‡é¡µé¢é“¾æ¥ï¼Œä»…ç”¨æˆ·æ˜ç¡®è¦YouTubeç›´æ¥é“¾æ¥æ—¶ä½¿ç”¨

# æ¨ç†å†…å®¹è¦æ±‚
**é‡è¦**ï¼šæ¯æ¬¡å›å¤å¿…é¡»ä¸¥æ ¼åˆ†ç¦»"æ¨ç†è¿‡ç¨‹"å’Œ"æœ€ç»ˆç­”æ¡ˆ"ï¼š

1. **æ¨ç†è¿‡ç¨‹**ï¼ˆæ€è€ƒéƒ¨åˆ†ï¼‰ï¼š
   - ç”¨ <thinking>...</thinking> æ ‡ç­¾åŒ…è£¹ä½ çš„æ‰€æœ‰æ¨ç†å’Œæ€è€ƒ
   - åœ¨æ ‡ç­¾å†…è‡ªç”±è¡¨è¾¾ä½ çš„æ€ç»´è¿‡ç¨‹ï¼ŒåŒ…æ‹¬åˆ†æã€åˆ¤æ–­ã€å†³ç­–ç­‰
   - è¿™éƒ¨åˆ†åªç»™ç”¨æˆ·çœ‹ä½ çš„æ€è€ƒï¼Œä¸åŒ…å«æœ€ç»ˆç­”æ¡ˆ

2. **æœ€ç»ˆç­”æ¡ˆ**ï¼ˆå›å¤éƒ¨åˆ†ï¼‰ï¼š
   - åœ¨ </thinking> æ ‡ç­¾ä¹‹åè¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
   - **å¿…é¡»**åŒ…å«ä¸‰ä¸ªéƒ¨åˆ†ï¼š
     a) **å¼€åœºç™½**ï¼ˆ1-2å¥è¯ï¼‰ï¼šè‡ªç„¶åœ°å›åº”ç”¨æˆ·éœ€æ±‚ï¼Œå¦‚"æ ¹æ®æ‚¨çš„è¦æ±‚..."ã€"ä¸ºæ‚¨æ‰¾åˆ°äº†..."ç­‰
     b) **æ ¼å¼åŒ–ç”µå½±åˆ—è¡¨**ï¼šä¸¥æ ¼æŒ‰ç…§"ç”µå½±è¾“å‡ºæ ¼å¼è¦æ±‚"å±•ç¤º
     c) **ç»“å°¾æ€»ç»“**ï¼ˆ1-2å¥è¯ï¼‰ï¼šè¡¥å……æ¨èç†ç”±ã€æä¾›å»ºè®®æˆ–å‹å¥½é—®å€™
   - è®©å›å¤åƒä¸€ä¸ªçœŸäººåŠ©æ‰‹åœ¨å¯¹è¯ï¼Œè€Œä¸æ˜¯æœºå™¨åœ¨è¾“å‡ºæ•°æ®

**ç¤ºä¾‹ç»“æ„**ï¼š
```
<thinking>
ç”¨æˆ·è¦æœç´¢ç§‘å¹»ç‰‡...æˆ‘éœ€è¦ç”¨ tmdb_discover_movies...å‚æ•°åº”è¯¥æ˜¯...
</thinking>

æ ¹æ®æ‚¨çš„éœ€æ±‚ï¼Œæˆ‘ä¸ºæ‚¨æ‰¾åˆ°äº†ä»¥ä¸‹å‡ éƒ¨é«˜åˆ†ç§‘å¹»ç‰‡ï¼š

1. **ç”µå½±æ ‡é¢˜** (å¹´ä»½)
   - è¯„åˆ†: 8.5/10
   ...

2. **ç”µå½±æ ‡é¢˜2** (å¹´ä»½)
   - è¯„åˆ†: 8.2/10
   ...

è¿™å‡ éƒ¨éƒ½æ˜¯å£ç¢‘å’Œçƒ­åº¦å…¼å…·çš„ä½³ä½œï¼Œç‰¹åˆ«æ¨èç¬¬ä¸€éƒ¨ï¼å¦‚æœ‰å…¶ä»–éœ€æ±‚è¯·éšæ—¶å‘Šè¯‰æˆ‘ï½
```

# æ ¼å¼è§„èŒƒ
## ç”µå½±è¾“å‡ºæ ¼å¼è¦æ±‚ï¼ˆå¿…é¡»è‡³å°‘åŒ…å«ä»¥ä¸‹ä¿¡æ¯ï¼Œå¦‚æœæœ‰å…¶ä»–æœ‰ç”¨çš„ä¿¡æ¯ä¹Ÿå¯ä»¥å±•ç¤ºå‡ºæ¥ï¼‰
1. **ç”µå½±æ ‡é¢˜** (å¹´ä»½)
   - è¯„åˆ†: X.X/10 (X,XXXäººè¯„ä»·)
   - çƒ­åº¦: XX.X
   - ç®€ä»‹: **[é‡è¦] å¦‚æœç®€ä»‹è¶…è¿‡80å­—ï¼Œè¯·è‡ªåŠ¨å‹ç¼©ä¸ºæ ¸å¿ƒå‰§æƒ…æ¦‚æ‹¬ï¼ˆä¿ç•™ä¸»è§’ã€ä¸»çº¿å†²çªã€æƒ…æ„ŸåŸºè°ƒï¼‰ï¼Œæ§åˆ¶åœ¨80å­—ä»¥å†…**
   ğŸ¬ é¢„å‘Šç‰‡: https://www.themoviedb.org/movie/{{{{movie_id}}}}/videos

2. **ç”µå½±æ ‡é¢˜2** (å¹´ä»½)
   - è¯„åˆ†: X.X/10
   - çƒ­åº¦: XX.X  
   - ç®€ä»‹: **[å‹ç¼©è§„åˆ™åŒä¸Šï¼Œ80å­—ä»¥å†…]**
   ğŸ¬ é¢„å‘Šç‰‡: æš‚æ— é¢„å‘Šç‰‡
   
"""

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder(variable_name="chat_history"),
    ]
)
# llm_with_tools å°†åœ¨ MovieAgent å†…éƒ¨åˆå§‹åŒ–


# ====== çŠ¶æ€å®šä¹‰ ======
class AgentState(TypedDict):
    memory: list
    thoughts: List[str]
    iterations: int
    max_iterations: int
    current_tool_calls: List[Dict[str, Any]]
    current_turn_id: str


# ====== Agentå…¥å£ ======
class MovieAgent:
    def __init__(
        self,
        user_id: str = "default_user",
        max_history_messages: int = 20,
        model_choice: str = "2",  # é»˜è®¤ä½¿ç”¨ GPT-4.1
    ):
        self.model_choice = model_choice
        self.current_model_info = AVAILABLE_MODELS[model_choice]
        self.llm = self._init_llm()
        self.llm_with_tools = prompt | self.llm.bind_tools(tools)
        self.graph = self._build_graph()
        self.memory = []  # ç”¨listç®¡ç†å†å²æ¶ˆæ¯
        self.max_history_messages = max_history_messages
        self.state: AgentState = {
            "memory": self.memory,
            "thoughts": [],
            "iterations": 0,
            "max_iterations": 5,  # å¢åŠ åˆ°5è½®ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢
            "current_tool_calls": [],
            "current_turn_id": "",
        }
        print(f"âœ… å·²åŠ è½½æ¨¡å‹: {self.current_model_info['name']}")

    def _init_llm(self):
        """åˆå§‹åŒ– LLM (æ”¯æŒ Bedrock å’Œ OpenRouter)"""
        provider = self.current_model_info["provider"]
        model_id = self.current_model_info["model_id"]

        try:
            if provider == "bedrock":
                import boto3

                client_kwargs = {"region_name": AWS_REGION}
                if AWS_ACCESS_KEY and AWS_SECRET_KEY:
                    client_kwargs.update(
                        {
                            "aws_access_key_id": AWS_ACCESS_KEY,
                            "aws_secret_access_key": AWS_SECRET_KEY,
                        }
                    )
                bedrock_client = boto3.client("bedrock-runtime", **client_kwargs)
                return ChatBedrock(
                    client=bedrock_client,
                    model_id=model_id,
                    model_kwargs={"temperature": 0.2, "max_tokens": 2048},
                )
            elif provider == "openrouter":
                from langchain_openai import ChatOpenAI

                if not OPENROUTER_API_KEY:
                    raise ValueError("è¯·å…ˆè®¾ç½®ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY")

                # é…ç½® headers,ä¼˜å…ˆä½¿ç”¨å®˜æ–¹æä¾›å•†,é¿å…ç¬¬ä¸‰æ–¹æä¾›å•†ä½™é¢é—®é¢˜
                headers = {
                    "HTTP-Referer": "https://github.com/your-repo",
                    "X-Title": "Movie Agent",
                }

                # å¯¹äº OpenAI æ¨¡å‹,è¦æ±‚ä½¿ç”¨ OpenAI å®˜æ–¹
                if "openai/" in model_id or "gpt" in model_id.lower():
                    headers["X-OpenRouter-Provider-Order"] = "OpenAI"
                # å¯¹äº DeepSeek æ¨¡å‹,æ’é™¤ Novita æä¾›å•†
                elif "deepseek/" in model_id:
                    # ä½¿ç”¨æ’é™¤åˆ—è¡¨,ä¸ä½¿ç”¨ Novita
                    headers["X-OpenRouter-Provider-Preferences"] = (
                        '{"ignore": ["Novita"]}'
                    )

                return ChatOpenAI(
                    model=model_id,
                    openai_api_key=OPENROUTER_API_KEY,
                    openai_api_base="https://openrouter.ai/api/v1",
                    temperature=0.2,
                    max_tokens=2048,
                    default_headers=headers,
                )
        except Exception as e:
            print(f"âŒ LLM åˆå§‹åŒ–å¤±è´¥: {str(e)}")
            raise

    def switch_model(self, model_choice: str):
        """è¿è¡Œæ—¶åˆ‡æ¢æ¨¡å‹"""
        if model_choice not in AVAILABLE_MODELS:
            print(f"âŒ æ— æ•ˆçš„æ¨¡å‹é€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-{len(AVAILABLE_MODELS)}")
            return False

        print(f"\nğŸ”„ æ­£åœ¨åˆ‡æ¢æ¨¡å‹...")
        old_model = self.current_model_info["name"]

        self.model_choice = model_choice
        self.current_model_info = AVAILABLE_MODELS[model_choice]
        self.llm = self._init_llm()
        self.llm_with_tools = prompt | self.llm.bind_tools(tools)
        self.graph = self._build_graph()

        # åˆ‡æ¢æ¨¡å‹åæ¸…ç©ºå†å²
        self.reset()

        print(f"âœ… æ¨¡å‹åˆ‡æ¢æˆåŠŸ!")
        print(f"   æ—§æ¨¡å‹: {old_model}")
        print(f"   æ–°æ¨¡å‹: {self.current_model_info['name']}")
        print(f"   (å¯¹è¯å†å²å·²æ¸…ç©º)\n")
        return True

    def _build_graph(self):
        """æ„å»º LangGraph æµç¨‹å›¾"""
        graph = StateGraph(AgentState)
        graph.add_node("llm", self._call_llm)
        graph.add_node("tools", self._process_tool_response)
        graph.set_entry_point("llm")
        graph.add_conditional_edges(
            "llm", self._should_continue, {"tools": "tools", END: END}
        )
        graph.add_edge("tools", "llm")
        return graph.compile()

    def _should_continue(self, state: AgentState) -> str:
        """åˆ¤æ–­æ˜¯å¦ç»§ç»­æ‰§è¡Œ (å¤åˆ¶åŸ should_continue é€»è¾‘)"""
        # 1. è¿­ä»£æ¬¡æ•°ä¸Šé™
        if state["iterations"] >= state["max_iterations"]:
            state["thoughts"].append(
                f"Iteration {state['iterations']} (max reached), terminate"
            )
            return END

        # 2. æ£€æŸ¥å·¥å…·è¿”å›
        last_tool_msg = None
        for msg in reversed(state["memory"]):
            if isinstance(msg, ToolMessage) and getattr(msg, "tool_call_id", None):
                last_tool_msg = msg
                break

        if last_tool_msg:
            try:
                tool_result_wrapper = json.loads(last_tool_msg.content)
                if (
                    isinstance(tool_result_wrapper, dict)
                    and "toolResult" in tool_result_wrapper
                ):
                    tool_data = json.loads(tool_result_wrapper["toolResult"]["content"])
                else:
                    tool_data = tool_result_wrapper
                # å·¥å…·é”™è¯¯ï¼šå…è®¸é‡è¯•
                if tool_data.get("status") == "error":
                    if state["iterations"] < state["max_iterations"] - 1:
                        state["thoughts"].append(
                            f"Tool error: {tool_data['msg'][:30]}, retry"
                        )
                        return "tools"
                    else:
                        state["thoughts"].append(f"Tool error (retried), terminate")
                        return END
                # ç©ºæ•°æ®ï¼šè®©LLMç”Ÿæˆå‹å¥½å›å¤
                if not tool_data.get("raw_data"):
                    state["thoughts"].append(
                        "No raw data returned, but let LLM generate friendly response"
                    )
            except Exception as e:
                if state["iterations"] < state["max_iterations"] - 1:
                    state["thoughts"].append("Tool response format error, retry")
                    return "tools"
                else:
                    state["thoughts"].append("Format error (retried), terminate")
                    return END

        # 3. å·²æœ‰æœ€ç»ˆå›å¤
        last_msg = state["memory"][-1] if state["memory"] else None
        if isinstance(last_msg, AIMessage) and not getattr(
            last_msg, "tool_calls", None
        ):
            state["thoughts"].append("Final response generated, terminate")
            return END

        return "tools"

    def _call_llm(self, state: AgentState) -> AgentState:
        """è°ƒç”¨ LLM (å¤åˆ¶åŸ call_llm é€»è¾‘)"""
        chat_history = state["memory"].copy()
        thoughts = state["thoughts"].copy()
        iterations = state["iterations"] + 1

        # è°ƒç”¨LLM
        response = self.llm_with_tools.invoke({"chat_history": chat_history})
        state["memory"].append(response)

        # ğŸ“ æ•è· LLM çš„æ¨ç†å†…å®¹ï¼ˆä» <thinking> æ ‡ç­¾ä¸­æå–ï¼‰
        import re

        llm_reasoning = ""
        full_content = ""

        if response.content:
            if isinstance(response.content, str):
                full_content = response.content.strip()
            elif isinstance(response.content, list):
                # æŸäº›æ¨¡å‹è¿”å›åˆ—è¡¨æ ¼å¼
                text_parts = []
                for item in response.content:
                    if isinstance(item, dict) and item.get("type") == "text":
                        text_parts.append(str(item.get("text", "")))
                    elif isinstance(item, str):
                        text_parts.append(item)
                full_content = " ".join(text_parts).strip()

        # æå– <thinking> æ ‡ç­¾å†…çš„æ¨ç†å†…å®¹
        if full_content:
            thinking_match = re.search(
                r"<thinking>(.*?)</thinking>", full_content, re.DOTALL | re.IGNORECASE
            )
            if thinking_match:
                llm_reasoning = thinking_match.group(1).strip()
            else:
                # å¦‚æœæ²¡æœ‰ <thinking> æ ‡ç­¾ï¼Œå°è¯•æå–"æ¨ç†è¿‡ç¨‹ï¼š"ä¹‹åã€"æœ€ç»ˆç­”æ¡ˆï¼š"ä¹‹å‰çš„å†…å®¹
                reasoning_match = re.search(
                    r"æ¨ç†è¿‡ç¨‹ï¼š(.*?)(?:æœ€ç»ˆç­”æ¡ˆï¼š|$)", full_content, re.DOTALL
                )
                if reasoning_match:
                    llm_reasoning = reasoning_match.group(1).strip()
                else:
                    # å…œåº•ï¼šå¦‚æœæœ‰ tool_callsï¼Œè¯´æ˜è¿™æ˜¯æ¨ç†é˜¶æ®µï¼Œæ•´ä¸ªå†…å®¹éƒ½æ˜¯æ¨ç†
                    if getattr(response, "tool_calls", None):
                        llm_reasoning = full_content

        new_tool_calls = getattr(response, "tool_calls", [])
        valid_calls = []
        validation_warnings = []

        for call in new_tool_calls:
            params = call.get("args") or call.get("parameters") or {}
            tool_name = call["name"]

            # ğŸ” å‚æ•°è‡ªæ£€ - æ£€æµ‹å¸¸è§é”™è¯¯
            if tool_name == "tmdb_discover_movies" and "genre_id" in params:
                genre_val = str(params["genre_id"])
                # æ£€æµ‹ HTML è½¬ä¹‰
                if "&quot;" in genre_val or "&#34;" in genre_val or "\\" in genre_val:
                    validation_warnings.append(
                        f"âš ï¸ å‚æ•°è­¦å‘Š: genre_id åŒ…å« HTML è½¬ä¹‰æˆ–ç‰¹æ®Šå­—ç¬¦ '{genre_val}' - å·²è‡ªåŠ¨æ¸…ç†ä¸ºçº¯æ•°å­—"
                    )

            valid_calls.append(call)

        state["current_tool_calls"] = valid_calls

        # è®°å½•è¿­ä»£ä¿¡æ¯å’Œæ¨ç†å†…å®¹
        thoughts.append(
            f"Iteration {iterations}: Tool calls â†’ {[call['name'] for call in valid_calls]}"
        )

        # ğŸ“ è®°å½• LLM çš„æ¨ç†æ–‡æœ¬ï¼ˆå¦‚æœæœ‰ï¼‰
        if llm_reasoning:
            thoughts.append(f"LLM reasoning â†’ {llm_reasoning}")

        # è®°å½•éªŒè¯è­¦å‘Š
        if validation_warnings:
            for warning in validation_warnings:
                thoughts.append(warning)

        return {
            "memory": state["memory"],
            "thoughts": thoughts,
            "iterations": iterations,
            "max_iterations": state["max_iterations"],
            "current_tool_calls": state["current_tool_calls"],
            "current_turn_id": state["current_turn_id"],
        }

    def _process_tool_response(self, state: AgentState) -> AgentState:
        """å¤„ç†å·¥å…·å“åº” (å¤åˆ¶åŸ process_tool_response é€»è¾‘)"""
        thoughts = state["thoughts"].copy()
        iterations = state["iterations"]

        if not state.get("current_tool_calls", []):
            thoughts.append("No valid tool calls, skip execution")
            return state

        # å‚æ•°æ¸…æ´—å‡½æ•°
        def clean_params(params: dict) -> dict:
            import html

            cleaned = {}
            for k, v in params.items():
                if isinstance(v, str):
                    v = html.unescape(v)
                    if v.startswith('"') and v.endswith('"') and len(v) > 2:
                        v = v[1:-1]
                    if v.startswith("'") and v.endswith("'") and len(v) > 2:
                        v = v[1:-1]
                cleaned[k] = v
            return cleaned

        for tool_call in state.get("current_tool_calls", []):
            tool_name = tool_call["name"]
            tool_input_raw = tool_call.get("args") or tool_call.get("parameters") or {}
            tool_call_id = tool_call["id"]

            tool_input = clean_params(tool_input_raw)
            # æ·»åŠ  current_turn_id ç”¨äºåç»­è¯†åˆ«
            tool_input_with_id = tool_input.copy()
            tool_input_with_id["current_turn_id"] = state["current_turn_id"]

            tool_found = False
            for tool in tools:
                if tool.name == tool_name:
                    tool_found = True
                    try:
                        result = tool.invoke(tool_input)
                        thoughts.append(
                            f"Tool {tool_name} success â†’ Params: {tool_input}"
                        )

                        try:
                            result_data = (
                                json.loads(result)
                                if isinstance(result, str)
                                else result
                            )
                            if isinstance(result_data, dict) and not result_data.get(
                                "raw_data"
                            ):
                                thoughts.append(
                                    "Tool returned empty data, guidance for LLM will be handled in prompt"
                                )
                        except:
                            pass

                        formatted_result = {
                            "toolResult": {"content": result, "toolUseId": tool_call_id}
                        }
                    except Exception as e:
                        error_msg = f"Execution error: {str(e)}"
                        formatted_result = {
                            "toolResult": {
                                "content": json.dumps(
                                    {"status": "error", "msg": error_msg},
                                    ensure_ascii=False,
                                ),
                                "toolUseId": tool_call_id,
                                "error": error_msg,
                            }
                        }
                        thoughts.append(f"Tool {tool_name} failed â†’ {str(e)}")
                    state["memory"].append(
                        ToolMessage(
                            content=json.dumps(formatted_result, ensure_ascii=False),
                            tool_call_id=tool_call_id,
                            name=tool_name,
                            tool_input=tool_input_with_id,
                        )
                    )
                    break
            if not tool_found:
                error_msg = f"å·¥å…·åé”™è¯¯ï¼š{tool_name} ä¸æ˜¯æœ‰æ•ˆå·¥å…·"
                formatted_result = {
                    "toolResult": {
                        "content": json.dumps(
                            {"status": "error", "msg": error_msg}, ensure_ascii=False
                        ),
                        "toolUseId": tool_call_id,
                        "error": error_msg,
                    }
                }
                state["memory"].append(
                    ToolMessage(
                        content=json.dumps(formatted_result, ensure_ascii=False),
                        tool_call_id=tool_call_id,
                        name=tool_name,
                        tool_input=tool_input_with_id,
                    )
                )

        state["current_tool_calls"] = []
        return {
            "memory": state["memory"],
            "thoughts": thoughts,
            "iterations": iterations,
            "max_iterations": state["max_iterations"],
            "current_tool_calls": [],
            "current_turn_id": state["current_turn_id"],
        }

    def _trim_history(self):
        """ä¿®å‰ªå†å²æ¶ˆæ¯,ä¿æŒåœ¨é™åˆ¶èŒƒå›´å†…ï¼Œä¿è¯Bedrockæ ¼å¼"""
        if len(self.memory) > self.max_history_messages:
            # åªä¿ç•™æœ€åmax_history_messagesæ¡
            self.memory = self.memory[-self.max_history_messages :]
        # å¿…é¡»ä»¥HumanMessageå¼€å¤´
        if self.memory and not isinstance(self.memory[0], HumanMessage):
            # æ‰¾åˆ°ç¬¬ä¸€ä¸ªHumanMessage
            for i, msg in enumerate(self.memory):
                if isinstance(msg, HumanMessage):
                    self.memory = self.memory[i:]
                    break
        # AIMessage(tool_calls)åå¿…é¡»ç´§è·ŸToolMessage
        trimmed = []
        i = 0
        while i < len(self.memory):
            msg = self.memory[i]
            trimmed.append(msg)
            if isinstance(msg, AIMessage) and getattr(msg, "tool_calls", None):
                # ä¸‹ä¸€ä¸ªå¿…é¡»æ˜¯ToolMessage
                if i + 1 < len(self.memory) and not isinstance(
                    self.memory[i + 1], ToolMessage
                ):
                    # è·³è¿‡éToolMessageç›´åˆ°é‡åˆ°ToolMessage
                    j = i + 1
                    while j < len(self.memory) and not isinstance(
                        self.memory[j], ToolMessage
                    ):
                        j += 1
                    if j < len(self.memory):
                        trimmed.append(self.memory[j])
                        i = j
            i += 1
        self.memory = trimmed

    def reset(self):
        self.memory = []
        self.state = {
            "memory": self.memory,
            "thoughts": [],
            "iterations": 0,
            "max_iterations": 5,  # å¢åŠ åˆ°5è½®ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢
            "current_tool_calls": [],
            "current_turn_id": "",
        }

    def chat(self, user_input: str) -> Dict[str, Any]:
        self.state["thoughts"] = []  # æ¯è½®é‡ç½®æ€è€ƒè¿‡ç¨‹
        current_turn_id = str(uuid.uuid4())[:8]
        self.state["current_turn_id"] = current_turn_id

        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯
        self.memory.append(HumanMessage(content=user_input))

        # ä¿®å‰ªå†å²æ¶ˆæ¯ï¼ˆåœ¨æ·»åŠ ç”¨æˆ·æ¶ˆæ¯ä¹‹åï¼‰
        self._trim_history()

        # åŒæ­¥stateå’Œmemory
        self.state["memory"] = self.memory
        self.state["iterations"] = 0
        self.state["current_tool_calls"] = []

        # æ‰§è¡Œ LLM è°ƒç”¨,æ•è·å¯èƒ½çš„å†…å®¹è¿‡æ»¤é”™è¯¯
        try:
            final_state = self.graph.invoke(self.state)
            self.state = final_state
            self.memory = final_state["memory"]  # åŒæ­¥å›æ¥
        except Exception as e:
            error_msg = str(e)
            # æ£€æŸ¥æ˜¯å¦æ˜¯å†…å®¹è¿‡æ»¤é”™è¯¯
            if (
                "content_filter" in error_msg
                or "content management policy" in error_msg.lower()
            ):
                return {
                    "answer": "æŠ±æ­‰ï¼Œæ‚¨çš„è¯·æ±‚è§¦å‘äº† AI å†…å®¹å®¡æ ¸ç­–ç•¥ã€‚\n\nğŸ’¡ å»ºè®®ï¼š\n1. å°è¯•ä½¿ç”¨æ›´ä¸­æ€§çš„æè¿°è¯ï¼Œæ¯”å¦‚ï¼š\n   - 'åŠ¨ä½œç‰‡' ä»£æ›¿æ•æ„Ÿé¢˜æ\n   - 'æ‚¬ç–‘ç‰‡' ä»£æ›¿çŠ¯ç½ªé¢˜æ\n   - 'å‰§æƒ…ç‰‡' ç­‰æ›´å®½æ³›çš„ç±»å‹\n\n2. ç›´æ¥æœç´¢å…·ä½“çš„ç”µå½±åç§°ï¼ˆå¦‚'æ— é—´é“'ï¼‰\n\n3. åˆ‡æ¢åˆ°å…¶ä»– AI æ¨¡å‹è¯•è¯•",
                    "thoughts": [f"âŒ å†…å®¹è¿‡æ»¤: {error_msg[:200]}"],
                }
            # æ£€æŸ¥æ˜¯å¦æ˜¯é€Ÿç‡é™åˆ¶é”™è¯¯
            elif "rate_limit" in error_msg.lower() or "429" in error_msg:
                return {
                    "answer": "æŠ±æ­‰,è¯·æ±‚è¿‡äºé¢‘ç¹,è¯·ç¨åå†è¯•ã€‚",
                    "thoughts": [f"âŒ é€Ÿç‡é™åˆ¶: {error_msg[:200]}"],
                }
            # å…¶ä»–é”™è¯¯
            else:
                return {
                    "answer": f"æŠ±æ­‰,å¤„ç†æ‚¨çš„è¯·æ±‚æ—¶å‡ºç°é”™è¯¯ã€‚è¯·ç¨åé‡è¯•æˆ–æ¢ä¸ªè¯´æ³•è¯•è¯•ã€‚\n\næŠ€æœ¯ä¿¡æ¯: {error_msg[:100]}",
                    "thoughts": [f"âŒ ç³»ç»Ÿé”™è¯¯: {error_msg[:200]}"],
                }

        # å±•ç¤ºæ€è€ƒè¿‡ç¨‹å’Œæœ€ç»ˆå›å¤
        ai_messages = [
            msg for msg in final_state["memory"] if isinstance(msg, AIMessage)
        ]
        final_answer = ""
        if ai_messages:
            last_ai_msg = ai_messages[-1]
            content = last_ai_msg.content
            # å¦‚æœæœ€åä¸€æ¡AIæ¶ˆæ¯è¿˜æœ‰tool_calls,è¯´æ˜æ²¡æœ‰ç”Ÿæˆæœ€ç»ˆå›å¤
            if getattr(last_ai_msg, "tool_calls", None):
                final_answer = ""  # ä¸ä½¿ç”¨å¸¦å·¥å…·è°ƒç”¨çš„æ¶ˆæ¯
            else:
                # è§£æçº¯æ–‡æœ¬å›å¤
                import re

                if isinstance(content, list):
                    texts = []
                    for x in content:
                        if isinstance(x, dict) and x.get("type") == "text":
                            text = str(x.get("text", ""))
                            texts.append(text)
                        # å¿½ç•¥ tool_use ç±»å‹çš„å†…å®¹
                    final_answer = "\n".join(texts)
                elif isinstance(content, dict) and "text" in content:
                    final_answer = str(content["text"])
                else:
                    final_answer = str(content)

                # âœ‚ï¸ æ¸…ç†æ¨ç†æ ‡ç­¾ï¼Œåªä¿ç•™æœ€ç»ˆç­”æ¡ˆ
                # 1. ç§»é™¤ <thinking> æ ‡ç­¾åŠå…¶å†…å®¹
                final_answer = re.sub(
                    r"<thinking>.*?</thinking>",
                    "",
                    final_answer,
                    flags=re.DOTALL | re.IGNORECASE,
                )

                # 2. æå–"æœ€ç»ˆç­”æ¡ˆï¼š"ä¹‹åçš„å†…å®¹ï¼ˆé’ˆå¯¹ GPT-4.1 æ ¼å¼ï¼‰
                answer_match = re.search(r"æœ€ç»ˆç­”æ¡ˆï¼š(.*)$", final_answer, re.DOTALL)
                if answer_match:
                    final_answer = answer_match.group(1).strip()

                final_answer = final_answer.strip()
            # æ¸…ç† Markdown é“¾æ¥æ ¼å¼ä¸ºçº¯é“¾æ¥
            # ä¾‹å¦‚: [https://xxx](https://xxx) => https://xxx
            final_answer = re.sub(
                r"\[([^\]]+)\]\((https?://[^)]+)\)", r"\2", final_answer
            )

        # ========== æ ¼å¼å¢å¼ºå™¨ï¼šç¡®ä¿é¢„å‘Šç‰‡é“¾æ¥è¢«æ­£ç¡®è¾“å‡º ==========
        def enhance_movie_format(answer: str, state: AgentState) -> str:
            """
            æ£€æŸ¥ LLM çš„å›å¤ä¸­æ˜¯å¦åŒ…å«é¢„å‘Šç‰‡é“¾æ¥
            å¦‚æœæ²¡æœ‰,åˆ™ä½œä¸ºå…œåº•,ä» raw_data ä¸­æå–é“¾æ¥å¹¶æ·»åŠ 

            æ³¨æ„: ç†æƒ³æƒ…å†µä¸‹,LLM åº”è¯¥ç›´æ¥ä» raw_data.trailer_link å­—æ®µè¯»å–é“¾æ¥å¹¶è¾“å‡º
            æ­¤å‡½æ•°ä»…ä½œä¸ºå…œåº•æªæ–½
            """
            # æå–æœ¬è½®çš„å·¥å…·è°ƒç”¨ç»“æœ
            tool_msgs = [
                msg
                for msg in state["memory"]
                if isinstance(msg, ToolMessage)
                and msg.tool_input.get("current_turn_id") == current_turn_id
            ]
            if not tool_msgs:
                return answer

            try:
                last_tool_msg = tool_msgs[-1]
                tool_result = json.loads(last_tool_msg.content)
                if isinstance(tool_result, dict) and "toolResult" in tool_result:
                    tool_data = json.loads(tool_result["toolResult"]["content"])
                else:
                    tool_data = tool_result
                if tool_data.get("status") != "success" or not tool_data.get(
                    "raw_data"
                ):
                    return answer

                movies = tool_data["raw_data"]

                # æ£€æŸ¥å›å¤ä¸­æ˜¯å¦å·²åŒ…å«é¢„å‘Šç‰‡é“¾æ¥
                if "ğŸ¬ é¢„å‘Šç‰‡:" in answer or "themoviedb.org/movie/" in answer:
                    return answer  # LLM å·²æ­£ç¡®è¾“å‡ºé“¾æ¥

                # ğŸ”§ å…œåº•é€»è¾‘: LLM å¿˜è®°è¾“å‡ºé“¾æ¥,æ‰‹åŠ¨æ·»åŠ 
                import re

                enhanced_answer = answer
                insertions = []

                for m in movies:
                    trailer_link = m.get("trailer_link", "")

                    title = m.get("title", "")
                    if not title:
                        continue

                    title_escaped = re.escape(title)
                    # ç²¾ç¡®åŒ¹é…æ ‡é¢˜(å¸¦å¹´ä»½æˆ–ä¸å¸¦)
                    pattern = f"((?:\\*\\*{title_escaped}(?:\\s*\\([^)]+\\))?\\*\\*|ã€Š{title_escaped}ã€‹).*?(?:è¯„åˆ†|çƒ­åº¦|ç®€ä»‹):.*?)(?=\\n\\n|\\n\\d+\\.|$)"
                    matches = list(re.finditer(pattern, enhanced_answer, re.DOTALL))

                    if matches:
                        last_match = matches[-1]
                        insert_pos = last_match.end()
                        # å¦‚æœæœ‰é“¾æ¥åˆ™æ˜¾ç¤ºé“¾æ¥ï¼Œå¦åˆ™æ˜¾ç¤º"æš‚æ— é¢„å‘Šç‰‡"
                        if trailer_link:
                            link_text = f"\n   ğŸ¬ é¢„å‘Šç‰‡: {trailer_link}"
                        else:
                            link_text = f"\n   ğŸ¬ é¢„å‘Šç‰‡: æš‚æ— é¢„å‘Šç‰‡"
                        insertions.append((insert_pos, link_text))

                # ä»åå¾€å‰æ’å…¥é¿å…ä½ç½®åç§»
                insertions.sort(key=lambda x: x[0], reverse=True)
                for position, link in insertions:
                    enhanced_answer = (
                        enhanced_answer[:position] + link + enhanced_answer[position:]
                    )

                return enhanced_answer

                # 4. å¦‚æœ LLM å›å¤ä¸å®Œæ•´ï¼Œä½¿ç”¨æ ¼å¼å¢å¼ºå™¨é‡æ–°ç”Ÿæˆ
                if not answer or ("ï¿½ è¯„åˆ†:" not in answer and "è¯„åˆ†" not in answer):
                    movie_titles_in_answer = []
                    for m in movies:
                        title = m.get("title", "")
                        if title:  # ä¸éœ€è¦æ£€æŸ¥titleæ˜¯å¦åœ¨answerä¸­
                            movie_titles_in_answer.append((title, m))
                if movie_titles_in_answer:
                    enhanced_lines = []
                    enhanced_lines.append("ä»¥ä¸‹æ˜¯æ¨èçš„ç”µå½±:\n")
                    for i, (title, m) in enumerate(movie_titles_in_answer, 1):
                        movie_id = m.get("id", "")
                        year = (
                            m.get("release_date", "")[:4]
                            if m.get("release_date")
                            else "æœªçŸ¥"
                        )
                        vote_avg = m.get("vote_average", 0)
                        vote_cnt = m.get("vote_count", 0)
                        popularity = m.get("popularity", 0)
                        overview = m.get("overview", "") or "æš‚æ— ç®€ä»‹"

                        enhanced_lines.append(f"{i}. ã€Š{title}ã€‹({year})")
                        if vote_cnt < 100:
                            enhanced_lines.append(
                                f"   ğŸ“Š è¯„åˆ†: {vote_avg}/10 ({vote_cnt:,}äººè¯„åˆ†) âš ï¸ å†·é—¨ç”µå½±,å‚è€ƒä»·å€¼ä½"
                            )
                        else:
                            enhanced_lines.append(
                                f"   ğŸ“Š è¯„åˆ†: {vote_avg}/10 ({vote_cnt:,}äººè¯„åˆ†) | ğŸ”¥ çƒ­åº¦: {popularity}"
                            )
                        if popularity > 1000:
                            enhanced_lines.append(f"   ğŸ”¥ çƒ­é—¨æ¨è!")
                        enhanced_lines.append(
                            f"   ğŸ“ ç®€ä»‹: {overview[:100]}{'...' if len(overview) > 100 else ''}"
                        )

                        # æ·»åŠ  TMDB é¢„å‘Šç‰‡é“¾æ¥
                        if movie_id:
                            enhanced_lines.append(
                                f"   ğŸ¬ é¢„å‘Šç‰‡: https://www.themoviedb.org/movie/{movie_id}/videos"
                            )

                        enhanced_lines.append("")
                    return "\n".join(enhanced_lines)
            except Exception as e:
                print(f"âš ï¸ æ ¼å¼å¢å¼ºå¤±è´¥: {e}")
                pass
            return answer

        # åº”ç”¨æ ¼å¼å¢å¼º
        final_answer = enhance_movie_format(final_answer, final_state)
        # å…œåº•ï¼šåªå–æœ¬è½®(current_turn_id)çš„ToolMessage
        if not final_answer.strip():
            tool_msgs = [
                msg
                for msg in self.state["memory"]
                if isinstance(msg, ToolMessage)
                and msg.tool_input.get("current_turn_id") == current_turn_id
            ]
            if tool_msgs:
                last_tool_msg = tool_msgs[-1]
                try:
                    tool_result = json.loads(last_tool_msg.content)
                    if isinstance(tool_result, dict) and "toolResult" in tool_result:
                        tool_data = json.loads(tool_result["toolResult"]["content"])
                    else:
                        tool_data = tool_result
                    if tool_data.get("status") == "success" and tool_data.get(
                        "raw_data"
                    ):
                        movies = tool_data["raw_data"]
                        lines = []
                        for m in movies:
                            title = m.get("title", "æœªçŸ¥")
                            year = (
                                m.get("release_date", "")[:4]
                                if m.get("release_date")
                                else "æœªçŸ¥"
                            )
                            overview = m.get("overview", "")
                            lines.append(f"ã€Š{title}ã€‹({year})\nç®€ä»‹ï¼š{overview}")
                        final_answer = "\n".join(lines)
                    elif tool_data.get("status") == "error":
                        final_answer = f"æŸ¥è¯¢å¤±è´¥ï¼š{tool_data.get('msg','')}"
                    else:
                        final_answer = "æœªæ‰¾åˆ°ç›¸å…³ç”µå½±ï¼Œå»ºè®®è¡¥å……æ›´è¯¦ç»†çš„ç‰‡åæˆ–å¹´ä»½ã€‚"
                except Exception as e:
                    final_answer = f"ç»“æœè§£æå¤±è´¥ï¼š{str(e)[:30]}"
        return {"answer": final_answer.strip(), "thoughts": final_state["thoughts"]}


# ====== CLIæµ‹è¯•å…¥å£ ======
if __name__ == "__main__":
    print("ğŸ¬ ç”µå½±æ¨èAIåŠ©æ‰‹")
    print(
        "æ”¯æŒä¸­è‹±æ–‡æœç´¢/æ¨èï¼Œè¾“å…¥'exit'é€€å‡ºï¼Œè¾“å…¥'reset'æ¸…ç©ºå†å²ï¼Œè¾“å…¥'switch'åˆ‡æ¢æ¨¡å‹\n"
    )

    # æ˜¾ç¤ºå¯ç”¨æ¨¡å‹
    print("ğŸ“‹ å¯ç”¨æ¨¡å‹:")
    for key, info in AVAILABLE_MODELS.items():
        print(f"   {key}. {info['name']}")

    # ç›´æ¥ä½¿ç”¨ GPT-4.1ï¼Œä¸å†è¯¢é—®
    print(f"\nğŸ¤– é»˜è®¤ä½¿ç”¨: GPT-4.1 (è¾“å…¥ 'switch' å¯åˆ‡æ¢æ¨¡å‹)\n")
    agent = MovieAgent(model_choice="2")  # ç›´æ¥ä½¿ç”¨ GPT-4.1

    try:
        while True:
            try:
                user_input = input("ä½ : ").strip()
            except KeyboardInterrupt:
                print("\n\nğŸ‘‹ æ£€æµ‹åˆ° Ctrl+Cï¼Œæ­£åœ¨é€€å‡º...")
                print("å¯¹è¯å†å²å·²ä¿å­˜ï½")
                break

            if user_input.lower() in {"exit", "quit", "q"}:
                print("å†è§ï¼å¯¹è¯å†å²å·²ä¿å­˜ï½")
                break

            if user_input.lower() == "reset":
                agent.reset()
                print("å·²æ¸…ç©ºå¯¹è¯å†å²ï½")
                continue

            if user_input.lower() == "switch":
                print("\nğŸ“‹ å¯ç”¨æ¨¡å‹:")
                for key, info in AVAILABLE_MODELS.items():
                    current = " (å½“å‰)" if key == agent.model_choice else ""
                    print(f"   {key}. {info['name']}{current}")

                new_choice = input(
                    f"\nè¯·é€‰æ‹©æ¨¡å‹ (1-{len(AVAILABLE_MODELS)}): "
                ).strip()
                agent.switch_model(new_choice)
                continue

            if not user_input:
                print("è¯·è¾“å…¥æœ‰æ•ˆçš„ç”µå½±éœ€æ±‚ï½")
                continue

            result = agent.chat(user_input)

            # æå– LLM åŸç”Ÿæ¨ç†å†…å®¹
            import re

            llm_reasoning_list = []
            for thought in result["thoughts"]:
                if thought.startswith("LLM reasoning â†’"):
                    reasoning_text = thought.replace("LLM reasoning â†’", "").strip()
                    # æ¸…ç† <thinking> æ ‡ç­¾ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
                    reasoning_text = re.sub(
                        r"<thinking>(.*?)</thinking>",
                        r"\1",
                        reasoning_text,
                        flags=re.DOTALL | re.IGNORECASE,
                    )
                    reasoning_text = reasoning_text.strip()
                    if reasoning_text:  # åªæ·»åŠ éç©ºçš„æ¨ç†å†…å®¹
                        llm_reasoning_list.append(reasoning_text)

            # æ ¼å¼åŒ–è¾“å‡º
            print("\n" + "=" * 80)
            print("ã€ç”¨æˆ·è¾“å…¥ã€‘\n" + "-" * 40)
            print(user_input)
            print("-" * 40)

            print("\nã€LLMæ¨ç†è¿‡ç¨‹ã€‘\n" + "-" * 40)
            if llm_reasoning_list:
                for i, reasoning in enumerate(llm_reasoning_list, 1):
                    print(reasoning)
                    if i < len(llm_reasoning_list):
                        print()  # å¤šä¸ªæ¨ç†ä¹‹é—´æ·»åŠ ç©ºè¡Œ
            else:
                print("æœ¬è½®æ— LLMæ¨ç†è¾“å‡º")
            print("-" * 40)

            print("\nã€æœ€ç»ˆå›å¤ã€‘\n" + "-" * 40)
            print(result["answer"])
            print("-" * 40)
    except Exception as e:
        import traceback

        print(f"\nâŒ ç¨‹åºå¼‚å¸¸: {e}")
        print("å¼‚å¸¸ç±»å‹:", type(e))
        print("\nå®Œæ•´è¿½è¸ª:")
        traceback.print_exc()
        print("å¯¹è¯å†å²å·²ä¿å­˜ï½")
