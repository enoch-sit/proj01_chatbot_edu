<!DOCTYPE html><html class="govuk-template govuk-template--rebranded" lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta property="og:description" content="">
<meta property="og:title" content="Artificial Intelligence Playbook for the UK Government (HTML)">
<meta property="og:url" content="https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html">
<meta property="og:type" content="article">
<meta property="og:site_name" content="GOV.UK">
<meta name="twitter:card" content="summary">
<meta name="govuk:organisations" content="&lt;OT1056&gt;">
<meta name="govuk:primary-publishing-organisation" content="Government Digital Service">
<meta name="govuk:public-updated-at" content="2025-02-10T00:00:00+00:00">
<meta name="govuk:updated-at" content="2025-11-03T17:45:57+00:00">
<meta name="govuk:first-published-at" content="2025-02-10T00:01:07+00:00">
<meta name="govuk:ga4-base-path" content="/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html">
<meta name="govuk:content-id" content="e46377fa-eae5-4c64-9437-e3612735aee5">
<meta name="govuk:schema-name" content="html_publication">
<meta name="govuk:rendering-app" content="government-frontend">
<meta name="govuk:publishing-app" content="whitehall">
<meta name="govuk:format" content="html_publication">
    <meta charset="utf-8">
    <title lang="en">
      Artificial Intelligence Playbook for the UK Government (HTML) - GOV.UK
  </title>

    <script src="/assets/static/govuk_publishing_components/vendor/lux/lux-measurer-d66192b1b665d415e1874e49a48138b7ae339548aa6cdb8687c7f9933e72b969.js" async="async"></script>
    <script src="/assets/static/govuk_publishing_components/rum-custom-data-b9e1806d1da2fa8ef1855d01aa71ba5eb0afc010dd2c4de3672cc7c7a39b0c8c.js" type="module"></script>
    <script src="/assets/static/govuk_publishing_components/rum-loader-a65b10e18ceeba3bd8a2eac507c7f2c513cdc82f35097df903fdea87f1dc2e33.js" async="async" data-lux-reporter-script="/assets/static/govuk_publishing_components/vendor/lux/lux-reporter-205c90cd95af651d7b312dbedc8ed8aa04bbc5f64067777f9b83385a74f7307b.js"></script>

    <meta name="govuk:components_gem_version" content="61.4.1">
    <script src="/assets/static/govuk_publishing_components/load-analytics-2fedf6967543ecf2e1442db2ee751262d0b0227274dc3fbb7f8df6db459b77d8.js" type="module"></script>

    

    <link rel="stylesheet" href="/assets/static/application-43dd279605d9d19856cf6ed3d4dc3e71f5e9fb96a39dea5d6b297b571a71e17b.css" media="all">
    <link rel="icon" sizes="48x48" href="/assets/static/favicon-24f9fbe064118d58937932e73edafd1d50cb60f7bd84f52308382a309bc2d655.ico">
    <link rel="icon" sizes="any" href="/assets/static/favicon-d962d21b5bb443f546c097ea21b567cde639adef7370da45be5e349ba8d62d33.svg" type="image/svg+xml">
    <link rel="mask-icon" href="/assets/static/govuk-icon-mask-cdf4265165f8d7f9eec54aa2c1dfbb3d8b6d297c5d7919f0313e0836a5804bb6.svg" color="#1d70b8">
    <link rel="apple-touch-icon" href="/assets/static/govuk-icon-180-d45a306f0549414cfa5085f16e4a3816a77558b139e5bc226f23162d1d0decb0.png">

    <meta name="theme-color" content="#1d70b8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://www.gov.uk/assets/static/govuk-opengraph-image-4196a4d6333cf92aaf720047f56cfd91b3532d7635fc21ebcf0d5897df6b5f77.png">

    
  <link rel="stylesheet" href="/assets/government-frontend/application-ae1d4c7a38e1c244473a4e6b36da6c8d42e9ba30a6ecd8fc52b6b17052ba96eb.css" media="all">
<link rel="canonical" href="https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html">
<link rel="stylesheet" href="/assets/government-frontend/views/_html-publication-54fffb27670e6a49b040058204ce99bfcfea7a0f078c0f4de1471ac6e1ee9256.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_organisation-logo-f689408b02a3303831ca6310b4fed69a988a13c6bcafb818a6bfef15eaabf652.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_inverse-header-650db705c01a51a37a7fb837bbeb8fbbae7b9ebc49b995f7e518e4010da3f478.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_contents-list-9cf09e785868fb478a0587bee10ce15724d1e44dfadd71b6528ba9d14cba38f5.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_print-link-315207d796d062285407e0b8ebbbb34d11ebf65c90ff77ebc345dd4cfb9380fd.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_govspeak-html-publication-3d45c330f2a6a08a72fb9a8f7da43e942723fa1aaab83bd281f87a7d19452e74.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_govspeak-c7479da415737489dac701594597e4c84359e5cc14b4cb5173f18c8a28fc1df3.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_attachment-link-32eab833e57657c72835f1c663012a730d34cca34931b9b99dabc803c382baf5.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_attachment-1c0388cdaad71801fcc8bccf49370ff9cc56f47fc31834ec595b84277017badd.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_details-593d45a9fd2a883887784a3e7dcafbac11d9c1e1001296ce48e957b1ee567b23.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_back-to-top-link-b2ac00053291bacbaad380cd9ed082e0289a6614b275a258ac7a989726053544.css">
<meta name="govuk:rendering-application" content="government-frontend">
</head>
  <body class="gem-c-layout-for-public govuk-template__body js-enabled govuk-frontend-supported">
    <script nonce="">
//<![CDATA[
      document.body.className += ' js-enabled' + ('noModule' in HTMLScriptElement.prototype ? ' govuk-frontend-supported' : '');

//]]>
</script>    
<div id="global-cookie-message" data-module="cookie-banner" data-nosnippet="" aria-label="Cookies on GOV.UK" class="gem-c-cookie-banner govuk-clearfix govuk-cookie-banner js-banner-wrapper" role="region" data-cookie-banner-module-started="true">
  <div class="govuk-cookie-banner__message govuk-width-container">
    <div class="govuk-grid-row">
      <div class="govuk-grid-column-two-thirds">
        <h2 class="govuk-cookie-banner__heading govuk-heading-m">Cookies on GOV.UK</h2>
        <div tabindex="-1" class="govuk-cookie-banner__content gem-c-cookie-banner__confirmation">
          <div class="gem-c-cookie-banner__content">
<p class="govuk-body">We use some essential cookies to make this website work.</p>
<p class="govuk-body">We’d like to set additional cookies to understand how you use GOV.UK, remember your settings and improve government services.</p>
<p class="govuk-body">We also use cookies set by other sites to help us deliver content from their services.</p>
</div>
          <p class="gem-c-cookie-banner__confirmation-message--accepted govuk-body" hidden="" data-ga4-cookie-banner="" data-module="ga4-link-tracker" data-ga4-track-links-only="" data-ga4-set-indexes="" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;cookie banner&quot;,&quot;section&quot;:&quot;You have accepted additional cookies&quot;}" data-ga4-link-tracker-module-started="true">You have accepted additional cookies. <span class="gem-c-cookie-banner__confirmation-message">You can <a class="govuk-link" href="/help/cookies">change your cookie settings</a> at any time.</span></p>
          <p class="gem-c-cookie-banner__confirmation-message--rejected govuk-body" hidden="">You have rejected additional cookies. <span class="gem-c-cookie-banner__confirmation-message">You can <a class="govuk-link" href="/help/cookies">change your cookie settings</a> at any time.</span></p>
        </div>
      </div>
    </div>
    <div class="js-confirmation-buttons govuk-button-group">
        

  <button class="gem-c-button govuk-button" type="submit" data-accept-cookies="true" data-cookie-types="all">Accept additional cookies</button>


        

  <button class="gem-c-button govuk-button" type="submit" data-reject-cookies="true">Reject additional cookies</button>


        <a class="govuk-link" href="/help/cookies">View cookies</a>
    </div>
    <div hidden="" class="js-hide-button govuk-button-group">
      <button class="gem-c-cookie-banner__hide-button govuk-button" data-hide-cookie-banner="true" data-module="ga4-event-tracker" data-ga4-event="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;type&quot;:&quot;cookie banner&quot;,&quot;action&quot;:&quot;closed&quot;,&quot;section&quot;:&quot;You have accepted additional cookies&quot;}" data-ga4-event-tracker-module-started="true">
          Hide cookie message
        </button>
    </div>
  </div>
</div>
    <a data-module="govuk-skip-link" class="gem-c-skip-link govuk-skip-link govuk-!-display-none-print" href="#content" data-govuk-skip-link-init="" data-govuk-skip-link-module-started="true">Skip to main content</a>

          <header data-module="ga4-event-tracker ga4-link-tracker" data-ga4-expandable="" class="gem-c-layout-super-navigation-header" data-ga4-event-tracker-module-started="true" data-ga4-link-tracker-module-started="true">
<div class="gem-c-layout-super-navigation-header__container govuk-width-container">
  <nav aria-labelledby="super-navigation-menu-heading" class="gem-c-layout-super-navigation-header__content js-module-initialised" data-module="super-navigation-mega-menu" data-super-navigation-mega-menu-module-started="true">
    <div class="gem-c-layout-super-navigation-header__header-logo">
      <a class="govuk-header__link govuk-header__link--homepage" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;external&quot;:&quot;false&quot;,&quot;text&quot;:&quot;GOV.UK&quot;,&quot;section&quot;:&quot;Logo&quot;,&quot;index_link&quot;:1,&quot;index_section&quot;:0,&quot;index_section_count&quot;:2,&quot;index_total&quot;:1}" id="logo" aria-label="Go to the GOV.UK homepage" href="https://www.gov.uk">
        <svg xmlns="http://www.w3.org/2000/svg" focusable="false" role="img" viewBox="0 0 324 60" height="30" width="162" fill="currentcolor" class="govuk-header__logotype" aria-label="GOV.UK">
  <title>GOV.UK</title>
  <g>
    <circle cx="20" cy="17.6" r="3.7"></circle>
    <circle cx="10.2" cy="23.5" r="3.7"></circle>
    <circle cx="3.7" cy="33.2" r="3.7"></circle>
    <circle cx="31.7" cy="30.6" r="3.7"></circle>
    <circle cx="43.3" cy="17.6" r="3.7"></circle>
    <circle cx="53.2" cy="23.5" r="3.7"></circle>
    <circle cx="59.7" cy="33.2" r="3.7"></circle>
    <circle cx="31.7" cy="30.6" r="3.7"></circle>
    <path d="M33.1,9.8c.2-.1.3-.3.5-.5l4.6,2.4v-6.8l-4.6,1.5c-.1-.2-.3-.3-.5-.5l1.9-5.9h-6.7l1.9,5.9c-.2.1-.3.3-.5.5l-4.6-1.5v6.8l4.6-2.4c.1.2.3.3.5.5l-2.6,8c-.9,2.8,1.2,5.7,4.1,5.7h0c3,0,5.1-2.9,4.1-5.7l-2.6-8ZM37,37.9s-3.4,3.8-4.1,6.1c2.2,0,4.2-.5,6.4-2.8l-.7,8.5c-2-2.8-4.4-4.1-5.7-3.8.1,3.1.5,6.7,5.8,7.2,3.7.3,6.7-1.5,7-3.8.4-2.6-2-4.3-3.7-1.6-1.4-4.5,2.4-6.1,4.9-3.2-1.9-4.5-1.8-7.7,2.4-10.9,3,4,2.6,7.3-1.2,11.1,2.4-1.3,6.2,0,4,4.6-1.2-2.8-3.7-2.2-4.2.2-.3,1.7.7,3.7,3,4.2,1.9.3,4.7-.9,7-5.9-1.3,0-2.4.7-3.9,1.7l2.4-8c.6,2.3,1.4,3.7,2.2,4.5.6-1.6.5-2.8,0-5.3l5,1.8c-2.6,3.6-5.2,8.7-7.3,17.5-7.4-1.1-15.7-1.7-24.5-1.7h0c-8.8,0-17.1.6-24.5,1.7-2.1-8.9-4.7-13.9-7.3-17.5l5-1.8c-.5,2.5-.6,3.7,0,5.3.8-.8,1.6-2.3,2.2-4.5l2.4,8c-1.5-1-2.6-1.7-3.9-1.7,2.3,5,5.2,6.2,7,5.9,2.3-.4,3.3-2.4,3-4.2-.5-2.4-3-3.1-4.2-.2-2.2-4.6,1.6-6,4-4.6-3.7-3.7-4.2-7.1-1.2-11.1,4.2,3.2,4.3,6.4,2.4,10.9,2.5-2.8,6.3-1.3,4.9,3.2-1.8-2.7-4.1-1-3.7,1.6.3,2.3,3.3,4.1,7,3.8,5.4-.5,5.7-4.2,5.8-7.2-1.3-.2-3.7,1-5.7,3.8l-.7-8.5c2.2,2.3,4.2,2.7,6.4,2.8-.7-2.3-4.1-6.1-4.1-6.1h10.6,0Z"></path>
  </g>
  <circle class="govuk-logo-dot" cx="226" cy="36" r="7.3"></circle>
  <path d="M93.94 41.25c.4 1.81 1.2 3.21 2.21 4.62 1 1.4 2.21 2.41 3.61 3.21s3.21 1.2 5.22 1.2 3.61-.4 4.82-1c1.4-.6 2.41-1.4 3.21-2.41.8-1 1.4-2.01 1.61-3.01s.4-2.01.4-3.01v.14h-10.86v-7.02h20.07v24.08h-8.03v-5.56c-.6.8-1.38 1.61-2.19 2.41-.8.8-1.81 1.2-2.81 1.81-1 .4-2.21.8-3.41 1.2s-2.41.4-3.81.4a18.56 18.56 0 0 1-14.65-6.63c-1.6-2.01-3.01-4.41-3.81-7.02s-1.4-5.62-1.4-8.83.4-6.02 1.4-8.83a20.45 20.45 0 0 1 19.46-13.65c3.21 0 4.01.2 5.82.8 1.81.4 3.61 1.2 5.02 2.01 1.61.8 2.81 2.01 4.01 3.21s2.21 2.61 2.81 4.21l-7.63 4.41c-.4-1-1-1.81-1.61-2.61-.6-.8-1.4-1.4-2.21-2.01-.8-.6-1.81-1-2.81-1.4-1-.4-2.21-.4-3.61-.4-2.01 0-3.81.4-5.22 1.2-1.4.8-2.61 1.81-3.61 3.21s-1.61 2.81-2.21 4.62c-.4 1.81-.6 3.71-.6 5.42s.8 5.22.8 5.22Zm57.8-27.9c3.21 0 6.22.6 8.63 1.81 2.41 1.2 4.82 2.81 6.62 4.82S170.2 24.39 171 27s1.4 5.62 1.4 8.83-.4 6.02-1.4 8.83-2.41 5.02-4.01 7.02-4.01 3.61-6.62 4.82-5.42 1.81-8.63 1.81-6.22-.6-8.63-1.81-4.82-2.81-6.42-4.82-3.21-4.41-4.01-7.02-1.4-5.62-1.4-8.83.4-6.02 1.4-8.83 2.41-5.02 4.01-7.02 4.01-3.61 6.42-4.82 5.42-1.81 8.63-1.81Zm0 36.73c1.81 0 3.61-.4 5.02-1s2.61-1.81 3.61-3.01 1.81-2.81 2.21-4.41c.4-1.81.8-3.61.8-5.62 0-2.21-.2-4.21-.8-6.02s-1.2-3.21-2.21-4.62c-1-1.2-2.21-2.21-3.61-3.01s-3.21-1-5.02-1-3.61.4-5.02 1c-1.4.8-2.61 1.81-3.61 3.01s-1.81 2.81-2.21 4.62c-.4 1.81-.8 3.61-.8 5.62 0 2.41.2 4.21.8 6.02.4 1.81 1.2 3.21 2.21 4.41s2.21 2.21 3.61 3.01c1.4.8 3.21 1 5.02 1Zm36.32 7.96-12.24-44.15h9.83l8.43 32.77h.4l8.23-32.77h9.83L200.3 58.04h-12.24Zm74.14-7.96c2.18 0 3.51-.6 3.51-.6 1.2-.6 2.01-1 2.81-1.81s1.4-1.81 1.81-2.81a13 13 0 0 0 .8-4.01V13.9h8.63v28.15c0 2.41-.4 4.62-1.4 6.62-.8 2.01-2.21 3.61-3.61 5.02s-3.41 2.41-5.62 3.21-4.62 1.2-7.02 1.2-5.02-.4-7.02-1.2c-2.21-.8-4.01-1.81-5.62-3.21s-2.81-3.01-3.61-5.02-1.4-4.21-1.4-6.62V13.9h8.63v26.95c0 1.61.2 3.01.8 4.01.4 1.2 1.2 2.21 2.01 2.81.8.8 1.81 1.4 2.81 1.81 0 0 1.34.6 3.51.6Zm34.22-36.18v18.92l15.65-18.92h10.82l-15.03 17.32 16.03 26.83h-10.21l-11.44-20.21-5.62 6.22v13.99h-8.83V13.9"></path>
</svg>

</a>
</div>    <h2 id="super-navigation-menu-heading" class="govuk-visually-hidden">
      Navigation menu
    </h2>


    <div class="gem-c-layout-super-navigation-header__navigation-item">
      <a class="gem-c-layout-super-navigation-header__navigation-item-link" href="/browse" hidden="hidden"><span class="gem-c-layout-super-navigation-header__navigation-item-link-inner">          Menu
</span></a>
      <button aria-controls="super-navigation-menu" aria-expanded="false" aria-label="Show navigation menu" class="gem-c-layout-super-navigation-header__navigation-top-toggle-button" data-text-for-hide="Hide navigation menu" data-text-for-show="Show navigation menu" data-toggle-desktop-group="top" data-toggle-mobile-group="top" data-ga4-event="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;text&quot;:&quot;Menu&quot;,&quot;index_section&quot;:1,&quot;index_section_count&quot;:2,&quot;section&quot;:&quot;Menu&quot;}" id="super-navigation-menu-toggle" type="button">
        <span class="gem-c-layout-super-navigation-header__navigation-top-toggle-button-inner">Menu</span>
</button>    </div>
    <div id="super-navigation-menu" hidden="hidden" class="gem-c-layout-super-navigation-header__navigation-dropdown-menu">
      <div class="govuk-grid-row gem-c-layout-super-navigation-header__navigation-items">


          <div class="govuk-grid-column-two-thirds-from-desktop gem-c-layout-super-navigation-header__column--services-and-information">
            <h3 class="govuk-heading-m gem-c-layout-super-navigation-header__column-header">
              Services and information
            </h3>
            <ul class="gem-c-layout-super-navigation-header__navigation-second-items gem-c-layout-super-navigation-header__navigation-second-items--services-and-information">
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:1,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/benefits">Benefits</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:2,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/births-deaths-marriages">Births, death, marriages and care</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:3,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/business">Business and self-employed</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:4,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/childcare-parenting">Childcare and parenting</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:5,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/citizenship">Citizenship and living in the UK</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:6,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/justice">Crime, justice and the law</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:7,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/disabilities">Disabled people</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:8,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/driving">Driving and transport</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:9,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/education">Education and learning</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:10,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/employing-people">Employing people</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:11,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/environment-countryside">Environment and countryside</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:12,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/housing-local-services">Housing and local services</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:13,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/tax">Money and tax</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:14,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/abroad">Passports, travel and living abroad</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:15,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/visas-immigration">Visas and immigration</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:16,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/working">Working, jobs and pensions</a>
                    
                  </li>
            </ul>
          </div>

          <div class="govuk-grid-column-one-third-from-desktop gem-c-layout-super-navigation-header__column--government-activity">
            <h3 class="govuk-heading-m gem-c-layout-super-navigation-header__column-header">
              Government activity
            </h3>
            <ul class="gem-c-layout-super-navigation-header__navigation-second-items gem-c-layout-super-navigation-header__navigation-second-items--government-activity">
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:1,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/organisations">Departments</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Departments, agencies and public bodies</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:2,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/news-and-communications">News</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">News stories, speeches, letters and notices</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:3,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/guidance-and-regulation">Guidance and regulation</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Detailed guidance, regulations and rules</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:4,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/research-and-statistics">Research and statistics</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Reports, analysis and official statistics</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:5,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/policy-papers-and-consultations">Policy papers and consultations</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Consultations and strategy</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:6,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/transparency-and-freedom-of-information-releases">Transparency</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Data, Freedom of Information releases and corporate reports</p>
                  </li>
            </ul>
          </div>
      </div>
</div>    <div class="gem-c-layout-super-navigation-header__search-item">
      <button id="super-search-menu-toggle" class="gem-c-layout-super-navigation-header__search-toggle-button" aria-controls="super-search-menu" aria-expanded="false" aria-label="Show search menu" data-text-for-hide="Hide search menu" data-text-for-show="Show search menu" data-toggle-mobile-group="top" data-toggle-desktop-group="top" data-ga4-event="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;text&quot;:&quot;Search&quot;,&quot;index_section&quot;:2,&quot;index_section_count&quot;:2,&quot;section&quot;:&quot;Search&quot;}" type="button">
        <span class="govuk-visually-hidden">
          Search GOV.UK
        </span>
        
<svg class="gem-c-layout-super-navigation-header__search-toggle-button-link-icon" width="27" height="27" viewBox="0 0 27 27" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
  <circle cx="12.0161" cy="11.0161" r="8.51613" stroke="currentColor" stroke-width="3"></circle>
  <line x1="17.8668" y1="17.3587" x2="26.4475" y2="25.9393" stroke="currentColor" stroke-width="3"></line>
</svg>

        <span aria-hidden="true" class="gem-c-layout-super-navigation-header__navigation-top-toggle-close-icon">
          ×
        </span>
</button>
      <a class="gem-c-layout-super-navigation-header__search-item-link" href="/search" hidden="hidden">
        <span class="govuk-visually-hidden">
          Search GOV.UK
        </span>
        
<svg class="gem-c-layout-super-navigation-header__search-item-link-icon" width="27" height="27" viewBox="0 0 27 27" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
  <circle cx="12.0161" cy="11.0161" r="8.51613" stroke="currentColor" stroke-width="3"></circle>
  <line x1="17.8668" y1="17.3587" x2="26.4475" y2="25.9393" stroke="currentColor" stroke-width="3"></line>
</svg>

</a>    </div>
    <div id="super-search-menu" hidden="hidden" class="gem-c-layout-super-navigation-header__navigation-dropdown-menu">
      <div class="gem-c-layout-super-navigation-header__search-container gem-c-layout-super-navigation-header__search-items">
        <h3 class="govuk-visually-hidden">
          Search
        </h3>
        <div class="govuk-grid-row">
          <div class="govuk-grid-column-full">
            <form class="gem-c-layout-super-navigation-header__search-form" id="search" data-module="ga4-search-tracker" data-ga4-search-type="header menu bar" data-ga4-search-url="/search/all" data-ga4-search-section="Search GOV.UK" data-ga4-search-index-section="3" data-ga4-search-index-section-count="3" action="/search/all" method="get" role="search" aria-label="Site-wide" data-ga4-search-tracker-module-started="true">
              <div class="gem-c-search-with-autocomplete gem-c-search-with-autocomplete--large govuk-!-margin-bottom-0" data-module="gem-search-with-autocomplete" data-source-url="https://www.gov.uk/api/search/autocomplete.json" data-source-key="suggestions" data-gem-search-with-autocomplete-module-started="true">
  <div data-module="gem-toggle-input-class-on-focus" class="gem-c-search govuk-!-display-none-print gem-c-search--large gem-c-search--on-white gem-c-search--separate-label govuk-!-margin-bottom-0" data-gem-toggle-input-class-on-focus-module-started="true">
    <label for="search-main-de17b5b4" class="govuk-label govuk-label--m gem-c-layout-super-navigation-header__search-label--large-navbar">Search GOV.UK</label>
  <div class="gem-c-search__item-wrapper">
    <div class="js-search-input-wrapper">
      
    <div class="gem-c-search-with-autocomplete__wrapper"><div class="gem-c-search-with-autocomplete__status" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin-bottom: -1px; margin-right: -1px; overflow: hidden; padding: 0px; position: absolute; white-space: nowrap; width: 1px;"><div id="search-main-de17b5b4__status--A" role="status" aria-atomic="true" aria-live="polite"></div><div id="search-main-de17b5b4__status--B" role="status" aria-atomic="true" aria-live="polite"></div></div><input aria-describedby="search-main-de17b5b4__assistiveHint" aria-expanded="false" aria-controls="search-main-de17b5b4__listbox" aria-autocomplete="list" autocomplete="off" class="gem-c-search-with-autocomplete__input gem-c-search-with-autocomplete__input--default gem-c-search__item gem-c-search__input js-class-toggle" id="search-main-de17b5b4" name="keywords" placeholder="" type="search" role="combobox"><ul aria-labelledby="search-main-de17b5b4" id="search-main-de17b5b4__listbox" role="listbox" class="gem-c-search-with-autocomplete__menu gem-c-search-with-autocomplete__menu--inline gem-c-search-with-autocomplete__menu--hidden"></ul><span id="search-main-de17b5b4__assistiveHint" style="display: none;">When search suggestions are available use up and down arrows to review and enter to select. Touch device users, explore by touch or with swipe gestures.</span></div></div>
    <div class="gem-c-search__item gem-c-search__submit-wrapper">
      <button class="gem-c-search__submit" type="submit" enterkeyhint="search">
        Search
        
<svg class="gem-c-search__icon" width="27" height="27" viewBox="0 0 27 27" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
  <circle cx="12.0161" cy="11.0161" r="8.51613" stroke="currentColor" stroke-width="3"></circle>
  <line x1="17.8668" y1="17.3587" x2="26.4475" y2="25.9393" stroke="currentColor" stroke-width="3"></line>
</svg>

</button>    </div>
  </div>
</div>
</div>
</form>          </div>
        </div>
      </div>
</div>  </nav>
</div>
</header>

    


      <div id="wrapper" class="direction-ltr govuk-width-container">
        
<div class="gem-c-contextual-breadcrumbs">
    <div class="govuk-!-display-none-print">
      


<nav data-module="ga4-link-tracker" aria-label="Breadcrumb" class="gem-c-breadcrumbs govuk-breadcrumbs govuk-breadcrumbs--collapse-on-mobile" data-ga4-link-tracker-module-started="true">
  <ol class="govuk-breadcrumbs__list">
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_total&quot;:&quot;3&quot;}" class="govuk-breadcrumbs__link" href="/">Home</a>
        </li>
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_total&quot;:&quot;3&quot;}" class="govuk-breadcrumbs__link" href="/government/all">Government</a>
        </li>
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_total&quot;:&quot;3&quot;}" class="govuk-breadcrumbs__link" href="/government/publications/ai-playbook-for-the-uk-government">AI Playbook for the UK Government</a>
        </li>
  </ol>
</nav>
    </div>
</div>

    

    

    <main role="main" id="content" class="html-publication govuk-main-wrapper govuk-main-wrapper--auto-spacing" lang="en">
      <span id="Top"></span>
          

  <div class="publication-external">
    <ul class="organisation-logos">
        <li class="organisation-logos__logo">
          
<div class="gem-c-organisation-logo brand--government-digital-service">
    <a class="gem-c-organisation-logo__container gem-c-organisation-logo__link gem-c-organisation-logo__crest gem-c-organisation-logo__crest--gds brand__border-color" href="/government/organisations/government-digital-service">
      <span class="gem-c-organisation-logo__name">Government<br>Digital Service</span>
</a>
</div>
        </li>
    </ul>
  </div>

  <header class="gem-c-inverse-header gem-c-inverse-header--html-publication-header govuk-!-padding-top-6 govuk-!-padding-bottom-6">
    
  
<div class="gem-c-heading  gem-c-heading--inverse govuk-!-margin-bottom-0">
      <span class="govuk-caption-xl gem-c-heading__context">
    Guidance
  </span>


  <h1 class="gem-c-heading__text govuk-heading-xl">
    Artificial Intelligence Playbook for the UK Government (HTML)
</h1>
</div>

    <p class="gem-c-inverse-header__subtext">Published 10 February 2025</p>
</header>


<div id="contents">
  <div class="govuk-grid-row gem-print-columns-none">
      <div class="govuk-grid-column-one-quarter-from-desktop contents-list-container">
          <nav data-module="ga4-link-tracker" aria-label="Contents" class="gem-c-contents-list govuk-!-margin-bottom-4" data-ga4-link-tracker-module-started="true">
    <h2 class="gem-c-contents-list__title">
      Contents
</h2>
    <ol class="gem-c-contents-list__list">
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:1}" href="#acknowledgements">Acknowledgements</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:2}" href="#update-log">Update log</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:3}" href="#foreword">Foreword</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:4}" href="#preface">Preface</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:5}" href="#principles">Principles</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:6}" href="#principle-1-you-know-what-ai-is-and-what-its-limitations-are">Principle 1: You know what AI is and what its limitations are</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:7}" href="#principle-2-you-use-ai-lawfully-ethically-and-responsibly">Principle 2: You use AI lawfully, ethically and responsibly</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:8}" href="#principle-3-you-know-how-to-use-ai-securely">Principle 3: You know how to use AI securely</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:9}" href="#principle-4-you-have-meaningful-human-control-at-the-right-stages">Principle 4: You have meaningful human control at the right stages</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:10}" href="#principle-5-you-understand-how-to-manage-the-full-ai-life-cycle">Principle 5: You understand how to manage the full AI life cycle</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:11}" href="#principle-6-you-use-the-right-tool-for-the-job">Principle 6: You use the right tool for the job</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:12}" href="#principle-7-you-are-open-and-collaborative">Principle 7: You are open and collaborative</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:13}" href="#principle-8-you-work-with-commercial-colleagues-from-the-start">Principle 8: You work with commercial colleagues from the start</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:14}" href="#principle-9-you-have-the-skills-and-expertise-needed-to-implement-and-use-ai-solutions">Principle 9: You have the skills and expertise needed to implement and use AI solutions</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:15}" href="#principle-10-you-use-these-principles-alongside-your-organisations-policies-and-have-the-right-assurance-in-place">Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:16}" href="#understanding-ai">Understanding AI</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:17}" href="#what-is-ai">What is AI?</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:18}" href="#fields-of-ai">Fields of AI</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:19}" href="#applications-of-ai-in-government">Applications of AI in government</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:20}" href="#limitations-of-ai">Limitations of AI</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:21}" href="#building-ai-solutions">Building AI solutions</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:22}" href="#building-the-team">Building the team</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:23}" href="#defining-the-goal">Defining the goal</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:24}" href="#buying-ai">Buying AI</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:25}" href="#using-ai-safely-and-responsibly">Using AI safely and responsibly</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:26}" href="#ethics">Ethics</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:27}" href="#legal-considerations">Legal considerations</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:28}" href="#data-protection-and-privacy">Data protection and privacy</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:29}" href="#security">Security</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:30}" href="#governance">Governance</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:31}" href="#appendix-example-ai-use-cases-in-the-public-sector">Appendix: example AI use cases in the public sector</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:32}" href="#govuk-chat-experimenting-with-generative-ai">GOV.UK Chat: experimenting with generative AI</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:33}" href="#govuk-chat-doing-user-research-for-ai-products">GOV.UK Chat: doing user research for AI products</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:34}" href="#ccs-commercial-agreement-recommendation-system">CCS commercial agreement recommendation system</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:35}" href="#digital-sensitivity-review-at-fcdo-services">Digital Sensitivity Review at FCDO Services</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:36}" href="#nhs-user-research-finder">NHS user research finder</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:37,&quot;index_link&quot;:37}" href="#nhsuk-reviews-an-automated-reviews-moderator">NHS.UK reviews: an automated reviews moderator</a>
        </li>
    </ol>
</nav>

        <div class="gem-c-print-link govuk-!-display-none-print govuk-!-margin-bottom-6">
    <button class="govuk-link gem-c-print-link__button" data-module="print-link" data-print-link-module-started="true">Print this page</button>
</div>
      </div>

    <div class="print-wrapper">
      <div class="meta-data meta-data--display-print">
        <p>
  <img class="meta-data-licence" src="/assets/government-frontend/open-government-licence-min-93b6a51b518ff99714a1aa2a7d2162735c155ec3cb073c75fb88b2a332fa83d3.png">
</p>
<p>
  © Crown copyright 2025
</p>
<p>
  This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visit <a href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3">nationalarchives.gov.uk/doc/open-government-licence/version/3</a> or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: <a href="mailto:psi@nationalarchives.gov.uk">psi@nationalarchives.gov.uk</a>.
</p>
<p>
  Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.
</p>
<p>
  This publication is available at https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html
</p>


      </div>
    </div>

    <div class="govuk-grid-column-three-quarters-from-desktop contents-container">
      <div class="gem-c-govspeak-html-publication">
  <div data-module="govspeak" class="gem-c-govspeak govuk-govspeak gem-c-govspeak--direction-ltr govuk-!-margin-bottom-0" data-govspeak-module-started="true">
    
    
        <div class="govspeak">
<h2 id="acknowledgements">Acknowledgements</h2>

<p>The publication of this playbook has been made possible by the support from a large number of stakeholders.</p>

<h3 id="central-government-departments">Central government departments</h3>

<p>Crown Commercial Service (CCS); Cabinet Office (CO), including the Number 10 Data Science (No.10 DS) and i.AI teams; Department for Business and Trade (DBT); Department for Education (DfE); Department for Environment, Food and Rural Affairs (DEFRA); Department for Energy Security and Net Zero (DESNZ); Department of Health and Social Care (DHSC); Department for Levelling Up, Housing and Communities (DLUHC); Department for Science, Innovation and Technology (DSIT), including the Government Office for Science (GO-Science) and the Responsible Technology Adoption Unit (RTA); Department for Transport (DfT); Department for Work and Pensions (DWP); Foreign, Commonwealth and Development Office (FCDO), including FCDO Services; Government Legal Department (GLD); HM Land Registry (HMLR); HM Revenue and Customs (HMRC); HM Treasury (HMT); Home Office (HO); Ministry of Defence (MoD); and Ministry of Justice (MoJ).</p>

<h3 id="arms-length-bodies-devolved-administrations-and-public-sector-bodies">Arm’s length bodies, devolved administrations and public sector bodies</h3>

<p>Driver and Vehicle Licensing Agency (DVLA); Government Communications Headquarters (GCHQ); Government Internal Audit Agency (GIAA); HM Courts and Tribunals Service (HMCTS); Information Commissioner’s Office (ICO); Met Office; National Health Service (NHS); Office for National Statistics (ONS); and the Scottish Government.</p>

<h3 id="industry">Industry</h3>

<p>Amazon (AWS), Google, IBM and Microsoft.</p>

<h3 id="academic-institutes">Academic institutes</h3>

<p>Alan Turing Institute; BCS, The Chartered Institute for IT; Oxford Internet Institute; Manchester Metropolitan University; and the University of Surrey.</p>

<h3 id="user-research">User research</h3>

<p>User research participants have come from a wide range of departments and have been very generous with their time.</p>

<h2 id="update-log">Update log&nbsp;</h2>

<p>We have made the following core updates since the previous edition:</p>

<ul>
  <li>we have reviewed and updated the 10 principles and the guidance within the <a href="https://www.gov.uk/government/publications/generative-ai-framework-for-hmg">Generative AI Framework</a> to cater for a wider range of AI technologies beyond generative AI</li>
  <li>we have added new sections on topics such as What is AI?, Fields of AI (including a new diagram), User research for AI, AI business cases, Societal wellbeing and public good, Security opportunities, Governance structures for teams, AI quality assurance and Managing risk. We have also included an Appendix with case studies on recent AI projects developed by public sector organisations</li>
  <li>we have reorganised and expanded the Working collaboratively, Using AI safely and responsibly and Ethics sections, to better articulate the importance of engaging with academia, industry and the broader civil society</li>
  <li>we have moved guidance and training on building AI solutions into separate <a href="https://www.gov.uk/government/publications/ai-insights">AI insights articles</a> and <a rel="external" href="https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg">e-learning courses</a> to allow for a more in-depth discussion</li>
</ul>

<h2 id="foreword">Foreword</h2>

<p>I am pleased to introduce the AI Playbook for the UK Government, which updates and expands on the <a href="https://www.gov.uk/government/publications/generative-ai-framework-for-hmg">Generative AI Framework for HMG</a>. This updated guidance will help government departments and public sector organisations harness the power of a wider range of AI technologies safely, effectively, and responsibly.</p>

<p>In January 2025 the government published the <a href="https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan#lay-the-foundations">AI Opportunities Action Plan</a> laying out a bold roadmap for maximising AI’s potential to drive growth and deliver real benefits to people across the UK.</p>

<p>The publication of the AI Playbook highlights the competence and extraordinary work already being done in the AI space across the public sector. Developed collaboratively, with input from many government departments, public sector institutions, academia, and industry, this guidance reflects our commitment to continuously engaging with and learning from wider civil society.&nbsp;</p>

<p>The AI Playbook will support the public sector in better understanding what AI can and cannot do, and how to mitigate the risks it brings. It will help ensure that AI technologies are deployed in responsible and beneficial ways, safeguarding the security, wellbeing, and trust of the public we serve.</p>

<p>The potential of AI to transform public services is enormous, giving us an unparalleled opportunity to do things differently and deliver more with less. AI is already helping civil servants spend less time on repetitive tasks, enabling teachers to personalise lessons, and can allow doctors to access life-saving insights faster, through AI-assisted diagnostics.&nbsp;</p>

<p>However, our journey with AI is just beginning. The AI Playbook is a launchpad that we will continuously revise and improve to help the UK public sector become a leading responsible user of AI technologies. As technology evolves, so too will our approach, ensuring we remain at the forefront of responsible innovation - always guided by the principle that technology must serve people.</p>

<p>I want to extend my sincere thanks to everyone who contributed their expertise to this AI Playbook, both within and beyond government. I look forward to ongoing collaboration as we continue to learn how to use AI safely, responsibly, and effectively to deliver solutions that are smarter, faster, and more responsive to the collective needs of our society.</p>

<p>Feryal Clark MP</p>

<p>Parliamentary Under-Secretary of State for AI and Digital Government</p>

<p>Department for Science, Innovation and Technology (DSIT)</p>

<h2 id="preface">Preface</h2>

<p>When we published the <a href="https://www.gov.uk/government/publications/generative-ai-framework-for-hmg">Generative AI Framework for HMG</a> in January 2024, we described it as ‘incomplete’ and ‘dynamic’. We didn’t pretend to have all of the answers in such a fast-moving field, but did aim to provide helpful, practical guidance to public servants on how to put generative AI to work confidently, responsibly and where it matters most.</p>

<p>The pace of change since we published that first version has not slowed, and interest in generative AI and other forms of AI has grown. The UK government continues to believe that AI has the power to drive productivity, innovation and economic growth. In 2021, the <a href="https://www.gov.uk/government/publications/national-ai-strategy">National AI Strategy</a> set out a 10-year vision for AI, while the 2023 white paper <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">A pro-innovation approach to AI regulation</a> set out the government’s proposals for implementing a proportionate, future-proof and pro-innovation framework for regulating AI. In 2025, the <a href="https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan#lay-the-foundations">AI Opportunities Action Plan</a> highlighted how the government can leverage AI to boost productivity and improve services.</p>

<p>This updated version of the framework has been expanded to cover new developments, and we’ve retitled it to encompass all current forms of AI. As well as a general overview, it includes primers on various AI fields for the curious, and links to learning resources for those who want to dive deeper. The AI Playbook now covers the important and emerging discipline of conducting research with the users of AI systems. It addresses emerging cyber threats to those systems, and the ways that attackers are using AI to create new threats.</p>

<p>I would like to echo Minister Clark’s thanks to everyone who has contributed to this playbook. It has been a collective effort of experts from government departments, arm’s length bodies, other public sector organisations, academic institutions and industry partners. As we continue to advance together in the safe, responsible, and effective use of AI, I look forward to even broader collaboration and further contributions from an expanding community.</p>

<p>David Knott</p>

<p>Government Chief Technology Officer</p>

<p>Department for Science, Innovation and Technology (DSIT)</p>

<h2 id="principles">Principles&nbsp;</h2>

<p>We have defined 10 common principles to guide the safe, responsible and effective use of artificial intelligence (AI) in government organisations. The white paper <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">A pro-innovation approach to AI regulation</a> sets out <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a-implementation-of-the-principles-by-regulators">5 principles</a> for regulators to inform AI development in all sectors. This playbook builds on those principles and defines 10 core principles for AI use in government and public sector organisations.</p>

<ul>
  <li>Principle 1: You know what AI is and what its limitations are</li>
  <li>Principle 2: You use AI lawfully, ethically and responsibly</li>
  <li>Principle 3: You know how to use AI securely</li>
  <li>Principle 4: You have meaningful human control at the right stage</li>
  <li>Principle 5: You understand how to manage the AI life cycle</li>
  <li>Principle 6: You use the right tool for the job</li>
  <li>Principle 7: You are open and collaborative</li>
  <li>Principle 8: You work with commercial colleagues from the start</li>
  <li>Principle 9: You have the skills and expertise needed to implement and use AI</li>
  <li>Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place</li>
</ul>

<p>You can find <a href="https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government">posters on each of the 10 principles</a> for you to display in your government organisation.</p>

<h2 id="principle-1-you-know-what-ai-is-and-what-its-limitations-are">Principle 1: You know what AI is and what its limitations are</h2>

<p>AI is a broad field subject to rapid research and innovation, and many claims have been made about both its promise and risks. You should learn about AI technology to understand what it can and cannot do, and the potential risks it poses.</p>

<p>AI systems currently lack reasoning and contextual awareness and their limitations vary depending on the tools you use and the context in which they operate. AI systems are also not guaranteed to be accurate. You should understand how to use AI tools safely and responsibly, employ techniques to increase the accuracy and correctness of their outputs, and have a process in place to test them.&nbsp;</p>

<p>You can find more about what AI is and its capabilities in the Understanding AI section.</p>

<h2 id="principle-2-you-use-ai-lawfully-ethically-and-responsibly">Principle 2: You use AI lawfully, ethically and responsibly</h2>

<p>AI solutions bring specific legal and ethical considerations. Your use of AI tools must be lawful and responsible. You should seek legal advice on the development and use of AI and engage with compliance, legal and data protection experts in your organisation early in your journey, including during product development. You may, for example, seek advice on equalities implications, fairness, intellectual property and other legal issues.&nbsp;</p>

<p>You should seek data protection advice on your use of AI. This may be from your lawyers or your data protection officer. AI systems can process personal data, so you need to consider how you protect this personal data, be compliant with data protection legislation, and minimise the risk of privacy intrusion from the outset.&nbsp;</p>

<p>You should establish and communicate how you will address ethical concerns throughout your project, from design to deployment, so that diverse and inclusive participation is built into the project life cycle. You should also consider the ethical principles presented in this playbook, and establish robust measures to suit your technological and deployment context.&nbsp;</p>

<p>AI models are trained on data which may include biased or harmful materials. As a result, AI systems may display biases and produce harmful outputs, such as unfair, prejudicial or derogatory representations of groups or individuals. You should consider all potential sources of bias throughout the development life cycle, including unrepresentative data sets and deployment scenarios that have unfair or undesirable impacts.</p>

<p>You must ensure that AI systems generate a positive impact on stakeholders and civil society at large while minimising potential harms as much as possible. When defining and deploying AI systems, you must understand people’s needs and priorities by conducting user research and engaging with the public as appropriate, including civil society groups, underrepresented individuals, those most likely to experience harm, NGOs, academia and industry.&nbsp;</p>

<p>You should understand and manage the environmental impact of the AI systems you are planning to use. You should also use AI technologies only when relevant, appropriate and proportionate. Choose the most suitable and sustainable option for your organisation’s needs.&nbsp;</p>

<p>You can find out more in the Using AI safely and responsibly section.&nbsp;</p>

<h2 id="principle-3-you-know-how-to-use-ai-securely">Principle 3: You know how to use AI securely&nbsp;</h2>

<p>When building and deploying AI services, you must make sure that they are secure to use and resilient to cyber attacks, as laid out in the <a href="https://www.gov.uk/government/publications/government-cyber-security-strategy-2022-to-2030">Government Cyber Security Strategy</a>. Your service must comply with the <a rel="external" href="https://www.security.gov.uk/guidance/secure-by-design/">Secure by Design</a> principles, which were developed by the Central Digital and Data Office (CDDO), and the government’s <a rel="external" href="https://www.security.gov.uk/standards/cyber_standard">Cyber Security Standard</a>.</p>

<p>Different types of AI are susceptible to different security risks. Some threats – such as data poisoning, perturbation attacks, prompt injections and hallucinations – are specific to AI. However, AI systems can also amplify generic risks such as phishing and cyber attacks. You must understand the risks associated with your use of AI and of adversaries potentially using AI against you.</p>

<p>To minimise these risks you should build in safeguards and put technical controls in place. These include security testing and, in the case of generative AI, content filtering to detect malicious activity, as well as validation checks to ensure responses are accurate and do not leak data.&nbsp;</p>

<p>You can find out more in the Security and Data protection and privacy sections.</p>

<h2 id="principle-4-you-have-meaningful-human-control-at-the-right-stages">Principle 4: You have meaningful human control at the right stages</h2>

<p>You need to monitor the AI’s behaviour and have plans in place to prevent any harmful effects on users. This includes ensuring that humans validate any high-risk decisions influenced by AI and that you have strategies for meaningful intervention.</p>

<p>For applications where instant responses are required and human review is not possible in real time, such as chatbots, it’s important that you ensure human control at other stages of the AI’s development and deployment.</p>

<p>You should fully test the product before deployment, and have robust assurance and regular checks of the live tool in place. Since AI models can sometimes produce unwanted or inaccurate results, incorporating feedback from users is crucial. You should have systems in place that allow users to report issues and prompt a human review.&nbsp;</p>

<p>You can find out more in the Human oversight section.</p>

<h2 id="principle-5-you-understand-how-to-manage-the-full-ai-life-cycle">Principle 5: You understand how to manage the full AI life cycle&nbsp;</h2>

<p>AI solutions, like other technology deployments, have a full product life cycle that you need to understand. You should know how to choose the right tool for the job, be able to set it up and have the right resource in place to support day-to-day maintenance of it. You should also know how to update the system and how to securely close it down at the end of its useful life.</p>

<p>You should understand how to monitor and mitigate for potential drift, bias, and, in the case of generative AI, hallucinations. You should also have a robust testing and monitoring process in place to catch these problems. You should use the <a href="https://www.gov.uk/guidance/the-technology-code-of-practice">Technology code of practice</a> to build a clear understanding of technology deployment life cycles, and understand and use the <a rel="external" href="https://www.ncsc.gov.uk/collection/cloud/the-cloud-security-principles">NCSC cloud security principles</a>.&nbsp;</p>

<p>You should understand the benefits, use cases and other applications that your solution could support across government and the wider public sector. The <a href="https://www.gov.uk/government/publications/knowledge-asset-management-in-government">Rose Book</a> provides guidance on government-wide knowledge assets and <a href="https://www.gov.uk/government/organisations/government-office-for-technology-transfer">The Government Office for Technology Transfer</a> can provide support and funding to help develop government-wide solutions. If you develop a service, you must use the government <a href="https://www.gov.uk/service-manual/service-standard">Service Standard</a>.</p>

<p>You can find out more about development best practices for AI in the Building AI solutions section.</p>

<h2 id="principle-6-you-use-the-right-tool-for-the-job">Principle 6: You use the right tool for the job</h2>

<p>You should select the most appropriate technology to meet your needs. AI is good at many tasks, but there are a wide range of models and products. You should be open to solutions involving AI because they can allow organisations to develop new or faster approaches to the delivery of public services, can provide a springboard for more creative and innovative thinking about policy and public sector problems, and help your team with time-consuming tasks. However, you should also be open to the conclusion that, sometimes, AI is not the best solution for your problem: it may be more easily solved with more established technologies.</p>

<p>When implementing AI solutions, you should select the most appropriate deployment patterns and choose the most suitable model for your use case. You can learn about how to choose the right technologies for your task or project in the Identifying use cases for AI and Use cases to avoid sections.</p>

<h2 id="principle-7-you-are-open-and-collaborative">Principle 7: You are open and collaborative</h2>

<p>There are many teams across government and the wider public sector using or exploring AI tools in their work. You should make use of existing cross-government communities where there is a space to solve problems collaboratively, such as the <a href="https://www.gov.uk/service-manual/communities/artificial-intelligence-community">AI community of practice</a>. You should also engage with other government departments that are trying to address similar issues and reuse ideas, code and infrastructure.&nbsp;</p>

<p>Where possible, you should engage with the wider civil society including groups, communities, and non-governmental, academic and public representative organisations that have an interest in your project. Collaborating with people both inside and outside government will help you ensure we use AI to deliver tangible benefits to individuals and society as a whole. Make sure you have a clear plan for engaging and communicating with these stakeholders at the start of your work.</p>

<p>You should be open with the public about where and how algorithms and AI systems are being used in official duties. If you’re a central government department or an arm’s length body within scope, you’re required to use the <a href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">Algorithmic Transparency Recording Standard (ATRS)</a>. This means you must document information about any algorithmic tools you use in decision-making processes and make it clearly accessible to the public. The ATRS is not a requirement for all arm’s length bodies and other public sector institutions yet, but we still encourage you to use it. You should also clearly identify any automated response to the public. For example, a response generated via a chatbot interface should include something like ‘this response has been written by an automated AI chatbot’.</p>

<p>You can find out more in the Ethics section.</p>

<h2 id="principle-8-you-work-with-commercial-colleagues-from-the-start">Principle 8: You work with commercial colleagues from the start</h2>

<p>AI is a rapidly developing market, and you should get specific advice from commercial colleagues on the implications for your project. Reach out to them early in your journey to understand how to use AI in line with commercial requirements.</p>

<p>You should also work with commercial colleagues to ensure that the expectations around the responsible and ethical use of AI are the same between AI systems developed in-house and those procured from a third party. For example, contracts between the public sector and third parties can be drafted to require transparency from the supplier on the different information categories, as set out in the <a href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">ATRS</a>. You can find out more in the Buying AI section.&nbsp;</p>

<h2 id="principle-9-you-have-the-skills-and-expertise-needed-to-implement-and-use-ai-solutions">Principle 9: You have the skills and expertise needed to implement and use AI solutions</h2>

<p>You should understand the technical and ethical requirements for using AI tools and have them in place within your team.&nbsp;</p>

<p>You and your team should gain the skills needed to use, design, build and maintain AI solutions, keeping in mind that developing bespoke AI solutions and training your own models require different specialist skills to using pre-trained models accessible through application programming interfaces (APIs).&nbsp;</p>

<p>Decision makers, policy professionals and senior responsible owners (SROs) should gain the skills they need to understand the risks and opportunities of AI, including its potential impact on organisational culture, governance, ethics and strategy.</p>

<p>You should take the free <a rel="external" href="https://learn.civilservice.gov.uk/search?q=Generative+AI">AI courses on Civil Service Learning</a> and proactively keep track of developments in the field. You can find out more in our Acquiring skills and talent section.</p>

<h2 id="principle-10-you-use-these-principles-alongside-your-organisations-policies-and-have-the-right-assurance-in-place">Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place</h2>

<p>These principles and this playbook set out a consistent approach for the UK government to use AI tools. While you should use these principles when working with AI, many government organisations have their own governance structures and policies in place. You should follow any organisation-specific policies, especially ones about security and data handling.&nbsp;</p>

<p>You should understand, monitor and mitigate the risks that using AI tools can bring. Connect with the right assurance teams in your organisation early in the project life cycle for your AI solutions. You should have clearly documented review and escalation processes in place, and have an AI review board or programme-level board.&nbsp;</p>

<p>You can find out more in the Governance section.</p>

<h2 id="understanding-ai">Understanding AI&nbsp;</h2>

<p>This section explains what AI is, what its main fields are, the applications of AI and generative AI in government and their limitations. It supports Principle 1: You know what AI is and what its limitations are.</p>

<h2 id="what-is-ai">What is AI?</h2>

<p>AI is not new. The term ‘artificial intelligence’ was coined in 1956 during the <a rel="external" href="https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth">Dartmouth workshop</a>, a gathering of scientists intent on exploring the potential of computing to emulate human reasoning. Since then, there have been recurring waves of progress and excitement, followed by periods of waning interest and investment referred to as ‘AI winters’. We use the <a rel="external" href="https://oecd.ai/en/wonk/ai-system-definition-update">definition of AI adopted by OECD countries</a>:</p>

<blockquote>
  <p class="last-child">An AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.</p>
</blockquote>

<p>The UK government’s paper <a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement">Establishing a pro-innovation approach to regulating AI</a> suggests that these systems are ‘adaptable’ because they can find new ways to meet the objectives set by humans, and ‘autonomous’ because, once programmed, they can operate with varying levels of autonomy, including without human control.</p>

<h2 id="fields-of-ai">Fields of AI</h2>

<p>AI comprises a complex and evolving set of fields. These include a broad, interconnected range of algorithms, models and processes. Advances in one area typically propagate throughout these networks of technologies, conferring novel behaviours and increases in accuracy and capability to ancestor models.</p>

<p>The following diagram illustrates the interdependencies between some existing and emerging fields of AI. As you can see, capturing the evolving complexity of AI is a challenging task.</p>

<figure class="image embedded"><div class="img"><img src="https://assets.publishing.service.gov.uk/media/67a376fab74b3d9dfe36ca9d/Image_1.jpg" alt=""></div></figure>

<p>The complexity of the AI space continues to increase: as AI technologies evolve they often branch out, advancing some research areas while leaving others unchanged. For example, in the field of computer vision (CV) some legacy systems still thrive today due to the simplicity of their behaviours, their low computational and memory requirements, and their stability and well-proven performance. These legacy CV systems fall within the domain of machine learning (ML). However, more recent and capable CV systems, such as those used in self-driving or medical imaging systems, are considerably more complex and belong to CV as part of deep learning (DL).</p>

<p>As software and algorithms are advancing, so too is hardware infrastructure. Traditional computer processing units (CPUs) can struggle with the large amounts of data and calculations AI requires. New types of hardware have been implemented to train and run AI models faster and more efficiently. For example, the largest modern AI systems (ranging in the hundreds of billions to trillions of parameters) are trained on networks of thousands of graphics processing units (<a rel="external" href="https://aws.amazon.com/what-is/gpu/">GPUs</a>), while Tensor Processing Units (<a rel="external" href="https://cloud.google.com/tpu/docs/intro-to-tpu">TPUs</a>) and Neural Processing Units (<a rel="external" href="https://support.microsoft.com/en-gb/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4">NPUs</a>) are increasingly used to optimise ML training.</p>

<p>Below we provide an overview of the main fields of AI. For a deeper understanding of these fields you can:</p>

<ul>
  <li>take the <a rel="external" href="https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg">free e-learning courses available</a> on Civil Service Learning</li>
  <li>use the other training opportunities mentioned in the Acquiring skills and talent section</li>
  <li>refer to the <a href="https://www.gov.uk/government/publications/ai-insights">AI insights articles</a>&nbsp;</li>
</ul>

<h3 id="neural-networks">Neural networks</h3>

<p>Neural networks (NNs), or artificial neural networks (ANNs), are a computational model inspired by biological NNs in the human brain. They were <a rel="external" href="https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html">initially created as an aspect of ML</a> in the 1940s but they may now be considered as components of more elaborate DL systems.</p>

<h4 id="how-do-nns-work">How do NNs work?</h4>

<p>An NN learns through exposure to data in training cycles. During these cycles, the model gradually adjusts the connections between different parts of the network, setting the weights between nodes of adjacent layers. When the model makes an error, the network calculates it as data and checks how far off it was. The error is then adjusted backwards through the network by updating the weights by a very small amount (the <a rel="external" href="https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate">learning rate</a>) in the direction that will serve to minimise that error. Repeated executions of this process during the training phase tune the network so that it can generalise faithfully on related data it has never seen before.&nbsp;</p>

<h4 id="applications-of-nns">Applications of NNs</h4>

<p>NNs have a wide range of applications due to their ability to recognise patterns, process new data, make predictions and improve over time. NNs are often deployed in image recognition applications, medical imaging, speech recognition models, autonomous systems, and in many other examples.</p>

<h3 id="machine-learning">Machine learning</h3>

<p>ML is the branch of AI that learns from data. It does this by extracting features from data and learning the relationships between those features. Anywhere there is data, there is an opportunity to learn from it. ML can provide the public sector with unique and quantifiable insights into data that were previously impossible, expensive, or of limited or short-term utility.</p>

<h4 id="how-does-ml-work">How does ML work?</h4>

<p>ML uses algorithms to analyse data, learn patterns, and then make predictions or decisions based on new data. To work effectively, ML systems must be trained using carefully selected information. This training helps create an optimised model that identifies pertinent features in the data, and weights their relationships to understand how they relate to each other.</p>

<p>ML systems can be trained using either ‘labelled’ or ‘unlabelled’ data, or a combination of both. Labelled data is data that has been tagged or categorised in advance. This form of training is usually called ‘supervised learning’ because the model is trained under the supervision of labelled data. Each example in the training data set comes with a correct answer, or label, from which the model is supposed to learn. When unlabelled data is used, the model is programmed to identify patterns, groupings or structures on its own. This is often referred to as ‘unsupervised learning’.</p>

<h4 id="applications-of-ml">Applications of ML</h4>

<p>The ability of ML models to learn from data, identify changes, and update based on changes in underlying data enables a host of capabilities that would simply not be possible through traditional linear, deterministic systems. ML models can be used as elements of much larger DL models, or as data processing participants in a sequence of operations, and they underpin many applications including fraud detection, feedback analysis, image processing, summarisation, and more. Modern large language models (LLMs) are also examples of ML systems.</p>

<h3 id="deep-learning">Deep learning</h3>

<p>DL is a subset of ML that involves more complex model structures and architectures, including sophisticated NNs, to learn complex patterns and representations from large amounts of data. DL has complemented and expanded upon ML, but ML remains the first-choice candidate for simpler tasks and can be easier to implement with less data and lower computational power. DL, while more resource-intensive, excels in handling complex tasks and larger data sets.</p>

<h4 id="how-does-dl-work">How does DL work?</h4>

<p>DL models initially detect simple features – edges in an image, for example – and gradually combine them to recognise more complex patterns, such as identifying a face or understanding speech.</p>

<h4 id="applications-of-dl">Applications of DL</h4>

<p>DL is used in advanced applications like biomedical research and autonomous driving, but also in image and speech recognition (SR), natural language processing (NLP), personalised recommendations, and more.</p>

<h3 id="speech-recognition">Speech recognition</h3>

<p>SR, sometimes referred to as automatic speech recognition (ASR), is a field of ML dedicated to processing speech. SR includes both systems that convert speech into text (STT) and new speech-to-speech (S2S) systems.&nbsp;</p>

<h4 id="how-does-sr-work">How does SR work?</h4>

<p>Computers only understand numbers, so the challenge with SR is to turn spoken words into numbers while keeping their meaning and context. To do so, SR models first convert speech into numeric representations called spectrograms, which show sound frequencies over time. DL models then analyse these spectrograms to identify sounds, words or sentences. In STT, this process results in written text. In S2S, the recognised words are further processed by another model that translates the text into a different language, which is then turned back into spoken words.</p>

<h4 id="applications-of-sr">Applications of SR</h4>

<p>SR has expanded considerably over recent years, especially in terms of voice recognition. This technology is now used in banking, personal agents like Siri and Alexa, and general SR functionality built into phones and cars, among other things. Modern SR applications include call analytics, emergency services, meeting summarisation and media subtitling, and more.</p>

<h3 id="computer-vision">Computer vision</h3>

<p>CV is a field of AI and ML that enables computers to interpret, analyse and understand visual information (images and videos) to perform tasks such as object recognition, facial recognition and scene understanding. Although CV has been researched since the 1960s, it has progressed considerably in recent decades due to a combination of developments in model architecture, hardware performance, and data quality and volumes.</p>

<h4 id="how-does-cv-work">How does CV work?&nbsp;</h4>

<p>CV systems are intended to make image data comprehensible and interpretable so that it can be consumed and evaluated appropriately. These evaluations most commonly include classification, object detection, video analysis and image segmentation.&nbsp;</p>

<p>To perform these tasks, CV ​​uses algorithms to break down images into pixels and process the pixels to detect edges, shapes and colours. The system then uses this information to recognise objects, people or scenes – similarly to how our brains process visual information – allowing computers to identify the content of images.</p>

<h4 id="applications-of-cv">Applications of CV</h4>

<p>CV has many applications in fields such as facial recognition, quality control, healthcare and medical imaging, surveillance, robotics, and more.&nbsp;</p>

<h3 id="natural-language-processing">Natural language processing</h3>

<p>NLP is a field within AI that focuses on processing human language. NLP combines computational linguistics and machine learning to analyse large amounts of natural language data and comprehend, interpret, translate, and generate language and language-related data. LLMs, which are in widespread use, are a subset of NLP, with NLP <a rel="external" href="https://georgetownlawtechreview.org/natural-language-processing/GLTR-11-2016/">preceding them by several decades</a>.</p>

<h4 id="how-does-nlp-work">How does NLP work?</h4>

<p>NLP uses algorithms to convert text into numerical representations that are then processed by an ML model. This process involves a series of steps including noise reduction, tokenisation, stop-words removal, stemming or lemmatisation, vectorisation, and embeddings. The model uses the weights learned during training to adjust the importance of different features in the input data, and considers various elements, including the order and context of words, to understand the overall meaning of the text and produce the required outputs.</p>

<h4 id="applications-of-nlp">Applications of NLP</h4>

<p>NLP is used for a wide variety of applications and tasks related to language, including machine translation, document classification, sentiment analysis, parts-of-speech tagging, named-entity recognition, text summarisation and conversational AI.</p>

<p>For government departments, NLP can play an important role in managing and processing large volumes of human language data from sources like emails, letters and online forms, enabling efficient and meaningful analysis.</p>

<h3 id="generative-ai">Generative AI</h3>

<p>Generative AI is a subset of AI capable of generating text, images, video or other forms of output by using probabilistic models trained across various domains. Generative AI learns from large amounts of specially curated training data to discern and replicate complex patterns and structures. The output of generative AI models mimics the characteristics learned from the training data, enabling a range of novel applications. These include personalised content generation, advanced analysis and evaluation, and aiding creative processes.</p>

<p>Examples of publicly accessible generative AI tools are ChatGPT, Claude, Gemini and Dall-E. Generative AI is also becoming increasingly integrated into mainstream products. Examples include the Adobe Photoshop Generative Fill tool, AWS ChatOps Chatbot, Microsoft 365 Copilot and Google Duet AI.</p>

<h4 id="how-does-generative-ai-work">How does generative AI work?</h4>

<p>Generative AI uses large quantities of carefully selected data to train models so they may learn the underlying patterns and structure of data. A well-known example of generative AI is LLMs, which are large neural networks specifically trained on text and natural language data to generate high quality, text-based outputs. However, there are many other generative AI models that are not purely text based.</p>

<p>Once trained, generative AI models are capable of generating new content consistent with the features and relationships learned from the training data. When a user provides a prompt or input to an LLM, the model evaluates the probability of potential responses based on what it has learned from its training data. It then selects and presents the response that is likeliest to be the right fit for the given prompt.</p>

<h4 id="applications-of-generative-ai">Applications of generative AI</h4>

<p>The applications of generative AI go beyond the simple generation of new text or images. For example, generative AI is used in medicine to test molecular structures for drug discovery, and in the financial sector to generate multimodal data surfaces approximating trading and market conditions in order to test safety, security, trading and trade surveillance models. Generative Adversarial Networks (GANs) are also used for creating synthetic data sets to train ML models, as well as in other areas such as <a rel="external" href="https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained">deepfake</a> detection, self-driving model ensembles, and text-to-image synthesis. You can explore more applications of generative AI in the Applications of AI in government section.</p>

<p>For more on how to build, fine-tune and use generative AI solutions, refer to our series of <a href="https://www.gov.uk/government/publications/ai-insights">AI insights</a>.</p>

<h3 id="agentic-ai">Agentic AI</h3>

<p>Agentic AI refers to autonomous AI systems that can make decisions and perform actions with minimal human intervention. These agents are capable of understanding their environment, identifying the set of tools and functions at their disposal, and using those to take actions to achieve their objectives.</p>

<p>Agent-based systems use Foundation Models (primarily LLMs) to match their capabilities with their objectives. For example, an order processing system may have multiple agents autonomously capturing pricing and market-related data. When a request for an order is raised, the agentic system may utilise those prices or, seeing that they haven’t been updated recently, may use another agent to retrieve the latest price available.This all happens automatically in the background because the system knows which agents can perform each task.</p>

<p>These systems provide a simpler way to build powerful AI-driven solutions by focusing on the capabilities of each agent and letting the AI model itself figure out the best way to achieve the system’s objectives. The high level of abstraction involved in this technology makes it easier to create more efficient and effective systems that take full advantage of AI.</p>

<h3 id="ethics-and-societal-impact">Ethics and societal impact</h3>

<p>As research developments have increased the presence of AI in the public sector and everyday life, ethical and societal considerations remain essential to the adoption and use of AI.</p>

<p>Applications of AI in government offer important benefits, such as improving productivity and enhancing access to services. However, AI systems also have limitations and can have two-sided impacts if not used in responsible ways and with the appropriate safeguards in place.</p>

<p>You can read more on the legal, ethical and security implications of AI in the Using AI safely and responsibly section.</p>

<h2 id="applications-of-ai-in-government">Applications of AI in government</h2>

<p>AI has a broad range of capabilities and has been relevant to the work of government for many years. The ability of generative AI to process language and produce new text, images and code has further enhanced the potential applications of AI technology.&nbsp;</p>

<p>You can find an overview of how government bodies are using AI – including the central government’s plans for supporting the adoption of AI – in the report on the <a rel="external" href="https://www.nao.org.uk/reports/use-of-artificial-intelligence-in-government/">Use of artificial intelligence in government (2024)</a> by the National Audit Office (NAO).&nbsp;</p>

<p>The following table presents some examples of potential applications of both AI and generative AI technologies in government.</p>

<table>
  <tbody>
    <tr>
      <td>Application</td>
      <td>AI</td>
      <td>Generative AI</td>
    </tr>
    <tr>
      <td>Speed up the delivery of services</td>
      <td>Machine learning (ML) and optical character recognition (OCR) algorithms can support the processing of thousands of handwritten letters per day, reducing response times.</td>
      <td>Can retrieve relevant organisational information faster to answer digital queries or route email correspondence to the right parts of the business.</td>
    </tr>
    <tr>
      <td>Reduce staff workload</td>
      <td>AI technologies for facial recognition and data analytics allow for the automatic control of passports in airports, freeing staff from this task.</td>
      <td>Can suggest first drafts of routine email responses or computer code, acting as an autocomplete tool for algorithms.</td>
    </tr>
    <tr>
      <td>Perform complicated tasks</td>
      <td>ML can analyse very large data sets, identify trends and anomalies in complex historical data, and support data-driven decision making.</td>
      <td>Can help review and summarise huge amounts of information, as well as identify and correct errors in long algorithms.</td>
    </tr>
    <tr>
      <td>Perform specialist tasks more cost-effectively</td>
      <td>Predictive analytics can identify future needs for resources, optimising budget allocations and planning. ML can detect fraudulent activities in different fields, preventing financial losses and protecting against cyber threats.</td>
      <td>Can summarise documentation containing specialist language like financial or legal terms, or translate a document into several different languages.</td>
    </tr>
    <tr>
      <td>Improve the quality of services</td>
      <td>Recommender systems can help users navigate government web pages and find the information they need. AI can also analyse thousands of feedback messages and suggest service improvements.</td>
      <td>Can improve the readability and accessibility of information on web pages. For example, by simplifying complex language, improving formatting and generating alternative text for images.</td>
    </tr>
  </tbody>
</table>

<p>The Identifying use cases for AI section can help you select appropriate applications of AI tools. For real-life examples of some of these applications, refer to the AI use cases in the public sector appendix.</p>

<p>However, AI systems still have limitations. You should make sure that you understand these and build appropriate testing and controls into any AI solutions.</p>

<h2 id="limitations-of-ai">Limitations of AI</h2>

<p>The capabilities of AI are improving over time. However, they do not provide the answer to every problem. Some of the limitations of AI systems are:</p>

<ul>
  <li>bias: AI systems lack consciousness and their outputs tend to replicate the bias present in the data they were trained on. For example, their performance can be affected by model bias – an innate deviation in the model giving rise to an error between predicted and actual values – and algorithmic bias – which refers to systematic inequality in outcome from a model. For more information, refer to the <a rel="external" href="https://assets.publishing.service.gov.uk/media/60142096d3bf7f70ba377b20/Review_into_bias_in_algorithmic_decision-making.pdf">Review into bias in algorithmic decision-making (PDF, 9.9MB)</a> and the <a rel="external" href="https://assets.publishing.service.gov.uk/media/65ccf508c96cf3000c6a37a1/Introduction_to_AI_Assurance.pdf">Introduction to AI assurance (PDF, 1,149KB)</a> guidance. There is more about the ethical implications of bias in the Fairness, bias and discrimination section</li>
  <li>data quantity and quality: AI systems heavily rely on the quality and quantity of data to drive accuracy. Insufficient data can lead to the model failing to generalise effectively, and, similarly, poor quality, biased or noisy data can lead to inaccurate outcomes if not managed correctly</li>
  <li>accuracy: it’s difficult to produce an AI system which provides 100% accurate outputs under all conditions. You must be clear about what objective measures you’re assessing the AI outputs against and any factors that impact them&nbsp;</li>
  <li>transparency and explainability: some AI models – for example, deep learning architectures and convolutional or recurrent neural networks – can be so sophisticated that it is challenging to trace how a specific input leads to a specific output</li>
  <li>cost and sustainability: depending on the tool you use, AI implementation can be complex, time consuming and have considerable compute costs. Ongoing investment is needed to maintain these systems, update them with new data if required, and ensure their performance remains stable over time. It’s important to consider the sustainability impact of deploying AI systems and check if there are better alternatives</li>
</ul>

<p>Generative AI also has further specific limitations. You need to be aware of these limitations and have checks and assurance in place when using generative AI in your organisation.</p>

<ul>
  <li>hallucination (confabulation): large language models (LLMs) and other probabilistic generative models are vulnerable to creating content that appears plausible but may actually be factually incorrect, as they generate content by returning the most likely output for a given input based on the patterns learnt from the data they were trained on, without an actual understanding of the content</li>
  <li>lack of critical thinking, personal experience and judgement: although some LLM-based AI systems can give the appearance of reasoning and their outputs may appear as if they come from a person, they are in no way sentient</li>
  <li>sensitive or inappropriate context: LLMs can generate offensive or inappropriate content if not properly guided, since they can replicate any bias or toxic material present in the data they were trained on</li>
  <li>
    <p>domain expertise: LLMs are not domain experts. They are not a substitute for professional advice, especially in legal, medical or other critical areas where precise and contextually relevant information is essential</p>
  </li>
  <li>dynamic real-time information retrieval: some well-known LLMs such as ChatGPT, Gemini and Copilot are now able to include access to real-time internet data in their results. But there are still many LLMs that do not have real-time access to the internet or data outside their training set</li>
</ul>

<h2 id="building-ai-solutions">Building AI solutions</h2>

<p>​​This section outlines the practical steps you’ll need to take when building AI solutions, including defining the goal, building the team, managing business change, buying AI and building the solution. It supports:&nbsp;</p>

<ul>
  <li>Principle 1: You know what AI is and what its limitations are</li>
  <li>Principle 3: You know how to use AI securely</li>
  <li>Principle 4: You have meaningful human control at the right stage</li>
  <li>Principle 5: You understand how to manage the AI life cycle</li>
  <li>Principle 6: You use the right tool for the job</li>
  <li>Principle 8: You work with commercial colleagues from the start</li>
  <li>Principle 9: You have the skills and expertise needed to implement and use AI</li>
  <li>Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place</li>
</ul>

<p>However, following the guidance in this section is only part of what is needed to build AI solutions. You also need to make sure that you’re using AI safely and responsibly.&nbsp;</p>

<h2 id="building-the-team">Building the team</h2>

<p>Like any other digital service, developing AI projects requires more than just technology. You will need to work collaboratively in a multidisciplinary team with diverse expertise.</p>

<p>The government Service Manual offers <a href="https://www.gov.uk/service-manual/the-team/set-up-a-service-team">step-by-step guidance on how to set up a service team</a> for the development and management of digital services. At the outset of your AI project, you need to form a service team and identify key collaborations with other teams and departments.</p>

<p>Your minimum viable AI team must be able to:</p>

<ul>
  <li>identify <a href="https://www.gov.uk/service-manual/user-centred-design/user-needs.html">user needs</a> and <a href="https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction#meeting-government-accessibility-requirements">accessibility requirements</a>&nbsp;</li>
  <li>manage and report to stakeholders and other teams, collaborating with different field experts</li>
  <li>design, build, test and iterate AI products, using <a href="https://www.gov.uk/service-manual/agile-delivery">agile methodologies</a>
</li>
  <li>ensure the responsible development of lawful, ethical, secure and safe-by-design AI services</li>
  <li>be able to collect, process, store and manage data ethically, safely and securely</li>
  <li>test with real users and measure the performance of the service</li>
  <li>support the live running of the service, iterate and retire it</li>
</ul>

<p>You can use the <a rel="external" href="https://ddat-capability-framework.service.gov.uk/">Government Digital and Data Profession Capability Framework</a> to identify skill gaps in your team, forecast workforce needs and create effective and consistent job adverts. Be aware that your capability needs will change during the project life cycle: use the Service Manual to understand the <a href="https://www.gov.uk/service-manual/the-team/set-up-a-service-team#the-service-team-you-need-in-each-phase">roles you will need in the different phases of your project</a>.</p>

<p>You will need the right balance between technical and domain expertise, depending on the nature of your project. As well as building a team that contains the right skills, make sure it includes a diversity of groups and viewpoints to help you stay alert to risks of bias and discrimination. Your team should include or be able to collaborate with:</p>

<ul>
  <li>business leaders and experts who understand the context and impact on users and services</li>
  <li>data scientists who understand the relevant data, how to use it effectively, and how to build and train and test models</li>
  <li>software engineers who can build and integrate solutions</li>
  <li>user researchers and designers who can help understand user needs and design compelling experiences</li>
  <li>legal, commercial and security colleagues, as well as ethics and data privacy experts, who can help you make your AI solution safe and responsible</li>
</ul>

<p>Considering the current shortage of AI talent, consider adopting strategies that combine new hires, <a href="https://www.gov.uk/service-manual/the-team/working-contractors-third-parties">working with contractors or third parties</a>, and internal upskilling.&nbsp;</p>

<h3 id="acquiring-skills-and-talent">Acquiring skills and talent</h3>

<p>The professional skills required for working in the digital space are outlined in the <a href="https://www.gov.uk/government/collections/digital-data-and-technology-profession-capability-framework#it-operations-job-family">Government Digital and Data Profession Capability Framework</a>. You can find more information on building a digital and data team in the government <a href="https://www.gov.uk/service-manual">Service Manual</a>.</p>

<p>As AI skills are in high demand, their inclusion in the government’s talent strategy is critical. Currently, the Government Digital and Data Profession’s approach involves integrating AI modules and learning materials into existing talent offerings. You can also source AI skills through senior digital secondments and internal collaboration with allied professions such as data engineering.&nbsp;</p>

<p>Raising general awareness of AI throughout the Civil Service and upskilling current digital and data professionals on AI remains crucial. Every civil servant can take up to <a href="https://www.gov.uk/guidance/training-and-development-opportunities-in-the-the-civil-service#building-skills-and-competencies">5 days of learning per year</a> and some departments allow more days than others. It’s important that the members of your team have the necessary time and resources to update their knowledge and skills. Data, analytics and digital professionals can also benefit from having sandboxes and spaces to experiment with AI in a safe and secure way.</p>

<p>The cross-government user research on AI conducted by the Central Digital and Data Office (CDDO) in 2023 and 2024 identified the 5 groups of learners below.</p>

<ol>
  <li>Beginners: civil servants who are new to AI and need to be familiar with its concepts, benefits, limitations and risks to be able to work with AI tools safely, responsibly and efficiently. Learning for this audience should focus on having an improved understanding of AI.</li>
  <li>Technical roles outside government digital and data: civil servants in roles such as operational delivery and policy profession who are likely to use AI in their work for tasks such as information retrieval and text or image generation. Learning for this audience should provide the necessary knowledge and skills to make effective and responsible use of appropriate AI tools – with particular attention given to generative AI, its limitations and prompt engineering.</li>
  <li>Data and analytics professionals: civil servants who work on the collection, organisation, analysis and visualisation of data, with varying levels of expertise in AI. Learning for this audience should advance their understanding of AI with a focus on the implementation and use of AI to facilitate automated data analysis, the synthesis of complex information, and the generation of predictive models.&nbsp;</li>
  <li>Digital professionals: civil servants with advanced digital skills who are likely to work on the development of AI solutions in government. Learning for this audience should address the technical aspects and implementation challenges associated with fostering AI innovation, and provide opportunities to collaborate with leading industries and academic institutions.&nbsp;</li>
  <li>Leaders: decision makers and senior civil servants who are responsible for creating an AI-ready culture in government. Learning for this audience should provide resources and workshops that are accessible to non-digital and data professionals and focus on the latest trends in AI, including its potential impact on organisational culture, governance, ethics and strategy.</li>
</ol>

<p>The AI Policy Directorate in the Department for Science, Information and Technology (DSIT) has published, in collaboration with <a rel="external" href="https://www.ukri.org/councils/innovate-uk/">Innovate UK</a> and the <a rel="external" href="https://www.turing.ac.uk/">Alan Turing Institute</a>, <a rel="external" href="https://iuk.ktn-uk.org/news/ai-skills-for-business-guidance-feedback-consultation-call-from-the-alan-turing-institute/">AI Skills for Business Guidance</a>. While designed for business, this guidance offers a helpful overview of the knowledge and skills needed to harness the opportunities and navigate the practical challenges of AI.</p>

<h4 id="learning-resources">Learning resources</h4>

<p>We’ve launched a series of free online learning resources for all civil servants. The resources cover multiple topics:</p>

<ul>
  <li>
<a rel="external" href="https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg">fundamentals of AI and generative AI</a>, including their capabilities and limitations</li>
  <li><a rel="external" href="https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw">understanding AI ethics</a></li>
  <li><a rel="external" href="https://learn.civilservice.gov.uk/courses/oPEgywmEQumlVAmXlgnRqw">the business value of AI</a></li>
  <li>an overview of the main <a rel="external" href="https://learn.civilservice.gov.uk/courses/P1HGnf2xQqeJraSkweLvXg">generative AI tools and applications</a>
</li>
  <li>courses on <a rel="external" href="https://learn.civilservice.gov.uk/courses/hSsuur6CR-K_-QsXhUxO0Q">working with large language models</a>, <a rel="external" href="https://learn.civilservice.gov.uk/courses/Bv0vdvyaQ7GuzX2Qgg46oA">machine learning and deep learning</a>, <a rel="external" href="https://learn.civilservice.gov.uk/courses/yBJYpdlUSEmpVWi1YLImUw">natural language processing and speech recognition</a> and <a rel="external" href="https://learn.civilservice.gov.uk/courses/OMT6z8o0S9yhQaIP8dbTQg">computer vision</a>
</li>
  <li>a <a rel="external" href="https://learn.civilservice.gov.uk/courses/tULvvOtbQaGmjLo9bazHtQ">technical curriculum</a> with courses leading to certificates in different AI fields&nbsp;</li>
</ul>

<p>AI courses are freely <a rel="external" href="https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg">available within Civil Service Learning</a>, as well as a series of <a rel="external" href="https://prospectus.governmentcampus.co.uk/04-artificial-intelligence/">AI courses from the Government Campus</a> which can be accessed through the learning frameworks.</p>

<p>Senior civil servants, grade 7s and grade 6s, can also sign up to the AI course within <a rel="external" href="https://cddo.blog.gov.uk/2023/01/11/the-digital-excellence-programme/">The Digital Excellence Programme</a>. This was designed by CDDO and the Government Skills and Curriculum Unit (GSCU) to support leaders outside of the digital and data profession to become pioneers of the government’s digital transformation.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations">Practical recommendations</h4>

  <ul>
    <li>Make full use of the training resources available, including those on <a rel="external" href="https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg">Civil Service Learning</a>.</li>
    <li>Build a multidisciplinary team with all the expertise and support you need.</li>
    <li>Use multiple talent acquisition strategies and upskill current digital and data professionals in your team.</li>
    <li>Consider providing data, analytics and digital professionals with sandboxes and safe spaces to experiment with AI securely.</li>
  </ul>
</div>

<h3 id="working-collaboratively">Working collaboratively</h3>

<p>You need to work collaboratively to develop effective AI solutions. Your team should establish and maintain relationships with internal and external stakeholders with varying levels of familiarity with AI technologies.</p>

<p>You must be ready to collaborate with <a href="https://www.gov.uk/service-manual/the-team/set-up-a-service-team#working-with-other-teams-in-your-organisation">other teams in your organisation</a> to address occasional knowledge and capability gaps in your team. Building AI solutions requires industry knowledge, whereas scaling and deploying these solutions will require considerations from software and/or technical infrastructure teams. Engaging these teams early on creates clear requirements and parameters for the proposed solutions, and sets a path for success.&nbsp;</p>

<p>To avoid siloed approaches and duplication of work, look for opportunities for cross-government and industry events that will help your team stay updated on the latest developments and best practices.&nbsp;</p>

<p>For example, consider joining the <a href="https://www.gov.uk/service-manual/communities/artificial-intelligence-community">AI community of practice</a>. This offers monthly meetings for people working on or interested in AI in the public sector. You can sign up to this community by using <a rel="external" href="https://www.us14.list-manage.com/subscribe?u=751cf84762295209ed8291813&amp;id=db38e5da8d">the dedicated form</a>.&nbsp;</p>

<p>Depending on the nature of your project, you should also consider <a href="https://www.gov.uk/government/news/whitehall-set-to-bring-in-ai-and-data-experts-under-plans-to-turbocharge-productivity">working with industry experts and academic institutions</a> to foster knowledge exchange and access to cutting-edge research. Additionally, engaging with broader civil society will help you understand people’s values, concerns, and priorities, ensuring your AI solution meets the needs of the public we serve.</p>

<h2 id="defining-the-goal">Defining the goal&nbsp;</h2>

<p>Like all technology, using AI is a means to an end – not an objective in itself. Whether planning your first use of AI or a broader transformation programme, you should be clear on:</p>

<ul>
  <li>the goals you want to achieve</li>
  <li>the needs of your users</li>
  <li>where you can most effectively use AI technologies, and where you should avoid them entirely</li>
</ul>

<p>For example, you can use AI to automate and streamline processes, optimise the use of resources, foster data-driven decision making, and improve the quality and accessibility of some services. Goals for the use of generative AI may include improved public services or productivity, increased staff satisfaction, cost savings or risk reduction.</p>

<p>The sections below will help you understand when AI might be the right tool for your project.</p>

<h3 id="user-research-for-ai">User research for AI</h3>

<p>User research (UR) can be critical to the success of your AI project. It is an important mechanism for engaging with people both inside and outside government to understand their needs, values and priorities. It provides key insights into the way the humans who will use the product or service behave, think and feel. This insight helps us ensure we use AI to deliver tangible benefits to individuals and society as a whole. It’s an exciting opportunity to collaborate on the design of an experimental approach, including the development of metrics, which will support the continuous improvement of your AI solution.</p>

<p>Doing UR for an AI project helps you keep the human in the loop and understand the human intelligence that the AI will replicate or imitate. Through UR, you can observe how people complete tasks and solve problems, understand their attitudes, and uncover the cultural issues that may impact the adoption and use of an AI solution. These insights will help you to identify where and how a human needs to be involved. You can find information about <a href="https://www.gov.uk/service-manual/user-research">how to do user research</a> in the government Service Manual.</p>

<p>Developing AI products and services involves some specific activities that user research can be particularly useful for. Some of these are critical to your AI project and some supplement the existing data science process. These include:</p>

<ul>
  <li>understanding if AI is the right tool: do early UR to observe and speak with people who are involved in processes, whether these are internal users or the general public. This can help you understand users’ problems and aspirations. At this stage, a user researcher can work with a business analyst to find efficiencies in business processes and measure dissatisfaction. This will help establish a baseline of metrics to measure your project outcomes against. UR insights can also help you to work out an appropriate level of model accuracy when working with subjective outputs, and understand biases in the existing human process&nbsp;</li>
  <li>defining performance metrics: user researchers can work with analytical colleagues to use insights from research to define model and system metrics and methodologies for measuring ongoing performance&nbsp;</li>
  <li>preparing data: understand what data your users use, and how they think and talk about an activity. This will help you find, categorise and summarise appropriate training data. User researchers can help with supervised learning if you need human judgement to produce correct labelling of your training data. To consider how bias can affect labelling, refer to the Fairness, bias and discrimination section</li>
  <li>synthesising data: user researchers can help review and assess the appropriateness of generated synthetic data by developing and managing a process for bringing subject matter experts (SMEs) into the loop. If you’re planning to use synthetic data, check the UK Statistic Authority’s <a rel="external" href="https://uksa.statisticsauthority.gov.uk/publication/ethical-considerations-relating-to-the-creation-and-use-of-synthetic-data/pages/2/#:~:text=Synthetic%20data%20are%20an%20artificial,re%2Didentification%20is%20almost%20impossible.">Ethical considerations relating to the creation and use of synthetic data</a>
</li>
  <li>evaluating your model’s output: as well as methods like statistical evaluation, it’s good practice to test the accuracy and relevance of your model’s outputs with a sample of your users. Methods such as reinforcement learning from human feedback (RLHF) are important to consider when training models – for example, when ranking different outputs. Humans can be particularly helpful when working on tasks that are difficult to specify but easy to judge, such as producing text that lacks bias, toxicity or other harmful content</li>
  <li>measuring usability of the product: observing how users respond to and use the outputs of your AI system will help you understand if they can use it to confidently complete their tasks. Design this testing into the way the AI solution is monitored, so that you can better understand data drift through user behaviour and changes over time</li>
  <li>understanding attitudes to the product or system: UR will help you understand levels of trust, confidence, and how the solution is used for decision making. These insights are useful to understand why model metrics might look good but service metrics are less positive. Model metrics measure how well your technology is performing, whereas service metrics help you understand if users’ needs and business goals are being met, which can be very different. For example, a machine learning (ML) algorithm could have a very high accuracy rate, but the users of the wider system could misinterpret its output and follow an unexpected course of action or completely ignore its recommendation through lack of trust</li>
  <li>
<a href="https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction">accessibility</a>: it’s particularly important to work out whether any users are being excluded from using your product or service, either intentionally or unintentionally. UR with a realistic sample can help you identify groups that may be excluded and why</li>
  <li>identifying how the system is being used: your AI solution may be used in ways you did not originally intend. Understanding why and what is happening can help you better meet the needs of your users and identify both risks and opportunities for innovation</li>
  <li>monitoring the AI solution while in service: building regular UR into how the solution is managed will help you continuously improve your product</li>
</ul>

<p>The skills needed to collaborate on the steps above are within the usual skillset of most experienced user researchers. A user researcher on an AI project will need to:</p>

<ul>
  <li>engage with the technology</li>
  <li>be adaptable in their approach</li>
  <li>design studies with technical and analytical colleagues</li>
  <li>use attitudinal methods</li>
  <li>understand how to do research that supports change management</li>
  <li>design studies to assess how user behaviour changes over time. For example, developing strategies for creating continuous feedback loops&nbsp;</li>
  <li>have experience doing generative research</li>
  <li>do concept testing</li>
  <li>understand privacy and data ethics</li>
  <li>be able to communicate in non-technical language to explain the AI system from a user perspective</li>
</ul>

<div class="call-to-action">
  <h4 id="practical-recommendations-1">Practical recommendations</h4>

  <ul>
    <li>Consult the government <a href="https://www.gov.uk/service-manual/user-research">Service Manual’s sections on user research</a>.</li>
    <li>Get your organisation’s user researchers involved in the project from the start so that research and evaluation is designed into the way the AI solution is supported.</li>
    <li>Work with user researchers to design into the product or service ways of continuously assessing how effectively the AI model meets user needs.&nbsp;</li>
    <li>Read the example use case GOV.UK Chat: doing user research for AI products.</li>
  </ul>
</div>

<h3 id="identifying-use-cases-for-ai">Identifying use cases for AI</h3>

<p>When thinking about how your organisation could benefit from AI, consider the possible situations or appropriate use cases. This must be led by business and user needs, pain points and inefficiencies, not what the technology can do.</p>

<p>Once you’ve identified the challenges and opportunities through user research, focus on use cases that can only be solved by AI or where AI offers significant advantages over existing techniques.&nbsp;</p>

<p>You can do this by considering whether traditional solutions might be unable to handle the volume, complexity or real-time nature of the task; or whether the problem relates to, for example, advanced pattern detection in large data sets, automating complex, dynamic decision-making processes, or providing personalisation. Evaluate the potential impact of implementing AI solutions on these problems using tools like cost-benefit analysis to consider improvements in efficiency, accuracy or cost reduction.</p>

<p>You should also assess the feasibility of implementing AI in your team. Are the necessary skills and infrastructure in place to implement and maintain AI solutions? Would you need to train current employees, hire new talent or partner with AI technology providers?</p>

<p>When deciding if you’re going to use AI, you should also consider the capabilities and limitations of AI, the use cases to avoid, and discuss your project with technical experts.</p>

<p>To consider potential use cases of AI in your organisation, check the:</p>

<ul>
  <li>examples of applications of AI in government</li>
  <li>appendix with case studies on AI projectsrecently developed in the public sector</li>
  <li>projects recorded in the <a href="https://www.gov.uk/government/publications/guidance-for-organisations-using-the-algorithmic-transparency-recording-standard/algorithmic-transparency-recording-standard-guidance-for-public-sector-bodies">Algorithmic Transparency Recording Standard (ATRS)</a> register</li>
</ul>

<h4 id="use-cases-to-avoid">Use cases to avoid&nbsp;</h4>

<p>Given the current limitations of AI, and their ethical, legal and social implications, there are use cases that are not appropriate and which should be avoided in the public sector. These include but are not limited to:</p>

<ul>
  <li>fully automated decision making: be cautious about any use case involving significant decisions, such as those involving someone’s health or safety. For more detail, refer to the section on Human oversight&nbsp;</li>
  <li>high-risk or high-impact applications: AI should not be used on its own in high-risk areas which could cause harm to someone’s health, safety, fundamental rights or the environment</li>
</ul>

<p>Follow the principles of using AI safely and responsibly to check if your use case involves significant risks related to bias, fairness, transparency, privacy or human rights.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-2">Practical recommendations</h4>

  <ul>
    <li>Define clear goals for your use of AI, and ensure they’re consistent with your organisation’s AI roadmap.</li>
    <li>Select use cases which meet a clear need and fit the capabilities of AI.</li>
    <li>Find out what use cases other government organisations are considering.</li>
  </ul>

  <p>Understand the limitations of AI and avoid high-risk use cases, considering the principles of using AI safely and responsibly.</p>
</div>

<h3 id="creating-the-ai-support-structure">Creating the AI support structure&nbsp;</h3>

<p>To ensure your organisation adopts AI smoothly, consider how AI will change the way your people and processes work. Check that you have the structures in place to support its adoption.&nbsp;</p>

<p>These structures do not need to be fully mature before your first project. Your experience in your first AI project will shape the way you organise these structures.&nbsp;</p>

<p>However, you should make sure that you have sufficient control over how you use AI and ensure all AI systems operate in a safe and responsible environment. If you do not already have them in place, you should establish the following:</p>

<ul>
  <li>AI strategy and adoption plan: a clear statement of the way that you plan to use AI within your organisation, including the impact on existing organisation structures and change management plans</li>
  <li>AI principles: a simple set of top-level principles which embody your values and goals, and that can be followed by all the people building solutions</li>
  <li>AI governance board: a group of senior leaders and experts to set principles and review and authorise uses of AI which fit these principles</li>
  <li>AI communication strategy: your approach for engaging with internal and external stakeholders to gain support, share best practice and show transparency</li>
  <li>AI sourcing and partnership strategy: a definition of which capabilities you will build within your own organisation, and which you will seek from partners</li>
  <li>AI training: resources that your team can use to upskill, based on a learning needs analysis</li>
</ul>

<p>You should also consider using:</p>

<ul>
  <li>a change management team: a small team with access to senior leadership that can shape your approach</li>
  <li>a use cases register: a way of capturing use cases and prioritising which you would like to explore first&nbsp;</li>
  <li>monitoring systems: to gather feedback and quickly identify emerging risks throughout change processes and for the duration of the system’s life cycle</li>
  <li>review and change processes: to provide staff with sufficient time, information and tools to identify and adapt to risks that emerge during a change process</li>
  <li>fallback processes: to ensure that critical functionality and services can be maintained if a change must be reverted and/or an AI system terminated</li>
</ul>

<div class="call-to-action">
  <h4 id="practical-recommendations-3">Practical recommendations</h4>

  <ul>
    <li>Identify the support structures you need for your level of AI maturity and adoption, or reuse support structures already in place for other technologies.</li>
    <li>Design an appropriate method of capturing and prioritising opportunities based on feasibility and business value.</li>
    <li>Develop a communication strategy for engaging a wider community of leaders and staff to explain AI, demonstrate activity and reduce resistance to change.</li>
  </ul>
</div>

<h2 id="buying-ai">Buying AI&nbsp;</h2>

<p>While AI is not new, the AI supply market is evolving rapidly. It’s important that you engage with commercial colleagues to discuss partners, pricing, products and services.</p>

<p>The Crown Commercial Service (CCS) can guide you through:</p>

<ul>
  <li>existing guidance</li>
  <li>routes to market</li>
  <li>specifying your requirements</li>
  <li>running your procurement</li>
</ul>

<p>They can also help you navigate:</p>

<ul>
  <li>running your procurement in an emerging market</li>
  <li>regulation and policy</li>
  <li>aligning procurement and ethics</li>
</ul>

<h3 id="ai-business-cases">AI business cases&nbsp;</h3>

<p>Creating the business case for your AI project is an important stage in the process of AI adoption as it gives decision makers the opportunity to assess the return of investment in both resources and costs.</p>

<p>Before writing your business case, you should engage with stakeholders and discuss your project to understand if AI needs to be used, and, if so, which solutions offer the best improvements in productivity.</p>

<p>There are several resources you can use to write a strong business case. Consider following the Treasury’s <a href="https://www.gov.uk/government/publications/the-green-book-appraisal-and-evaluation-in-central-government/the-green-book-2020">Green Book (2022)</a> guidance to create a fully fledged, five-part business case. This may be a requirement depending on the scale of your project and investment.</p>

<p>Your organisation will likely have investment thresholds. Typically, any investment approaching £10 million will require a five-part business case. You must use the Green Book when that is the case. If the scale of investment is below this threshold, you should strongly consider using the guidance on <a href="https://www.gov.uk/government/publications/agile-digital-and-it-projects-clarification-of-business-case-guidance/agile-digital-and-it-projects-clarification-of-business-case-guidance">agile business cases</a>, which was developed by the Government Digital Service (GDS).</p>

<p>When working on your business case, note that it’s mandatory to assure all digital and technology spend above £100,000 for anything public facing and £1 million for anything else, through your assurance boards. The process for doing so will be subject to your organisation’s requirements. Follow GDS’s <a href="https://www.gov.uk/government/publications/digital-and-technology-spend-control-version-6">guidance on getting spend approvals</a>.</p>

<p>In the following section, we summarise existing guidance on purchasing AI products and services. However, as the field is evolving rapidly this is not exhaustive, and engaging with commercial experts early in your project remains crucial.</p>

<h3 id="existing-guidance">Existing guidance</h3>

<p>There is detailed guidance to support the procurement of AI in the public sector. You should familiarise yourself with this guidance and make sure you’re taking steps to align with best practice. The:</p>

<ul>
  <li>
<a href="https://www.gov.uk/government/publications/guidelines-for-ai-procurement">Guidelines for AI procurement</a> provide a summary of best practice when buying AI technologies in government, including <a href="https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#preparation-and-planning">preparation and planning</a>, <a href="https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#publication">publication</a>, <a href="https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#selection-evaluation-and-award">selection, evaluation and award</a> and <a href="https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#contract-implementation-and-ongoing-management">contract implementation and ongoing management</a>
</li>
  <li>
<a href="https://www.gov.uk/government/publications/the-digital-data-and-technology-playbook">Digital, Data and Technology Playbook</a> provides general guidance on sourcing and contracting for digital and data projects and programmes. All central government departments and their arm’s length bodies are expected to follow this on a ‘comply or explain’ basis. It includes specific guidance on AI and machine learning, as well as Intellectual Property Rights (IPR)</li>
  <li>
<a href="https://www.gov.uk/government/publications/the-sourcing-and-consultancy-playbooks">Sourcing Playbook</a> defines the commercial process as a whole and includes key policies and guidance for making sourcing decisions for the delivery of public services</li>
  <li>
<a href="https://www.gov.uk/government/publications/knowledge-asset-management-in-government">Knowledge asset management in government (The Rose Book)</a> provides guidance on managing and exploiting the wider value of knowledge assets (including software, data and business processes). Annex D contains specific guidance on managing these in procurement</li>
  <li>
<a href="https://www.gov.uk/government/publications/digital-and-technology-spend-control-version-6/digital-and-technology-spend-control-version-6">Digital and technology spend control version 6</a> details in which instances your requirements will be subject to functional assurance of central government spending. Spend controls enable the UK government to achieve greater efficiency and better outcomes</li>
  <li>
<a href="https://www.gov.uk/government/publications/ppn-0224-improving-transparency-of-ai-use-in-procurement">Procurement Policy Note (PPN) 02/24: Improving Transparency of Artificial Intelligence Use in Procurement</a> provides optional questions to help identify suppliers’ use of AI in response to government procurements and in the delivery of services to government</li>
</ul>

<h3 id="routes-to-market">Routes to market&nbsp;</h3>

<p>Consider the available routes to market and commercial agreements and determine which one is most suitable based on your requirements.&nbsp;</p>

<p>However you decide to source your requirement, you must ensure you comply with procurement legislation. This is primarily the <a rel="external" href="https://www.legislation.gov.uk/uksi/2015/102/contents/made">Public Contract Regulations 2015</a> and the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2023/54/contents/enacted">Procurement Act 2023</a>, which is set to come into effect in 2025. Commercial colleagues will be able to assist with this.</p>

<p>There are various routes to market to purchase AI systems. Depending on the kind of challenges you’re addressing, you may prefer to use a <a rel="external" href="https://www.crowncommercial.gov.uk/news/what-is-a-framework-procurement-essentials">framework</a> or a <a rel="external" href="https://www.crowncommercial.gov.uk/news/what-is-a-dynamic-purchasing-system">Dynamic Purchasing System (DPS)</a>. CCS offers several frameworks and DPSs for the public sector to procure AI.</p>

<p>A <a href="https://www.gov.uk/guidance/public-sector-procurement">public-sector procurement</a> route also exists, sometimes known as a ‘Find a tender service procurement’ due to the requirement to advertise on that platform. This may be appropriate for bespoke requirements or contractual terms, or where there is no suitable standard offering.</p>

<p>The table below summarises the differences between a framework agreement and a DPS. There’s more information on the <a rel="external" href="https://www.crowncommercial.gov.uk/">CCS website</a> and, on the use of frameworks, in the <a href="https://www.gov.uk/government/publications/the-digital-data-and-technology-playbook">Digital, Data and Technology Playbook</a>.</p>

<table>
  <thead>
    <tr>
      <th scope="col">Category</th>
      <th scope="col">Framework</th>
      <th scope="col">DPS</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Supplier access</td>
      <td>Successful suppliers are awarded to the framework at launch. <br><br>Closed to new supplier registrations.&nbsp; <br><br>Prime suppliers can request to add new subcontractors.</td>
      <td>Open for new supplier registrations at any time.</td>
    </tr>
    <tr>
      <td>Structure</td>
      <td>Often divided into lots by product or service type.</td>
      <td>Suppliers filterable by categories.</td>
    </tr>
    <tr>
      <td>Compliance</td>
      <td>Thorough ongoing supplier compliance checks carried out by CCS, meaning buyers have less to do at call-off (excluding G-Cloud).</td>
      <td>Basic compliance checks are carried out by CCS, allowing the buyer to complete these at the call-off.</td>
    </tr>
    <tr>
      <td>Buying options</td>
      <td>Various options, including direct award, depending on the agreements.</td>
      <td>Further competition only.</td>
    </tr>
  </tbody>
</table>

<p>Note that a number of CCS agreements include AI within their scope.</p>

<p>Examples of DPSs include:</p>

<ul>
  <li><a rel="external" href="http://www.crowncommercial.gov.uk/agreements/rm6200">Artificial Intelligence (AI)</a></li>
  <li><a rel="external" href="http://www.crowncommercial.gov.uk/agreements/rm6173">Automation Marketplace</a></li>
  <li><a rel="external" href="http://www.crowncommercial.gov.uk/agreements/rm6094">SPARK</a></li>
</ul>

<p>Examples of frameworks include:</p>

<ul>
  <li><a rel="external" href="http://www.crowncommercial.gov.uk/agreements/rm6195">Big Data and Analytics</a></li>
  <li><a rel="external" href="https://www.crowncommercial.gov.uk/agreements/RM6098">Technology Products &amp; Associated Services 2</a></li>
  <li><a rel="external" href="https://www.crowncommercial.gov.uk/agreements/RM6100">Technology Services 3</a></li>
  <li><a rel="external" href="https://www.crowncommercial.gov.uk/agreements/RM6194">Back Office Software</a></li>
  <li><a rel="external" href="https://www.crowncommercial.gov.uk/agreements/RM6292">Cloud Compute 2</a></li>
</ul>

<p>In addition to commercial agreements, CCS has signed a number of <a rel="external" href="https://www.crowncommercial.gov.uk/products-and-services/technology/technology-memorandum-of-understanding/">Memorandums of Understanding (MoUs) with suppliers</a>. These MoUs set out preferential pricing and discounts on products and services across the technology landscape, including cloud, software, technology products and services, and networks. You can access MoU savings through any route to market. To get support or find out more, email <a href="mailto:info@crowncommercial.gov.uk">info@crowncommercial.gov.uk</a>.&nbsp;</p>

<h3 id="specifying-your-requirements">Specifying your requirements&nbsp;</h3>

<p>When buying AI products and services, you’ll need to document your requirements to tell your suppliers what you need. To define what you need, you should engage with subject matter experts (SMEs) as soon as possible, and take time to consider the most appropriate type of AI solution for your project. This might be an off-the-shelf product, an existing technology with bolt-on AI elements (paid or free), outsourcing AI builds (if applicable), or co-creating AI with suppliers. The <a href="https://www.gov.uk/government/publications/the-digital-data-and-technology-playbook">Digital, Data and Technology Playbook</a> has guidance on commercial off-the-shelf (COTS) software licensing terms and build versus buy decisions.</p>

<p>When drafting requirements for AI, you should:</p>

<ul>
  <li>start with your problem statement</li>
  <li>highlight your data strategy and requirements</li>
  <li>focus on data quality, bias (mitigation) and limitations&nbsp;</li>
  <li>underline the need for you to understand the supplier’s AI approach</li>
  <li>consider strategies to avoid vendor lock-in</li>
  <li>apply the <a href="https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework">Data Ethics Framework principles</a> and consider the appropriate <a rel="external" href="https://assets.crowncommercial.gov.uk/wp-content/uploads/Data-ethics-requirements-for-suppliers-template.pdf">Data ethics requirements and questions (PDF, 62.7KB)</a>
</li>
  <li>mention any integration with associated technologies or services</li>
  <li>consider your ongoing support and maintenance requirements</li>
  <li>consider data format and provide suppliers with dummy data where possible</li>
  <li>provide guidance on budget to consider hidden costs</li>
  <li>consider intellectual property rights and who will have these if new software is developed</li>
  <li>consider any acceptable liabilities and appetite for risk, to match against draft terms and conditions, once provided</li>
</ul>

<p>For more information, read the ‘Selection, evaluation and award’ section of the <a href="https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#selection-evaluation-and-award">Guidelines for AI Procurement</a> and CCS’s guide on <a rel="external" href="https://www.crowncommercial.gov.uk/news/how-to-write-a-specification-procurement-essentials">How to write a specification</a>.</p>

<p>Having prepared your procurement strategy, defined your requirements and selected your commercial agreement, you can proceed with your procurement. Your commercial team will lead this.&nbsp;</p>

<p>If you’re using an existing commercial agreement such as a framework or DPS, you’ll conduct a call-off in accordance with the process set out in the relevant commercial agreement. Non-framework or DPS procurements must comply with procurement legislation and relevant policy – for example, the <a rel="external" href="https://www.legislation.gov.uk/uksi/2015/102/contents/made">Public Contract Regulations 2015</a>, the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2023/54/contents/enacted">Procurement Act 2023</a> and PPNs.&nbsp;</p>

<p>CCS offers buyer guidance tailored to each of its agreements, which describe each step in detail, including completing your order contract and compiling your contract.</p>

<h3 id="running-your-procurement-in-an-emerging-market">Running your procurement in an emerging market&nbsp;</h3>

<h4 id="commercial-agreements">Commercial agreements</h4>

<p>While AI is not new, it is an emerging market from a commercial perspective. As well as rapidly evolving technology, there are ongoing changes in the supply base and the products and services it offers. DPSs offer flexibility for new suppliers to join, which often complement these dynamics for buyers.&nbsp;</p>

<p>Any public sector buyers interested in shaping CCS’s longer term commercial agreement portfolio should express their interest by emailing <a href="mailto:info@crowncommercial.gov.uk">info@crowncommercial.gov.uk</a>.</p>

<h4 id="regulation-and-policy">Regulation and policy</h4>

<p>Regulation and policy will also evolve to keep pace. However, there are already a number of legal and regulatory provisions which are relevant to the use of AI technologies. These include:</p>

<ul>
  <li>
<a href="https://www.gov.uk/government/consultations/ai-regulation-a-pro-innovation-approach-policy-proposals/outcome/a-pro-innovation-approach-to-ai-regulation-government-response#introduction">A pro-innovation approach to AI regulation: government response</a>: this details the government’s response to the white paper consultation published in March 2023, which set out early steps towards establishing a regulatory regime for AI, including 5 principles to guide responsible AI innovation in all sectors</li>
  <li>
<a href="https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques">Portfolio of AI assurance techniques</a>: this portfolio has been developed by the Responsible Technology Adoption Unit (RTAU), initially in collaboration with <a rel="external" href="https://www.techuk.org/">techUK</a>. It’s useful for anybody involved in designing, developing, deploying or procuring AI-enabled systems. It shows examples of AI assurance techniques being used in the real world to support the development of trustworthy AI</li>
  <li>
<a href="https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance">Introduction to AI assurance</a>: this guidance has been developed by the Department for Science, Information and Technology (DSIT) to help private sector organisations better understand how to implement AI assurance to ensure the responsible development and deployment of AI systems. It’s designed to be accessible to a range of users, especially those who may not engage with assurance on a day-to-day basis. It introduces them to core assurance definitions and concepts and then how these can be applied to support the development and use of trustworthy AI</li>
  <li>AI Management Essentials: DSIT is developing guidance to support private sector organisations to engage in the development of ethical, robust and responsible AI organisational practice. This self-assessment tool will distil key principles from existing AI-related standards and frameworks and provide simple baseline requirements for government suppliers of AI products and services. After testing and consultation, DSIT is planning to work with the Cabinet Office to embed the tool in government procurement frameworks</li>
</ul>

<p>This list is not exhaustive. For further guidance, refer to the Legal considerations section.</p>

<h3 id="aligning-procurement-and-ethics">Aligning procurement and ethics&nbsp;</h3>

<p>It’s important to consider and factor data ethics into your commercial approach from the outset.&nbsp;</p>

<p>There’s a range of <a href="https://www.gov.uk/guidance/data-ethics-and-ai-guidance-landscape">guidance relating to AI and data ethics</a> to support public servants working with data and/or AI. This guidance collates existing ethical principles, developed by government and public sector bodies. You can also refer to the:</p>

<ul>
  <li>
<a href="https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework">Data ethics framework</a>: this outlines appropriate and responsible data use in government and the wider public sector. The framework helps public servants understand ethical considerations, address these within their projects, and encourages responsible innovation</li>
  <li>
<a rel="external" href="https://assets.crowncommercial.gov.uk/wp-content/uploads/Data-ethics-requirements-for-suppliers-template.pdf">Data ethics requirements checklist (PDF, 62.7KB)</a> for suppliers: this will mitigate bias. It will also ensure diversity in development teams, transparency and interpretability, and explainability of the results</li>
  <li>
<a href="https://www.gov.uk/government/collections/the-public-sector-contract">Public Sector Contract (PSC)</a>: this includes provisions related to intellectual property rights, data protection, and equality, diversity and human rights</li>
</ul>

<div class="call-to-action">
  <h4 id="practical-recommendations-4">Practical recommendations</h4>

  <ul>
    <li>Engage your commercial colleagues from the outset. Understand and make use of existing guidance.</li>
    <li>Understand and make use of existing routes to market, including frameworks, DPSs and MoUs.</li>
    <li>Specify clear requirements and plan your procurement carefully.</li>
    <li>Seek support from your commercial colleagues to help navigate the evolving market, regulatory and policy landscape.</li>
    <li>Ensure that your procurement is aligned to ethical principles.</li>
  </ul>
</div>

<h2 id="using-ai-safely-and-responsibly">Using AI safely and responsibly</h2>

<p>This section outlines the steps you’ll need to take to ensure that you build AI solutions in a safe and responsible way, taking account of legal considerations, ethics, data protection, privacy, security and governance.&nbsp;</p>

<p>Many of these considerations interact with each other, so you should read all of these topics together and seek support from data ethics, privacy, legal and security experts when planning and developing your project.&nbsp;</p>

<p>This section supports:</p>

<ul>
  <li>Principle 2: You use AI lawfully, ethically and responsibly</li>
  <li>Principle 3: You know how to use AI securely</li>
  <li>Principle 4: You have meaningful human control at the right stage</li>
  <li>Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place</li>
</ul>

<h2 id="ethics">Ethics</h2>

<p>The ethical and responsible use of AI is crucial for:</p>

<ul>
  <li>maintaining public trust</li>
  <li>protecting individual rights</li>
  <li>fostering equitable societal progress</li>
</ul>

<p>This is covered in the white paper <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">A pro-innovation approach to AI regulation</a> and its <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a-implementation-of-the-principles-by-regulators">5 principles</a> to guide AI development in all sectors.</p>

<p>The ethical risks and opportunities presented by your use of AI will depend on your context and the nature of your solutions. The key themes you should address are:</p>

<ul>
  <li>safety, security and robustness</li>
  <li>transparency and explainability</li>
  <li>fairness, bias and discrimination</li>
  <li>accountability and responsibility</li>
  <li>contestability and redress</li>
  <li>societal wellbeing and public good</li>
</ul>

<p>The following sections explore these themes separately. However, there may be overlaps, with the impacts and outputs of one area introducing considerations in others. This means that, in some instances, the promotion of one ethical value may come at the cost of one or more other ethical values. For example, to promote fairness, you may need to collect demographic data to accurately assess the impact of a tool on different groups, which would have a detrimental impact on privacy. You must consider early on whether trade-offs are appropriate, if the benefits outweigh the risks, and that you’re avoiding any unacceptable risks.</p>

<p>Where possible, you should make specific and robust measurements to assess AI systems. These may, for instance, evaluate the accuracy and quality of a model’s outputs against the protected characteristics listed in the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2010/15/contents">Equality Act 2010</a>. You should select the most appropriate assessments for your technology and use case, being aware that the state of the art is rapidly developing.</p>

<p>As well as the guidance in this playbook, you can use the <a href="https://www.gov.uk/government/publications/data-ethics-framework">Data Ethics Framework</a> and the <a rel="external" href="https://uksa.statisticsauthority.gov.uk/wp-content/uploads/2022/03/2021_Self-assessment_guidance.pdf">UKSA ethics self-assessment (PDF, 449KB)</a> guidance.</p>

<h3 id="safety-security-and-robustness">Safety, security and robustness</h3>

<p>You must build safe, secure and robust AI solutions. This means that your AI systems must be resilient, sustainable and function reliably, even in unforeseen situations or against adversarial attacks.&nbsp;</p>

<p>Safety, security and robustness are important because they promote privacy rights, reduce harm, and uphold all the other ethical principles across the life cycle of your AI system.</p>

<p>Safety refers to a system’s ability to operate without causing harm to people or the environment. This is particularly important in high-risk areas such as healthcare, policing and justice.</p>

<p>Security refers to the protection of data, assets and functionality against unauthorised access, misuse or damage.</p>

<p>Robustness refers to the ability of your algorithm or model to maintain its performance and stability under different conditions. A core component of robustness is reliability: to be considered reliable, an AI system should consistently perform as intended across all expected scenarios within the system’s life cycle. You should therefore establish accuracy measures to ascertain whether the system is producing correct outputs. If a reliable system encounters an unexpected event, it should adapt or respond in a consistent way, minimising harm and providing teams with suitable warnings to respond and rectify issues.&nbsp;</p>

<p>Building safe, robust and secure AI solutions includes elements relating to privacy. You must understand that considerations of privacy extend beyond the requirements set out in the <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/">UK General Data Protection Regulation (UK GDPR)</a>, the <a href="https://www.gov.uk/data-protection">Data Protection Act 2018</a> and other relevant legislation. For example, risks relating to group privacy, which consider the rights and interests of a group or collective rather than individuals, may include characteristics or behaviours that, if exposed, could lead to discrimination, stigmatisation or other forms of harm.</p>

<p>Wider themes relating to safety, security and robustness are described in the Data protection and privacy and Security sections.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-5">Practical recommendations</h4>

  <ul>
    <li>Establish performance metrics for AI systems that measure the accuracy of model outputs.&nbsp;</li>
    <li>Test your system in a variety of scenarios, including those capturing extreme or rare potential events.</li>
    <li>Implement red teaming processes and record how your model behaves when it encounters unexpected and anomalous scenarios.</li>
    <li>Make full use of the training resources available, including the <a rel="external" href="https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw">courses on risks and ethics on Civil Service Learning</a>.</li>
  </ul>
</div>

<h3 id="transparency-and-explainability">Transparency and explainability</h3>

<p><a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">The AI regulation white paper</a> establishes that AI systems should be appropriately transparent and explainable.&nbsp;</p>

<p>Transparency is the communication of appropriate information about an AI system to the right people – for example, information on how, when and for which purposes an AI system is being used. A lack of transparency can lead to:</p>

<ul>
  <li>harmful outcomes</li>
  <li>public distrust</li>
  <li>a lack of accountability and the ability to appeal</li>
</ul>

<p>You must consider transparency issues before deploying an AI system.</p>

<p>Explainability describes the ability to clarify how an AI system arrives at a given output or decision, such as explaining what factors lead to a loan application being granted or denied. Explainability may be impacted by the technologies used to build a system. You should consider this when designing your system.</p>

<p>Ensuring transparency and explainability can be challenging in the context of AI. Transparency can be limited by proprietary and ‘black box’ commercial tools, while explainability may not be possible for certain forms of machine learning, or may only be achievable at the cost of performance.</p>

<p>When checking the transparency of your AI systems, you should consider the:</p>

<ul>
  <li>technical transparency: information about the technical operation of the AI system, such as the code used to create the algorithms and the underlying data sets used to train the model</li>
  <li>process transparency: information about the design, development and deployment decisions and practices behind your generative AI solutions, and the mechanisms used to demonstrate that the solution is responsible and trustworthy. Putting in place robust reporting mechanisms, process-centred governance frameworks and AI assurance techniques is essential for facilitating process-based transparency</li>
  <li>outcome-based transparency and explainability: the ability to clarify to any user using or impacted by a service that utilises AI how the solution works and which factors influence its decision making and outputs, including individual-level explanations of decisions where this is requested</li>
  <li>internal transparency: retention of up-to-date internal records on technology and processes, and process-based transparency information</li>
  <li>public transparency: communication about the department’s use of AI systems, made available to the general public in an open and accessible format</li>
</ul>

<p>All central government departments and certain arm’s length bodies that are in scope of the <a href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">Algorithmic Transparency Recording Standard (ATRS)</a> must use it to ensure transparency around the algorithmic tools used in decision-making processes by public bodies. We also recommend ATRS for use by other public sector bodies, although they are not required to use it yet. You can refer to additional standards and external resources. These include:</p>

<ul>
  <li>the UK’s national public sector AI ethics and safety guidance, <a rel="external" href="https://zenodo.org/record/3240529#.XSCMBy2ZMXr">understanding AI ethics and safety</a>, which outlines a process-based governance framework that can help project teams establish and document proportionate governance actions</li>
  <li>data and model cards or fact sheets, which can be used as a reference point when documenting information about AI models and the data sets used in training and testing. A good example of these are Google’s <a rel="external" href="https://github.com/PAIR-code/datacardsplaybook/tree/main/templates">data cards</a> and <a rel="external" href="https://arxiv.org/abs/1810.03993">model cards</a>
</li>
  <li>the <a rel="external" href="https://ico.org.uk/">Information Commissioner’s Office (ICO)</a>, which also offers AI auditing consultation and support to government organisations. Refer to <a rel="external" href="https://ico.org.uk/media/for-organisations/documents/4022651/a-guide-to-ai-audits.pdf">A guide to ICO Audit: Artificial Intelligence (AI) Audits (PDF, 156KB)</a> for more information</li>
  <li>
<a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence/">Explaining decisions made with AI guidance</a>, which is the UK’s national AI explainability guidance co-produced by <a rel="external" href="https://www.turing.ac.uk/">The Alan Turing Institute</a> and the ICO. This details 6 types of explanations as well as documentation processes</li>
</ul>

<div class="call-to-action">
  <h4 id="practical-recommendations-6">Practical recommendations</h4>

  <ul>
    <li>Use existing standards and recording mechanisms such as the <a href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">ATRS</a> to communicate information about AI solutions to the general public. ATRS is particularly relevant if your tool is deployed, directly or indirectly, in decision-making processes.</li>
    <li>Clearly signpost when AI has been used to create content, is interacting with members of the public, or is used in decision-making processes that impact members of the public.&nbsp;</li>
    <li>Put in place evaluation and auditing structures, tracking data provenance, design decisions, training scenarios and processes.</li>
    <li>Implement transparency and auditing requirements for suppliers.</li>
    <li>Use external resources and emerging best practice, such as data cards and model cards for internal transparency.</li>
    <li>Make model outputs as explainable as possible, and avoid using less explainable AI methods in areas where explainability is essential.</li>
    <li>Consider the use of open source models, which provide more transparency about data sets, code and training processes.</li>
  </ul>
</div>

<h3 id="fairness-bias-and-discrimination">Fairness, bias and discrimination&nbsp;</h3>

<p>The white paper <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">A pro-innovation approach to AI regulation</a> sets out that AI systems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes.&nbsp;</p>

<p>You must ensure fairness in the development and use of AI solutions to comply with legal and human rights requirements, including consumer and competition law, public and common law, and rules protecting vulnerable people. In the context of AI, fairness has many facets. It means ensuring that a system’s outputs are unprejudiced and do not amplify existing social, demographic or cultural disparities.&nbsp;</p>

<p>This includes ensuring that:</p>

<ul>
  <li>AI systems fairly allocate resources or services to all people, across all protected characteristics</li>
  <li>certain subgroups are not disproportionately adversely impacted or harmed – for example, in employment-related decisions</li>
  <li>in the context of generative AI systems, different groups are not, for example, represented in harmful, prejudiced or offensive ways</li>
</ul>

<p>You should understand that AI systems are designed, developed and deployed by human beings who are bound by the limitations of their contexts and biases. AI models are also trained on data which encodes present and past biases and inequalities of society. These can be present across the AI life cycle. Be aware that generative AI models are particularly vulnerable to bias because they’re trained on vast amounts of unfiltered data scraped from the internet, which is likely to contain a wide range of content reflecting historical and social biases. The wording of prompts may also inadvertently introduce bias.</p>

<p>A well-documented form of algorithmic bias is representational bias, which describes instances in which people are underrepresented, overrepresented or misrepresented in data sets used as training data. This form of biased input data can lead to the generation of harmful stereotypes or abusive content targeted at people from specific genders, sexual orientations and identities, ethnicities, countries of origin, or religions.&nbsp;</p>

<p>Another form of harm that can result from representational bias in the input data is performance disparities across different social groups. For example, a given system may perform more poorly on certain underrepresented dialects or skin colours. Other systems, such as AI applications that support healthcare, may provide beneficial results primarily to privileged members of society who are more heavily represented in the data.</p>

<p>Bias issues may be compounded when protected characteristics are considered in combination. The opacity and complexity of some AI systems can make it difficult to identify exactly where and how biases are introduced. Issues of fairness and bias may manifest when an AI system is implemented and used, even if issues were checked during system testing and validation. This may be due to a variety of reasons – from unexpected interactions by the users with software and hardware, to cultural or personal needs and specificities that were not captured earlier in the development process. For example, users may choose to ignore or only selectively pay attention to recommendations provided by an AI system, which may introduce new forms of bias.</p>

<p>It’s also important to consider <a href="https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction">accessibility</a>, ensuring that equal benefits can be achieved by all and considering interactions with assistive technologies. You can find more on <a href="https://www.gov.uk/service-manual/helping-people-to-use-your-service/testing-for-accessibility">designing and testing for accessibility</a> in the government <a href="https://www.gov.uk/service-manual">Service Manual</a>.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-7">Practical recommendations</h4>

  <ul>
    <li>Implement bias mitigation and fairness evaluation across the entire AI project life cycle.</li>
    <li>Comply with human rights law, the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2010/15/contents">Equality Act 2010</a>, the <a href="https://www.gov.uk/government/publications/public-sector-equality-duty">Public Sector Equality Duty</a> and the <a href="https://www.gov.uk/government/organisations/equality-and-human-rights-commission">Equality and Human Rights Commission</a> guide to using AI in public services.</li>
    <li>Review model outputs and decisions for bias, measuring performance and decisions for different groups of people including intersectional groups.</li>
    <li>Put feedback mechanisms in place to allow individuals to report unfair decisions, harmful outputs and accessibility challenges.</li>
    <li>Beware of instances in which the use of AI can make legal obligations to the Equality Act 2010 more difficult to uphold – for example, difficulties with making links between abstract algorithmic groupings and protected groups.</li>
    <li>Adopt an approach of continuous evaluation to keep pace with changing fairness considerations and societal expectations.</li>
  </ul>
</div>

<h3 id="accountability-and-responsibility">Accountability and responsibility&nbsp;</h3>

<p>Ensuring accountability and responsibility in the context of AI means that individuals and organisations can be held responsible for the effects that the AI systems they develop, deploy or use, have on people and society.</p>

<p>You must think about this at the start of your project to:</p>

<ul>
  <li>encourage mindful creation and usage of AI systems</li>
  <li>ensure that people who design and deploy AI systems can be held accountable for their outputs and impacts</li>
</ul>

<p>Accountability mechanisms will help you address and remediate issues when an error occurs. This involves establishing which parties are responsible at each stage of the system’s life cycle. Governance mechanisms may help to establish clear guidelines and structures for the development and deployment of AI systems. Refer to the Governance section for more information.</p>

<p>To establish accountable practices across the AI life cycle, you should consider 3 key elements: answerability, auditability and liability.</p>

<h4 id="answerability">Answerability</h4>

<p>You should establish a chain of human responsibility across the AI project life cycle, including responsibility throughout the supply chain. In cases of harm or errors caused by AI, you need to establish recourse and feedback mechanisms for affected individuals.&nbsp;</p>

<p>Identifying the specific actors involved in AI systems is vital to answerability. This includes model developers, application developers, policymakers, regulators, system operators and end-users. In each case, you must define their roles and responsibilities, and align these with legal and ethical standards.</p>

<h4 id="auditability">Auditability</h4>

<p>You should demonstrate the responsibility and trustworthiness of your development and deployment practices by:</p>

<ul>
  <li>upholding robust reporting and documentation protocols</li>
  <li>retaining traceability throughout the AI project’s life cycle</li>
</ul>

<p>Auditability is the process of documenting every stage of the AI innovation life cycle – from data collection and base model training to implementation, system deployment, updating and retirement –&nbsp;in a way that’s accessible to relevant stakeholders and easily understood.</p>

<h4 id="liability">Liability</h4>

<p>You should make sure that all parties involved in the AI project life cycle, from vendors and technical teams to system users, are acting lawfully and understand their respective legal obligations.</p>

<p>As an end-user, being accountable means that you take responsibility for a system’s outputs and its potential consequences. You should check that outputs are accurate, non-discriminatory, non-harmful, and do not violate existing legal provisions, guidelines, policies or the provider’s terms of use.&nbsp;</p>

<p>You must also put the necessary oversight and human-in-the-loop processes in place to validate output in situations with high impact or risk. Where these risks are too high, you must reconsider if AI should be used at all. Refer to the Identifying use cases for AI section for more on this.</p>

<p>Ultimately, responsibility for any output or decision made or supported by an AI system always rests with the public organisation. Where AI is bought commercially, ensure that vendors understand their responsibilities and liabilities, put the required risk mitigations in place and share all relevant information.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-8">Practical recommendations</h4>

  <ul>
    <li>Follow existing legal provisions, guidelines and policies as well as the provider’s terms of use when developing, deploying or using AI.</li>
    <li>Clearly define responsibilities, accountability and liability across all actors involved in the AI life cycle. Where AI is bought commercially, define detailed responsibilities and liability contractually.</li>
    <li>Nominate a senior responsible owner (SRO) who will be accountable for the use of AI in a specific project.</li>
    <li>Where AI is used in situations of high impact or risk, establish a human-in-the-loop process to oversee and validate outputs and decisions. Make sure that these people can effectively identify risks and intervene, where appropriate.</li>
    <li>As an end-user, assume responsibility for the outputs and decisions made by the AI systems you use.&nbsp;</li>
    <li>Adopt a risk-based approach to the use of AI and consider whether it’s appropriate to use AI in high-risk applications.</li>
  </ul>

  <p>Use assurance techniques to evaluate the performance of AI systems. The <a href="https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance#:~:text=2.1%20Introduction,the%20wider%20AI%20governance%20landscape.">Introduction to AI assurance</a> provides a useful starting point, and the <a href="https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques">Portfolio of AI assurance techniques</a> offers real-world examples.</p>
</div>

<h3 id="contestability-and-redress">Contestability and redress</h3>

<p>The principles of contestability and redress refer to the mechanisms through which AI systems and their outputs or decisions can be challenged, and how impacted individuals can seek remedy.</p>

<p>Contestability and redress are important because they help identify and correct ethical issues in AI systems after deployment. You must design appropriate mechanisms before deployment, and continue to maintain them throughout the full life cycle of your AI system.</p>

<p>These principles are interlinked with transparency and explainability requirements, as public awareness of the use of algorithms and effective explainability of AI systems are essential for questioning or challenging their use or outputs.&nbsp;</p>

<p>You should make people aware when an AI system is used, and clearly signpost mechanisms for contestability and redress to impacted individuals. These include:</p>

<ul>
  <li>public awareness: to enable users to contest and seek redress about your AI system, you must ensure that they are aware of the presence of the AI system and the function that it plays in the services that they’re interacting with. This includes making users aware of mechanisms for contestability and redress clearly and in a timely fashion</li>
  <li>mechanisms for appeal: you must establish and promote clear and accessible mechanisms for people to challenge the decisions made by AI systems, and ask wider questions concerning the training, deployment and impacts of AI systems employed by the UK government</li>
  <li>change processes: you must ensure that mechanisms are in place to investigate any areas highlighted by users, and make changes to or decommission AI systems if unacceptable risks or harms are identified</li>
</ul>

<p>Though contestability and redress mechanisms offer a route to mitigate harms, by themselves they do not sufficiently ensure the responsible use of these technologies, as harms may not be apparent to all who are impacted. Adhering to the principles discussed in the Using AI safely and responsibly section will help you identify risks before harms occur.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-9">Practical recommendations</h4>

  <ul>
    <li>Put mechanisms in place for users or impacted individuals to report instances of potential risk or harm relating to AI systems.</li>
    <li>Nominate one or more SROs to be responsible for monitoring reports and implementing adequate changes throughout the AI system’s life cycle.</li>
    <li>Ensure that routes for reporting risks or harms are clearly disclosed, and signposted to users at the point where they are, directly or indirectly, interacting with or impacted by an AI system.</li>
    <li>Where possible, provide impacted individuals with the option to contact the responsible team directly.</li>
    <li>For each project using AI, ensure you allocate adequate resources to respond to messages received through public communication channels and make changes where necessary across the life cycle of the system.</li>
    <li>Create contingency plans to maintain essential services in the event that an unacceptable risk is identified and the use of an AI system needs to be temporarily or permanently stopped.</li>
  </ul>
</div>

<h3 id="societal-wellbeing-and-public-good">Societal wellbeing and public good</h3>

<p>Societal wellbeing in the context of AI means not only ensuring that AI is developed in a way that minimises and mitigates harms, but also actively promoting ethical applications of AI that solve societal challenges and deliver good for society.&nbsp;</p>

<p>AI may be perceived as an impersonal, distant, or even alienating technology. Government engagement with academia, industry and especially the broader civil society is crucial to dispel fears and foster understanding of the potential benefits of AI technologies.&nbsp;</p>

<p>The <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">AI regulation white paper</a> and the <a href="https://www.gov.uk/government/publications/artificial-intelligence-ai-opportunities-action-plan-terms-of-reference/artificial-intelligence-ai-opportunities-action-plan-terms-of-reference">AI opportunities action plan</a> highlight the potential for AI to deliver tangible benefits to members of the public and the economy. The applications of AI in government and the potential public benefits that we can achieve using AI are wide-ranging. AI technologies can be used to improve productivity and access to services; advance the effectiveness of health interventions and diagnoses; create and deliver more personalised training; promote sustainability (by supporting the work of conservationists, for example); and even reduce existing inequalities, for example by increasing access to health services for at-risk groups. By engaging in dialogue with civil society and acting as a leading, responsible user of AI, government can help the people of the UK better embrace AI’s potential for positive change.</p>

<p>However, AI systems and applications may have two-sided impacts. If not used in responsible ways and with appropriate safeguards in place, AI tools may risk generating harm, entrenching inequalities or undermining democratic processes. When considering the use of an AI system, you should identify and weigh both the potential positive impacts of new technologies as well as any negative impacts or unintended consequences.&nbsp;</p>

<p>You must ensure that AI systems generate a net positive impact on stakeholders and society at large, while minimising potential harms as much as possible. If the potential negative consequences are too high, you must consider terminating the project.</p>

<p>Some of the elements you should consider when assessing the potential impact of AI tools are:</p>

<ul>
  <li>justified trust: this is essential to promote the uptake and long-term adoption of technology. AI can promote justified public trust in the government as a whole if the public understands and recognises that the AI solution has been developed competently, responsibly and ethically</li>
  <li>public benefit: you must ensure that the AI solutions you develop and/or use represent good value for money and benefit the public. This aligns with the UK government’s ambitions to use AI to help solve societal and global challenges – as long as the AI solution is safe, lawful and compatible with other ethical principles</li>
  <li>harm minimisation: illustrating public benefit is an important principle when evaluating the ethics of an AI project, but it’s equally important to minimise harms</li>
  <li>misinformation and disinformation: both unintentionally and intentionally, AI systems can be used to generate factually incorrect information. Some AI systems may facilitate the spread of misinformation or disinformation through their ability to generate plausible but false content, or by promoting this content online with recommender systems. This risk is particularly prevalent in generative AI systems, which are typically designed to create statistically likely language patterns rather than reliable accounts of reality. The ability of large language models (LLMs) to create text that appears credible and convincing enhances the potential for false or misleading information to be believed</li>
  <li>sustainability: AI can potentially provide a public benefit by helping meet sustainability goals. However, it can also present risks relating to energy and resource consumption derived from both the training and deployment of some AI technologies. If you plan to use generative AI tools, consider that it’s usually less environmentally sound to train your own model if appropriate pre-trained models are available. Generative AI can be expensive to operate, so it should not be used for tasks that could be undertaken by other available technologies</li>
</ul>

<p>In addition to the points above, you can consider the <a rel="external" href="https://uksa.statisticsauthority.gov.uk/publication/considering-public-good-in-research-and-statistics-ethics-guidance/">ethics guidance from the Office for National Statistics</a> to understand and articulate the potential public goods of your project.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-10">Practical recommendations</h4>

  <ul>
    <li>Engage with broader society – including civil society groups, underrepresented individuals, those most likely to experience harm, NGOs, academia, and more – when defining and deploying AI systems. Understanding their values, needs and priorities will help ensure your AI products deliver tangible benefits to society.</li>
    <li>Weigh any positive impacts of using an AI system against potential negative ones.</li>
    <li>Verify the information generated by AI systems to ensure it’s accurate.</li>
    <li>Assess the potential risks of your AI system being used to generate or spread misinformation or disinformation.</li>
    <li>Assess the environmental impact of training and/or deploying your AI system before commencing development. Consider whether the impact represents a reasonable trade-off between benefits and energy consumption, and whether a less energy-intensive system might be able to achieve the same or similar results. Also consider any actions you can take to <a href="https://www.gov.uk/guidance/make-your-technology-sustainable">make technology sustainable</a>.</li>
    <li>Evaluate the environmental credentials of potential model providers and wider partner organisations – including their use of renewable energy, energy-efficient infrastructure and sustainable practices – and select low carbon emission energy grids.</li>
  </ul>
</div>

<h2 id="legal-considerations">Legal considerations&nbsp;</h2>

<p>Different types of AI and use cases will likely create different types of legal issues. You’re not alone and should seek advice from government legal advisers who can help you navigate the design and use of AI in government.</p>

<p>Many of the legal issues that surround AI are not new. For example, the ethical principles discussed in this playbook, such as fairness, discrimination, transparency and bias, have sound foundations in public and other law. The ethical issues that your team identifies are also likely to be legal issues that your lawyers will be able to help guide you through.</p>

<p>The Lawfulness and purpose limitation section explains how to ensure that personal data is processed lawfully, securely and fairly at all times. Your lawyers can advise you on that.</p>

<p>You may face procurement and commercial issues when buying AI products. Alongside commercial colleagues, your lawyers can help you navigate those challenges.</p>

<p>When you contact your legal team, you should explain your aims for the AI solution, what it will be capable of doing, and any potential risks you’re aware of. This will help you to understand, for example, if you need legislation to achieve what you want to do.</p>

<p>It will also help to minimise the risk of your work:</p>

<ul>
  <li>being challenged in court</li>
  <li>having unintended and/or unethical consequences</li>
  <li>having a negative impact on the people you want it to benefit</li>
</ul>

<h3 id="example-legal-issues">Example legal issues</h3>

<p>These are designed to help you understand when you might want to consider getting legal advice. They should not be read as real legal advice and their application to any given scenario will depend on the specific facts. You should always consult your organisation’s lawyer.</p>

<h3 id="data-protection">Data protection</h3>

<p>Data protection is a legal issue, with potentially serious consequences if the government gets it wrong.&nbsp;</p>

<p>Although your organisation will likely have a data protection officer, and there may also be data protection experts in your team, your legal team can help you unpick some of the difficult data protection issues that are created by AI.</p>

<p>Refer to the Data protection and privacy section for more information.</p>

<h3 id="contractual-issues">Contractual issues</h3>

<p>Your lawyers will help you draw up the contracts and other agreements for the procurement or licensing of AI tools. There may be special considerations for these contracts. For example, how to:&nbsp;</p>

<ul>
  <li>deal with intellectual property</li>
  <li>ensure the level of transparency needed to help buyers understand their systems</li>
  <li>transfer a project to new or successor suppliers&nbsp;</li>
  <li>assist with the defence against any legal challenge</li>
</ul>

<p>Contracts for technology services may need to incorporate procedures for system errors and outages that recognise the potential consequences of performance failures.&nbsp;</p>

<p>It’s important that you consider appropriate contractual terms early on because this may, in part, drive decisions on the appropriate route to market. Refer to the Buying AI section for more information.</p>

<h3 id="intellectual-property-including-copyright">Intellectual property, including copyright</h3>

<p>The potential intellectual property issues with AI have been much discussed. Your lawyers can help you navigate these.&nbsp;</p>

<p>For example, you should consider at the outset:&nbsp;</p>

<ul>
  <li>which parties will own which parts of any intellectual property generated during the project</li>
  <li>which parties will have ongoing rights to use any intellectual property that is generated (and on what basis)</li>
  <li>how the balance of risk and liability should be determined between the parties, as this will be relevant to any claims for infringement of third party intellectual property</li>
</ul>

<h3 id="equality-issues">Equality issues</h3>

<p>Lawyers can help you navigate the equality issues raised by the use of AI in government – for example, obligations arising under the Equality Act 2010 and the Public Sector Equality Duty. Conducting an assessment of the equality impacts of your use of AI can also be one way to guard against bias, which is particularly important in the context of AI.</p>

<p>If approached early, before contracts are signed, your legal advisers can help you ensure the government is fulfilling its responsibilities to the public to assess the impacts of the technology it’s using.</p>

<h3 id="public-law-principles">Public law principles</h3>

<p>Public law principles explain how public bodies should act rationally, fairly, lawfully and in compatibility with human rights. These are guidelines for public bodies on how to act within the law.&nbsp;</p>

<p>Many of these public law principles overlap with the ethical principles set out in this guidance. As a result, your lawyers will likely be able to guide you on how to apply the ethical principles based on their knowledge of public law, the court cases that have occurred and the detail of the judgments.</p>

<p>For example, public law involves a principle of procedural fairness. This is not so much about the decision that is eventually reached but about how a decision is arrived at. The transparency and explainability of the AI tool may well be key in being able to demonstrate that the procedure was fair. Similarly, an inability to determine how AI tools have arrived at their decisions or outputs may introduce risk into the decision-making process.&nbsp;</p>

<p>Public law also considers rationality. Rationality may be relevant in testing the choice of an AI system, considering the features used in a system, and considering the outcomes of the system and the metrics used to test those outcomes.&nbsp;</p>

<p>If you’re considering using AI in decision making, public law can also guide you. For example, it can help you determine whether a particular decision should be delegated to a decision maker, rather than letting an AI tool make an automated decision. When operating in a regulated environment, such as a procurement process, automated decision making or assessments could be subject to legal challenge if procedural fairness, lack of bias and rationality cannot be evidenced.&nbsp;</p>

<h3 id="human-rights">Human rights</h3>

<p>Public authorities must act in a way that is compatible with human rights. It’s possible that AI systems (especially those involving the use of personal data) may in some way affect at least one of an individual’s rights, as set out in the <a rel="external" href="https://www.coe.int/en/web/human-rights-convention">European Convention on Human Rights (ECHR)</a>. Examples of the rights most likely to be impacted are Article 8 (right to a private and family life) and Article 10 (freedom of expression).</p>

<h3 id="legislation">Legislation</h3>

<p>Sometimes, in order to do something, a public authority needs a legislative framework. Your lawyers will be able to advise you whether your use of AI is within the current legal framework or needs new legislation.</p>

<p>For example, the legislative framework might not allow the process you’re automating to be delegated to a machine, or it might provide for a decision to be made by a particular person.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-11">Practical recommendations</h4>

  <ul>
    <li>Ensure you engage legal professionals at the outset of your AI project. They can help you navigate legal complexities and identify potential legal risks associated with data protection, contractual agreements, intellectual property, equality issues, and compliance with public law principles.</li>
    <li>Given the potential consequences of mishandling personal data, collaborate with legal experts to ensure you comply with data protection regulations and understand how to mitigate risks associated with data privacy and security.</li>
    <li>Work with legal experts to develop robust contracts and agreements for procuring or licensing AI tools, considering issues such as intellectual property rights, transparency levels, liability distribution, and procedures for addressing system errors or failures.</li>
    <li>Seek legal advice to determine whether your AI project aligns with existing legislative frameworks or requires new legislation. Understanding legislative constraints helps mitigate the risk of legal challenges and ensures you comply with legislative requirements.</li>
  </ul>
</div>

<h2 id="data-protection-and-privacy">Data protection and privacy</h2>

<p>AI-driven technologies offer significant benefits but they also pose potential risk of harm to individuals and groups if they’re not implemented with specific focus on protecting individuals’ <a rel="external" href="https://ico.org.uk/for-organisations/data-protection-fee/legal-definitions-fees/#data">personal data</a> and right to privacy.&nbsp;</p>

<p>Be aware that organisations developing and deploying AI systems must consider the principles of data protection outlined in the <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/">UK General Data Protection Regulation (UK GDPR)</a> and the <a href="https://www.gov.uk/data-protection">Data Protection Act 2018</a>, and minimise the risk of privacy intrusion from the outset.</p>

<p>The UK data protection law applies irrespective of the type of technology used, so its basic principles of compliance will also apply to any AI system. The data protection principles most relevant to the use of AI are:</p>

<ul>
  <li>accountability: your organisation has clear ownership of risk and responsibility for mitigations and compliance</li>
  <li>lawfulness: you have an applicable lawful basis for processing personal data and ensure the processing is lawful under data protection or any other regulation</li>
  <li>purpose limitation: you define why you’re processing personal data and only process data for that purpose</li>
  <li>transparency and individual rights: you’re open about what it uses personal data for, and your users can exercise their information rights</li>
  <li>fairness: you avoid processing personal data in ways that are detrimental, unexpected or misleading</li>
  <li>data minimisation: you develop systems that process only the data that is needed for the task at hand</li>
  <li>storage limitation: you don’t accumulate large amounts of personal data for unjustifiably long periods</li>
  <li>human oversight: you build in human oversight to automated decision making</li>
  <li>accuracy: you have steps in place to ensure the accuracy of AI-generated responses and data related to individuals</li>
  <li>security: you implement appropriate technical and organisational mitigations to protect sensitive and personal data</li>
</ul>

<p>Data protection and privacy considerations require specialist expertise, so it’s crucial to involve relevant data protection, legal and other information governance professionals in AI projects from the outset to follow data protection by design principles.</p>

<h3 id="accountability">Accountability&nbsp;</h3>

<p>Accountability is a key principle that establishes ownership of risk, responsibility for mitigations, compliance with legislation, the ability to demonstrate compliance, and high standards for privacy.</p>

<p>Organisations should take the following steps when planning AI solutions:</p>

<ul>
  <li>make a strategic decision on how any use of AI technology fits with your existing risk tolerance</li>
  <li>review your risk governance model to establish clear ownership of AI risks at a senior level</li>
  <li>implement measures to mitigate these risks and test their effectiveness</li>
  <li>make sure you identify residual risks and align them with your organisation’s risk threshold</li>
  <li>be collaborative, work transparently and demonstrate how you mitigate risks</li>
  <li>due to the evolving nature of AI technologies and new regulations, ensure you conduct regular reviews, with a view to making further iterations</li>
  <li>engage with internal data protection, privacy and legal experts from the outset</li>
</ul>

<p><a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-by-design-and-default/#:~:text=Data%20protection%20by%20design%20is%20ultimately%20an%20approach%20that%20ensures,and%20then%20throughout%20the%20lifecycle.">Data protection by design</a> is an important component of the UK GDPR risk-based approach. It requires you to integrate data protection safeguards into personal data processing activities throughout the AI product life cycle.&nbsp;</p>

<p>This will ensure that you implement appropriate technical and organisational measures to protect <a rel="external" href="https://ico.org.uk/for-organisations/data-protection-fee/legal-definitions-fees/#:~:text=compliance%20with%20them.-,Data%20subject,to%20whom%20personal%20data%20relates">data subject</a> rights, and comply with the <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-protection-principles/a-guide-to-the-data-protection-principles/">data protection principles</a> defined in the UK GDPR and Data Protection Act 2018.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-12">Practical recommendations</h4>

  <ul>
    <li>Establish ownership of AI risks at a senior level.</li>
    <li>Integrate oversight of AI into your governance processes.</li>
  </ul>

  <p>Take a risk-based approach, defining risk appetite and following <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-by-design-and-default/">principles of data protection by design and by default</a>.</p>
</div>

<h3 id="lawfulness-and-purpose-limitation">Lawfulness and purpose limitation</h3>

<p>Before implementing AI solutions, you need to undertake a <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-impact-assessments/">data protection impact assessment (DPIA)</a>. This involves an assessment of data protection and privacy risks, and the implementation of appropriate technical and organisational measures to sufficiently mitigate them.</p>

<p>Article 35(3)(a) of the UK GDPR requires you to undertake a DPIA if your use of AI involves any of the following:</p>

<ul>
  <li>systematic and extensive evaluation of personal data aspects based on automated processing, including profiling, on which decisions are made that produce legal or similarly significant effects</li>
  <li>large-scale processing of special categories of personal data</li>
  <li>systematic monitoring of publicly accessible areas on a large scale</li>
</ul>

<p>The <a href="https://www.gov.uk/government/organisations/information-commissioner-s-office">Information Commissioner’s Office (ICO)</a> also requires a DPIA if your processing of personal data involves the use of innovative technologies. In your DPIA, you should take all of the actions below.</p>

<ol>
  <li>Describe the purpose of personal data processing activities.</li>
  <li>Assess the necessity and proportionality of personal data processing.</li>
  <li>Identify all <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/personal-information-what-is-it/what-is-personal-information-a-guide/">personal data</a>, including <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/special-category-data/#scd1">special category data</a>, that is being processed, including sources and flows of data.</li>
  <li>Identify the <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/">valid lawful basis</a> under Article 6 (and any <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/special-category-data/#scd3">additional special conditions</a> under Article 9 for special category data) of the UK GDPR.</li>
  <li>Identify your organisation’s role and obligations as a data controller and whether any data processors are involved.</li>
  <li>Identify the stages when AI processes and automated decisions may have an impact on individuals.</li>
  <li>Seek and document the views of individuals whose personal data is being processed. This includes finding out whether data subjects are aware that this processing is taking place.</li>
  <li>Identify the stages when any human is involved in the decision-making process.</li>
  <li>Consider any potential detriment to individuals due to bias or inaccuracy.&nbsp;</li>
  <li>&nbsp;Document measures and safeguards put in place, and any residual levels of risk posed by the processing.</li>
</ol>

<p>The purpose for which data is collected and used has a significant effect on whether individuals perceive it as being invasive to privacy. A clear and well-defined articulation of the purpose from the outset will guide your deliberations about an applicable, lawful basis and the minimum personal data that is absolutely necessary to deploy the AI service.&nbsp;</p>

<p>AI systems often reuse personal data for new purposes that are different from those for which it was originally collected. This may cause tension with the purpose limitation of the UK GDPR. Repurposing of personal data is only legitimate if a new purpose is ‘compatible’ with the purpose for which the data was originally collected.&nbsp;</p>

<p>You should consider the following criteria when repurposing personal data:</p>

<ul>
  <li>whether the new purpose aligns with the data subjects’ expectations</li>
  <li>what type of personal data is involved</li>
  <li>what potential impact it will have on data subjects’ interests</li>
  <li>whether the data controller will need to adopt additional safeguards to ensure fairness and transparency</li>
</ul>

<p>The DPIA process should identify personal data processing at each stage of the AI life cycle – from design to data acquisition and preparation, training, testing, deployment and monitoring. Although it’s common to characterise AI with large volumes of data, AI systems are able to directly perceive and evaluate their environment, and adapt to the data received. You should not underestimate AI’s interactive qualities, such as its ability to collect new data in real time from touchscreens and audiovisual inputs, and adapt its responses and subsequent functions based on these inputs.</p>

<p>When mapping personal data flows, you should identify the geographic location of each distinct processing activity because the processing of data outside the United Kingdom will increase the risk of losing the protection of UK data protection laws. Data controllers may need to bring in additional safeguards, such as <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/international-transfers/international-data-transfer-agreement-and-guidance/">international data transfer agreements</a> if personal data is being processed in jurisdictions where the data protection regime is not deemed to be adequate and transfers of personal data are restricted under Article 46 of the UK GDPR.</p>

<p>If your assessment indicates that there’s a high risk to the data protection rights of individuals, and that you’re unable to sufficiently reduce these risks despite mitigating actions, you must consult the ICO before you can start processing personal data.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-13">Practical recommendations</h4>

  <ul>
    <li>When building your team, seek support from data compliance professionals – including data protection, legal and privacy experts.</li>
    <li>Identify data processing operations and their purpose, and map personal data sources and flows.</li>
    <li>Determine whether personal data is necessary for each activity, and whether you’re processing special category data or children’s data.</li>
    <li>Identify the applicable lawful basis of your data processing and assess data protection and privacy risk through <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/">DPIAs</a> and <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/legitimate-interests/">legitimate interest assessments</a>.</li>
    <li>If data protection and privacy risks remain high even after mitigations, <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/do-we-need-to-consult-the-ico/">consult with the ICO</a>.</li>
    <li>Identify any processing outside the UK to take additional safeguards to protect personal data in jurisdictions where the data protection regime may not be adequate.&nbsp;</li>
    <li>Assess any changes in the purpose of your AI system and make sure your AI system remains compliant and lawful.</li>
  </ul>
</div>

<h3 id="transparency-and-individual-rights">Transparency and individual rights</h3>

<p>In addition to the ethical reasons for seeking transparency, organisations need to be transparent about how they process personal data in an AI system so that individuals can effectively exercise the rights granted to them by the UK GDPR.</p>

<p>The UK GDPR requires data controllers to:</p>

<ul>
  <li>provide information to users in a concise, transparent, intelligible and easily accessible form using clear and plain language</li>
  <li>be transparent about the purpose for processing personal data, retention periods and third parties involved in the processing activity</li>
  <li>be transparent about the existence of automated decision making, providing meaningful information about the logic involved, and about the significance and envisaged consequences for the data subject of processing in this way</li>
  <li>provide a clear explanation of the results these systems produce</li>
  <li>uphold individuals’ rights, including the right of access to the personal data that you hold on them, and have a simple and clear process to exercise their right to correction and to object to the processing of their personal data at any time</li>
</ul>

<p>The transparency principle applies to personal data collected from all sources, including the interactive qualities of AI systems that have the ability to collect new data, which may include text and audiovisual inputs. For example, if you’re using facial recognition technology for public area monitoring, you need to be transparent by clearly informing data subjects. You can do this with clear signage and information on relevant data controllers, what information is collected, the purpose and legal basis of processing, and for how long the data is kept.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-14">Practical recommendations</h4>

  <ul>
    <li>Explain your system in plain English.</li>
    <li>Be transparent about the purpose for processing personal data, retention periods and third parties involved in the processing activity.</li>
    <li>Be transparent about the existence and nature of automated decision making, using the <a href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">Algorithmic Transparency Recording Standard (ATRS)</a> where required or on a voluntary basis as best practice.</li>
  </ul>

  <p>Provide a clear explanation of the results these systems produce, following guidance such as the ICO’s <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence/">Explaining decisions made with AI</a>.</p>
</div>

<h3 id="fairness">Fairness&nbsp;</h3>

<p>Fairness in processing is another principle under the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted">UK GDPR</a> which applies to AI systems that process personal data. In the context of data protection legislation, fairness means that ‘you should only process personal data in ways that people would reasonably expect and not use it in any way that could have unjustified adverse effects on them’.</p>

<p><a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/">DPIAs</a> are the main tool to help you consider the risks to the rights and freedoms of individuals, including the potential for any significant social or economic disadvantage. DPIAs also help demonstrate whether your processing is necessary to achieve your purpose, proportionate and fair.</p>

<p>The Responsible Technology Adoption Unit (RTA) in the Department for Science, Information and Technology (DSIT) published the results of its <a href="https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-3">Public Attitudes to Data and AI survey</a> in December 2023. This report found that people’s comfort with the use of AI greatly depends on the specific context. Perceptions of the need for AI governance also vary considerably by sector, with a substantial proportion of the public prioritising careful management of AI used in healthcare, by the military, or in banking and finance.</p>

<p>You must make sure that AI systems do not process personal data in ways that are unduly detrimental, unexpected or misleading to the individuals concerned. If AI systems infer data about people, you need to ensure that the system is accurate and is not discriminatory. You need to uphold the ‘right to be informed’ for individuals whose personal data is used at any stage of the development and deployment of AI systems. This is part of fulfilling the transparency and fairness principles.&nbsp;</p>

<p>Data protection aims to protect individuals’ rights and freedoms with regard to the processing of their personal data, not just their information rights. This includes the right to privacy but also the right to non-discrimination. For example, computer vision technologies such as facial recognition have raised concerns due to the risk of errors in matching faces. This technology has proven to be less accurate when used on women and people of colour, producing biased results. Ultimately, this can create discrimination, raising fundamental rights concerns because of the disadvantage to some individuals whose facial images are captured and processed.</p>

<p>People’s facial images constitute biometric data. This is personal data because it’s the result of specific technical processing related to physical, physiological or behavioural characteristics of a natural person, which can confirm the unique identification of the person. Facial images may fall into the <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/special-category-data/#scd1">special categories of personal data</a> because they’re likely to reveal sensitive characteristics such as racial or ethnic origin, and so require enhanced protection and additional safeguards.&nbsp;</p>

<p>Biometric data is also considered special category data when processed for the purposes of identification. You must ensure that the technologies used to capture and process this data are overt, accurate, proportionate, fair and deploy a narrow ‘zone of recognition’. For example, if someone walks past a camera and their image does not meet the threshold for a potential match, their data needs to be promptly deleted.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-15">Practical recommendations</h4>

  <ul>
    <li>Identify the risks to the rights and freedoms of individuals through <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/">DPIAs</a> and assess whether your processing is necessary, proportionate and fair to achieve your purpose.</li>
    <li>Use the ICO’s <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/ai-and-data-protection-risk-toolkit/">AI data protection and risk toolkit</a> to reduce the risks to individuals’ rights and freedoms.</li>
    <li>Mitigate risks using the ICO’s <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/what-about-fairness-bias-and-discrimination">guidance on fairness in AI systems</a>.</li>
    <li>Provide users with clear reassurance that you’re upholding their right to privacy, including simple processes to exercise their rights in clear <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/the-right-to-be-informed/what-methods-can-we-use-to-provide-privacy-information/">privacy notices</a>.</li>
    <li>Address any objections from users –&nbsp;for example, related to solely automated decisions, or where there’s a significant legal impact –&nbsp;by implementing safeguards such as meaningful human intervention, or an effective process to obtain and consider individuals’ views and corrections of factual errors.</li>
  </ul>
</div>

<h3 id="data-minimisation">Data minimisation</h3>

<p>The data minimisation principle requires you to identify the minimum amount of personal data you need to fulfil your purpose, and to only process that information and no more. This does not mean that AI tools should not process personal data, but if you can achieve the same outcome by processing less personal data then, by definition, the data minimisation principle requires you to do so.&nbsp;</p>

<p>Retaining data that is not strictly necessary is a risk to the individuals from whom the data is derived. Excluding irrelevant data prevents algorithms from identifying correlations that lack significance or are coincidental. There are a number of techniques that you can adopt to develop AI systems that process only the data you need, while still remaining functional.&nbsp;</p>

<p>For example, you can consider using privacy-enhancing technologies (PETs) to offer stronger protections and preserve data privacy while enabling effective use of data. Some PETs provide new tools for anonymisation, and some enable collaborative analysis on privately held data sets, allowing data to be used without disclosing copies of the data. PETs are multi-purpose: you can use them to reinforce data governance choices, or as tools for data collaboration and greater accountability through audits. A data-focused example solution is to create ‘synthetic data’. This is an artificial data set that does not include any actual data on ‘real’ individuals but mirrors in characteristics and proportional relationships all statistical aspects of the original data set.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-16">Practical recommendations</h4>

  <ul>
    <li>Justify your use of personal data, using your DPIA to think about the problem you’re solving so that you settle with the minimum personal data that’s required. Less personal data means less risk.</li>
    <li>Reduce the risk of individuals being identified through the processing of their personal data by using appropriate de-identification techniques (such as redaction, pseudonymisation and encryption).</li>
  </ul>

  <p>Refer to the ICO guidance on <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-sharing/privacy-enhancing-technologies/">privacy-enhancing technologies (PETs)</a>.</p>
</div>

<h3 id="storage-limitation">Storage limitation</h3>

<p>The UK GDPR states that you should only hold personal data as long as you can reasonably justify it for the purpose of your processing, and that you should not retain personal data longer than you need it. Think through:</p>

<ul>
  <li>what personal data the technology will hold</li>
  <li>why you have it and what it’s used for</li>
  <li>whether you can justify keeping it for that period of time</li>
</ul>

<p>You should map all personal data flows through every stage of development, testing and deployment, and utilise data minimisation, anonymisation techniques and eventual deletion to irreversibly transform or remove personal data.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-17">Practical recommendations</h4>

  <ul>
    <li>Use data minimisation and <a rel="external" href="https://ico.org.uk/media/about-the-ico/documents/4018606/chapter-2-anonymisation-draft.pdf">anonymisation techniques (PDF, 325KB)</a> as needed to remove or irreversibly transform personal data where possible.</li>
    <li>Be transparent about the length of personal data retention in privacy notices.</li>
  </ul>
</div>

<h3 id="human-oversight">Human oversight&nbsp;</h3>

<p>Although it is possible to use AI systems for automated decision making where the system makes a decision automatically without any human involvement, this may infringe the UK GDPR. Article 22 currently prohibits decision(s) based solely on automated processing that have <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/automated-decision-making-and-profiling/what-does-the-uk-gdpr-say-about-automated-decision-making-and-profiling/#:~:text=by%20automated%20means.-,What%20types%20of%20decision%20have%20a%20legal%20or%20similarly%20significant,law%2C%20such%20as%20housing%20benefit.">legal or similarly significant consequences</a> for individuals. Services using AI that affect a person’s legal status or their legal rights must only use AI to support decisions that must be made by a human decision maker.&nbsp;</p>

<p>AI systems need to introduce deliberation processes into all stages of the life cycle so that the abilities of humans and machines are combined to reach the best results when performing tasks. However, the human input needs to be ‘meaningful’. Several factors determine how much human involvement there should be in AI systems, such as the complexity of the output, its potential impact, and the amount of specialist human knowledge (for example, legal and medical) required.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-18">Practical recommendations</h4>

  <ul>
    <li>Design, document and assess the stages when meaningful human review processes are incorporated and what additional information will be taken into consideration when making the final decision.</li>
  </ul>

  <p>Use the ICO guidance on <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/automated-decision-making-and-profiling/what-does-the-uk-gdpr-say-about-automated-decision-making-and-profiling/">automated decision making under UK GDPR</a> for more clarity on types of decisions that have a legal or similarly significant effect.</p>
</div>

<h3 id="accuracy">Accuracy&nbsp;</h3>

<p>Accuracy in the context of data protection requires that personal data is not factually incorrect or misleading, and, where necessary, is corrected, deleted and kept up to date without delay.</p>

<p>You should not treat AI outputs as factual information about the individual, but instead consider these as a ‘statistically informed guess’. You also need to factor in the possibility of outputs being incorrect and the impact this may have on any decisions.&nbsp;</p>

<p>You need to make it explicit that the outputs of your AI systems are statistically informed guesses rather than facts, including information about the source of the data and how the inference has been generated.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-19">Practical recommendations</h4>

  <ul>
    <li>Test AI outputs against existing knowledge and expertise during training and testing.&nbsp;</li>
    <li>Be transparent that outputs are statistically informed guesses rather than facts.&nbsp;</li>
    <li>Document the source of the data and the AI system used to generate the conclusion.</li>
    <li>Implement processes to consider individuals’ feedback, views and corrections of factual errors.</li>
  </ul>
</div>

<h2 id="security">Security</h2>

<p>Cyber security is a primary concern for all government services, as laid out in the <a href="https://www.gov.uk/government/publications/government-cyber-security-strategy-2022-to-2030">Government Cyber Security Strategy</a>. When building and deploying new services, including AI systems, the government has a responsibility to make sure these are secure to use and also resilient to cyber attacks. To meet this requirement, your service must comply with the government’s <a rel="external" href="https://www.security.gov.uk/guidance/secure-by-design/">Secure by Design principles</a> before it can be deployed.&nbsp;</p>

<p>There are some security risks that apply uniquely to AI and/or generative AI technologies. This section takes you through some of these risks to help you keep AI solutions in government secure.&nbsp;</p>

<p>To learn more about AI security, you’re encouraged to join the <a href="mailto:x-gov-genai-security-group@digital.cabinet-office.gov.uk">cross-government AI security group</a> that brings together security practitioners, data scientists and AI experts. Please note that only those with GOV.UK email addresses can currently join this group.</p>

<h3 id="how-to-deploy-ai-securely">How to deploy AI securely&nbsp;</h3>

<p>Depending on how AI systems are used, they can present different security challenges and varying levels of risk that must be managed. This section covers some of the approaches that you need to take for:</p>

<ul>
  <li>public AI applications and web services</li>
  <li>embedded AI applications</li>
  <li>public AI application programming interfaces (APIs)</li>
  <li>privately hosted open source AI models</li>
  <li>working with your organisational data</li>
  <li>open-source vs closed-source models</li>
</ul>

<h4 id="public-ai-applications-and-web-services">Public AI applications and web services</h4>

<p>A simple way to implement an AI solution is to use publicly available commercial applications – such as Google Gemini or ChatGPT in the case of generative AI. While you might think that these public tools are more secure, you should consider that you cannot easily control the data input to the models: you must rely on educating users on what data they can and cannot enter into these services.</p>

<p>You also have no control over the outputs from these models, and you’re subject to their commercial licence agreements and privacy statements. For example, <a rel="external" href="https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance">OpenAI</a> will use the prompt data you enter directly into the ChatGPT website to improve their models, although individual users can opt out. When using public AI applications, you must not enter official information unless it has been published or is cleared for publication.</p>

<h4 id="embedded-ai-applications">Embedded AI applications</h4>

<p>Many vendors include AI features and capabilities directly within their products, for example Slack GPT and Microsoft Copilot. While this guidance applies at a high level to each of these applications, they come with their own unique security concerns. Before adopting any of these products it’s important to understand the underlying architecture of the solution, and what mitigations the vendor has put in place for the inherent risks associated with AI.</p>

<p>In addition to embedded applications, there are also many AI-powered plugins or extensions to other software. For example, Visual Studio Code has a large ecosystem of community-built extensions, many of which offer AI functionality. You must take extreme caution before installing any unverified extensions as these can pose a security risk.</p>

<p>There has also been a proliferation of AI transcription tools that are capable of joining virtual meetings and transcribing meeting notes. These present a serious risk of data leakage as they silently upload meeting recordings to an AI service for transcription and analysis. When hosting virtual meetings, organisers should verify the identity of all attendees and state up front that the use of third-party meeting transcription tools is not allowed.</p>

<p>You should always speak with your security team to discuss your requirements before deploying any embedded AI applications, extensions or plugins.</p>

<h4 id="public-ai-apis">Public AI APIs</h4>

<p>Many public AI applications offer the ability to access their services through APIs. By using the API you can integrate AI capabilities into your own applications, intercept the data being sent to the AI model, and also process the responses before returning them to the user.</p>

<p>For example, when integrating a large language model (LLM) through an API you can include privacy-enhancing technology (PET) to prevent data leakage, add content filters to sanitise the prompts and responses, and log and audit all interactions with the model. Be aware that PETs come with their own limitations, therefore selection of the PET should be proportionate to the sensitivity of the data.</p>

<p>Refer to the Information Commissioner’s Office (ICO)’s guidance on <a rel="external" href="https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-sharing/privacy-enhancing-technologies/">privacy-enhancing technologies (PETs)</a> and the Responsible Technology Adoption Unit (RTA)’s <a href="https://www.gov.uk/government/publications/privacy-enhancing-technologies-adoption-guide">PET adoption guide</a> for more information. Consider also that your data is still passed over to the provider when you use an API, although retention policies tend to be more flexible for API use. For example, OpenAI only <a rel="external" href="https://openai.com/enterprise-privacy#api-platform-faq">retains prompt data sent to the API for 30 days</a>.</p>

<h4 id="privately-hosted-ai-models">Privately hosted AI models</h4>

<p>Instead of using a public AI offering, the alternative is to host your own AI model. This could be a model taken from one of the many publicly available pre-trained open source models or it could be a model you have built and trained yourself. By running a model in your own private cloud infrastructure, you ensure that data never leaves an environment that you own.</p>

<p>In the case of generative AI, the models you can run in this way are not on the scale of the publicly available ones, but can still provide acceptable results for certain applications. The advantage is that you have complete control over the model and the data it consumes. The disadvantage is that you’re responsible for ensuring the model is secure and up to date. Consider that you must also maintain the infrastructure to host your model, which brings additional costs along with the specialist skills you’ll need in machine learning (ML) operations.</p>

<h4 id="managed-machine-learning-model-hosting-platform">Managed machine learning model hosting platform</h4>

<p>An alternative approach to setting up the infrastructure to host your own model from scratch, is to use a fully managed ML model hosting platform. For example, <a rel="external" href="https://aws.amazon.com/bedrock/">Amazon Bedrock</a> and <a rel="external" href="http://watsonx.ai">IBM watsonx.ai</a> allow you to host different open source or commercially available AI models and compare their performance, while <a rel="external" href="https://learn.microsoft.com/en-gb/azure/ai-services/openai/overview">Microsoft Azure OpenAI service</a> offers access to the OpenAI GPT models but running in a private instance with zero-day retention policies.</p>

<h4 id="running-ai-models-locally">Running AI models locally</h4>

<p>Many open source AI models are capable of being run locally on a single machine. This is often attractive because it allows the models to be run in isolation, with limited or no network access. This type of deployment is not recommended for most production services, but might be appropriate for ad-hoc or one-off applications where performance of the model is not paramount.&nbsp;</p>

<h4 id="training-ai-models">Training AI models</h4>

<p>In addition to where your AI model runs, you should also consider how it was trained because this is important from a security perspective. For example, many of the publicly available generative AI models were trained using data from the public internet. This means that they could include data that is personally identifiable, inaccurate, illegal or harmful, any of which could present a security risk.</p>

<p>It’s possible to train an AI model using your own data, and for many specific and limited tasks this is often the most appropriate approach because it gives you complete control of the training data. For generative AI models, the cost of doing this for larger and more capable systems is prohibitive and the amount of private data required to produce acceptable performance of a large model is beyond the capacity of most organisations. You should assume that any data you use to train your model could be extracted by an attacker. There’s more about this in the Data leakage section.</p>

<h4 id="working-with-your-organisational-data">Working with your organisational data</h4>

<p>A key application of AI is working with your organisation’s private data. By enabling the model to access, understand and use this data, insights and knowledge can be provided to users that are specific to their subject domain and will provide more reliable results. For a standard ML model, you can train them directly with your private data set. For generative AI models, you can fine-tune them or use approaches like retrieval augmented generation (RAG) to augment the model with your private data.</p>

<p>If you use your own data with an AI model, you immediately increase the data security risk and you need to apply additional security controls to stop data leakage and privacy violations.</p>

<p>Questions you should consider when using your own private data with an AI model include:</p>

<ul>
  <li>where is your data being sent and how is it being processed?</li>
  <li>is your data being used to train future models?</li>
  <li>how long is your data being retained?&nbsp;</li>
  <li>is your data being logged and who has access to those logs and for what purpose?</li>
</ul>

<h4 id="open-source-vs-closed-source-models">Open-source vs closed-source models</h4>

<p>Neither open-source or closed-source AI models are inherently less secure than the other. A fully open-source model may expose not only the model code, but also the weights of its parameters and the data used to train the model. While this increases transparency, it also potentially presents a greater risk, as knowing the weights and the training data could allow an attacker to create attacks carefully tailored to the specific model.</p>

<p>One benefit of fully open-source models is that they allow you to inspect the source code and model architecture, enabling security experts to audit the code for vulnerabilities. Despite this, because of its complexity, even an open source generative AI model remains mostly opaque and hard to analyse.&nbsp;</p>

<h3 id="security-risks">Security risks</h3>

<p>AI security risks are divided into 2 main categories: the risk of using AI and the risk of adversaries using AI against you.&nbsp;</p>

<h4 id="using-ai">Using AI</h4>

<p>There are many resources you can use to explore the risks of using AI:</p>

<ul>
  <li>the National Cyber Security Centre (NCSC) has published a set of <a rel="external" href="https://www.ncsc.gov.uk/collection/machine-learning">principles around securing machine learning (ML) solutions</a>
</li>
  <li>Microsoft has compiled a list of <a rel="external" href="https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning">ML failure modes</a>
</li>
  <li>
<a rel="external" href="https://atlas.mitre.org/matrices/ATLAS">MITRE Adversarial Threat Landscape for AI Systems (ATLAS) matrix</a> is an open source knowledge base of techniques used to attack AI systems</li>
  <li><a href="https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai">International Scientific Report on the Safety of Advanced AI has an analysis of risks posed by general purpose advanced AI systems</a></li>
  <li>the Open Worldwide Application Security Project (OWASP) has done significant work to identify the <a rel="external" href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">unique risks posed by LLMs</a> – these risks focus on the use of LLMs but many of them will also apply to other types of generative AI models and more widely to AI in general</li>
</ul>

<p>From a combination of these sources, we can draw out some of the most common vulnerabilities and discuss them in context of AI applications in government.</p>

<h5 id="data-and-model-poisoning">Data and model poisoning</h5>

<p>This is when data used to train an AI model has been tampered with, leading the model to produce incorrect or harmful output.</p>

<p>Attackers can target the data used to train an AI model to introduce vulnerabilities, backdoors or biases that compromise the model’s security and behaviour.&nbsp;</p>

<p>For a traditional ML model that uses a limited amount of training data for a specific task, this type of attack can be prevented by training the model yourself on a known data set that you control.&nbsp;</p>

<p>For frontier generative AI models, the barrier to entry for training a model from scratch is high and fine-tuning an existing model is much easier and cheaper. There are many open source models that are easy to fine-tune (for image generation, for example), and these can and are used to produce specific types of outputs, some of which are harmful or illegal.&nbsp;</p>

<p>When using an AI model – particularly a specialised, fine-tuned model – from a third-party source, it’s difficult to ascertain if it has been tampered with. Poisoned models may appear to be functioning as expected until a specific prompt triggers the malicious behaviour.</p>

<p>Supply chain vulnerabilities of this kind are not unique to AI. For example, when software libraries are hacked, all downstream systems that depend on those libraries are affected – a notable example of this was the <a rel="external" href="https://www.bleepingcomputer.com/news/security/dev-corrupts-npm-libs-colors-and-faker-breaking-thousands-of-apps/">Faker NPM hack</a>. Many automated tools exist for detecting, tracking and fixing security issues with open source software, but the current tooling for doing this with open source AI models is much more limited. Popular open source AI model site <a rel="external" href="https://huggingface.co/">Hugging Face</a> does have some <a rel="external" href="https://huggingface.co/docs/hub/security-malware">malware scanning tooling</a>, but this is not capable of determining if a given AI model has been trained on poisoned data.&nbsp;</p>

<p>AI model hosting platforms like Microsoft’s <a rel="external" href="https://azure.microsoft.com/en-gb/products/ai-studio/">Azure AI</a>, Amazon’s <a rel="external" href="https://aws.amazon.com/bedrock/">Bedrock</a> and IBM’s <a rel="external" href="http://watsonx.ai">watsonx.ai</a> allow developers to use commercial models and other third-party AI models. These services do not make any guarantees about the security and integrity of the third-party models that they’re capable of hosting.</p>

<p>Training data can also be poisoned indirectly through the introduction of malicious data into known collections of open data that are used to train or fine-tune frontier generative AI models. This is likely to become an increasing threat as hackers learn which publicly available data sets (for example, Wikipedia or Reddit) have been used to train generative AI models, and target these to poison future versions of the models.</p>

<p>The impacts of an AI model trained or fine-tuned with poisoned data are wide ranging, including direct security threats to the organisation running the model and biased or harmful outputs to the users of the model. Poisoned models could push people to particular products or subvert confidence in government services.&nbsp;</p>

<p>To help detect and prevent data poisoning, you’ll need to make sure your users and developers are trained on the risks and aware that the results from AI models can be false or biased. Outputs of models should be tested against known good responses and should be systematically tested for biases. ML hosting platforms often include evaluation tools that can measure and test the performance of an ML model, such as <a rel="external" href="https://cloud.google.com/vertex-ai/docs/evaluation/introduction">Google’s Vertex AI</a> or <a rel="external" href="https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2">Microsoft’s Prompt Flow</a>. Improved explainability of the models themselves would also help, enabling the output to be traced back to the source training data. For more information, refer to the Transparency and explainability section.</p>

<h5 id="data-leakage">Data leakage</h5>

<p>This is when responses from an AI model reveal confidential information, such as personal data.&nbsp;</p>

<p>AI models are trained using data. In the case of generative AI, data is commonly taken from the public internet and will contain personal data and other confidential information. AI models can suffer from data leakage, depending on their intended mode of operation. For example, a model that is being used as a classifier to rank or sort data into groups based on criteria would necessarily be less likely to leak training data than a generative AI model, as its outputs are confined to the specific classification problem. However, if an AI model has been trained or fine-tuned with private data that has different levels of security controls based on the user who should be seeing it, for example documents that are restricted to a specific group of people, there is currently no way to preserve these controls when training the model.</p>

<p>Generative AI models can also be <a rel="external" href="https://arxiv.org/abs/2311.17035">made to reveal their original training data</a> through their responses, meaning that any outputs from a generative AI model could potentially contain confidential information. For generative AI, a way to preserve user access controls is to use ‘in-context learning’. A search is carried out first on the private data a user is permitted to see, and the retrieved results are passed in context to the generative AI model. This type of approach is known as retrieval augmented generation (RAG), and is used in many commercial generative AI tools (such as Microsoft Copilot).&nbsp;</p>

<p>In-context learning has limitations and can degrade the performance of a generative AI model in certain applications. RAG tools are susceptible to indirect prompt injection, either in the information retrieved by the initial search or through the user prompt, meaning that security controls could be circumvented and private data could still be leaked.</p>

<p>You should make sure your AI model only has access to the data the user of the model should be able to access.</p>

<h5 id="insecure-ai-tool-chain">Insecure AI tool chain</h5>

<p>This refers to when tools used to train, fine-tune, host, serialise and operate AI models are not secure.&nbsp;</p>

<p>Specialised tools built to support AI models have been found to lack basic security features. For example, the Pickle format used to serialise ML models <a rel="external" href="https://huggingface.co/docs/hub/security-pickle">has serious security flaws</a>. This may be because the tools were developed at pace by AI researchers and data scientists not following secure coding practices. AI tools often have elevated access rights to the systems they’re running on, making the impact of a security breach even worse. This is not a new risk, as any developer tooling can be insecure, but AI tools appear to be particularly prone to security issues. It’s easier for a hacker to target the tool chain for an AI model than the model itself.</p>

<p>You should make sure your cybersecurity team has approved the tools you use to support your AI models. This includes checking that the tools implement user authentication and follow the principle of least privilege, meaning they are not running with administrator permissions to your system.</p>

<h5 id="exacerbates-previously-existing-risks">Exacerbates previously existing risks</h5>

<p>This refers to when the use of AI exacerbates previously existing risks, such as poor data management, insufficient security classification, insecure storage of credentials, and more.&nbsp;</p>

<p>An example of this sort of risk is over-privileged access. This happens when an AI tool is used to enhance enterprise-wide search capabilities. A user may not be aware of sensitive data that they currently have access to on a government system, but when they use an AI-enhanced search tool, the power of the tool exposes the lack of access controls and brings back sensitive data that the user was unaware of being able to see. The AI tool is not creating this issue: the problem already exists, but the AI tool is making it worse.&nbsp;</p>

<p>In line with the advice of vendors, you should review all enterprise access controls before deploying an AI tool to your system. This should be an ongoing exercise because no system is static. It’s essential that you’re able to continually monitor and review access controls when deploying AI applications across your organisation.</p>

<h5 id="perturbation-attack">Perturbation attack</h5>

<p>This is when an attacker stealthily modifies the inputs to an AI model to get a desired response.&nbsp;</p>

<p>An example of this type of threat is a computer vision (CV) system for medical diagnostics trained to distinguish between abnormal and normal scans. The system can be fooled when presented with an image containing specific amounts of noise, causing it to classify a scan incorrectly. Mitigations include adversarial training of the model with noisy images to improve robustness against this type of attack.</p>

<h5 id="prompt-injection">Prompt injection&nbsp;</h5>

<p>This is when hackers use prompts that can make the generative AI model behave in unexpected ways.&nbsp;</p>

<p>Prompt injection is a type of perturbation attack specifically targeted at generative AI systems that use text prompts to generate new content (text, image, audio, video). Developers lay down rules about how a model should behave and respond as system instructions, which are provided to the model along with the user prompt. Fundamentally, a generative AI model cannot distinguish between the user prompt and these system instructions because both are just seen as input to the model. A hacker can exploit this flaw by crafting special prompts that circumvent the system instructions, causing the model to respond in an unintended way.&nbsp;</p>

<p>The potential impact of prompt injections ranges from very mild,&nbsp;like a user making a banking chatbot tell jokes in the style of a pirate,&nbsp;to much more serious. For example, a hacker might trick a generative AI model designed to send alerts to patients about medical appointments into sending fake messages about non-existing appointments.</p>

<p>Prompt injections come in 2 forms:&nbsp;</p>

<ul>
  <li>direct, which means the user who is interacting with the generative AI model crafts the prompt injection themselves</li>
  <li>indirect, when other information that is being sent to a generative AI model is tampered with to include a prompt injection. For example, an email attachment can include a prompt and when a generative AI model that is tasked with summarising emails reads the attachment, the prompt injection is triggered</li>
</ul>

<p>Generative AI has the ability to take natural language inputs and have a machine act on them. A common pattern for using LLMs in this way is called ReAct (<a rel="external" href="https://arxiv.org/abs/2210.03629">Reason-Act</a>): the LLM is prompted to reason about how to perform a task, and the response from the model is processed and used to automate calls to different services that perform actions. Hackers can subvert this approach to make the model perform different actions, which significantly limits the utility of generative AI in fully automated solutions. To make sure the model is doing the right thing, there must be a human present to review the action before carrying it out. This is why the majority of commercial applications of generative AI are in the space of human assistants (‘copilots’).&nbsp;</p>

<p>Work is underway to address the prompt injection issue and a number of mitigations are already available. These include:</p>

<ul>
  <li>filtering prompts before they’re sent to the model (by sending the prompts through another ML model trained to detect likely prompt injections)</li>
  <li>filtering the outputs of the model before they’re returned to the user</li>
  <li>more <a rel="external" href="https://arxiv.org/html/2312.14197v1">speculative work</a> around fine-tuning models to better distinguish between user input and system prompts</li>
</ul>

<p>To defend against prompt injection, you should log all prompts sent to a model and carry out ongoing audits to determine if prompt injection is happening, blocking users you find who are responsible.</p>

<h5 id="hallucinations">Hallucinations</h5>

<p>Hallucinations are when the generative AI model responds with information that appears to be truthful but is actually false.</p>

<p>Counterintuitively for a machine, generative AI is better at creative tasks than fact retrieval. This is because all generative AI models predict and generate content by determining the most likely subsequent pattern based on previous training (for example, an LLM will predict the next most likely word). The models are therefore very good at generating plausible predictions that look correct but may not actually be correct. The risk is that overreliance by human operators on the outputs of generative AI models results in misinformation, miscommunication, legal issues and security vulnerabilities.&nbsp;</p>

<p>Fundamentally, generative AI models cannot be trusted to produce factual content. Any generative AI services that output generated content directly to the public – for example, an LLM-powered chatbot giving advice on a government website – would be prone to hallucination and could lead to someone being misled about a government service, policy or point of law.&nbsp;</p>

<p>A <a rel="external" href="https://bc.ctvnews.ca/air-canada-s-chatbot-gave-a-b-c-man-the-wrong-information-now-the-airline-has-to-pay-for-the-mistake-1.6769454">legal case in Canada</a> found an organisation that owned a site with a hallucinating chatbot financially responsible for the bad advice it dispensed. In the worst case, hallucination could even lead to direct harm if a user acted on faulty advice. For example, a user being advised not to seek medical attention when they needed to.&nbsp;</p>

<p>In addition to this direct risk, there’s also a significant indirect risk if officials are relying on generative AI as a primary information source when providing the public with guidance, advising ministers or informing policy decisions.&nbsp;</p>

<p>You should make sure your users are trained not to uncritically trust the outputs of generative AI or to rely exclusively on their responses. Specifically around cybersecurity, if security practitioners in government become overly reliant on the advice of generative AI assistants, they may become less effective at spotting novel attacks and may even be misled into following bad advice and exposing systems to increased cybersecurity risks.</p>

<h4 id="adversaries-using-ai">Adversaries using AI</h4>

<h5 id="misinformation">Misinformation</h5>

<p>This is when AI is used to create realistic synthetic media, leading to the spread of misinformation, undermining trust in digital media and manipulating public opinion.&nbsp;</p>

<p>Adversaries could use AI to interfere in electoral processes and spread misinformation. What makes this threat more potent is the ease with which bad actors can produce content for multiple audiences in many languages, with translations reflecting nuance and common parlance to make them more credible.</p>

<p>The technical ease with which generative AI models can be integrated with social media or other platforms also means that bad actors could spread misinformation automatically and at scale. At present, misinformation is the most pressing risk that AI (particularly generative AI) presents to governments, specifically relating to the integrity of democratic elections. There have already been a number of instances of <a rel="external" href="https://www.bbc.co.uk/news/world-us-canada-68064247">suspected AI-generated fake media</a> being deployed in the US. With the release of new, more powerful generative AI models capable of generating realistic video content, such as OpenAI’s <a rel="external" href="https://openai.com/sora">video generation model Sora</a>, this risk is only going to increase.</p>

<p>Big tech companies have committed to address the issue by including <a rel="external" href="https://arstechnica.com/ai/2023/07/openai-google-will-watermark-ai-generated-content-to-hinder-deepfakes-misinfo/">watermarks in the generated AI content</a> their models create, but so far this has not become widespread. <a rel="external" href="https://www.bbc.co.uk/news/technology-66618852">Google has announced a tool</a> that can detect and watermark AI-generated content. The <a rel="external" href="https://c2pa.org/">C2PA open standard</a> for embedding metadata into media content, which allows the source and provenance of the content to be verified, is also gaining some traction.</p>

<p>When building services that receive and process digital content (text, images or even video), you’ll need to consider the impact of that content being generated by AI and therefore being unreliable, misleading or malicious. For more information about the ethical implications of misinformation, refer to the Societal wellbeing and public good section.&nbsp;</p>

<h5 id="phishing">Phishing</h5>

<p>This is when generative AI is used to craft more convincing phishing emails and messages that can be tailored to specific user groups, leading to an increase in internet fraud.</p>

<p>LLMs make it easy for fraudsters to create convincing phishing emails and messages in different languages, even those they do not speak. LLMs can be easily automated to produce unique, targeted and personalised phishing emails at scale, making detection much harder. There is already <a rel="external" href="https://www.prnewswire.com/news-releases/fido-alliance-study-reveals-growing-demand-for-password-alternatives-as-ai-fuelled-phishing-attacks-rise-301957007.html">some evidence</a> that the amount of phishing emails and messages is rising, with the likely cause being the advent of generative AI.&nbsp;</p>

<p>Government is likely to see an increase in phishing emails and social engineering attacks as a result of generative AI. The risk of cyber security breaches through targeted, socially engineered attacks driven by generative AI could become more acute, as it may become easier to identify likely targets by using generative AI to trawl across social networks and other public resources, looking for contact details for government employees in sensitive roles.</p>

<p>To detect scams of this sort, you’ll need more sophisticated counter measures – for example, using another specially trained ML model to detect and block phishing emails produced by generative AI.&nbsp;</p>

<p>You’ll also need to educate users about how to detect AI-produced fake messages, because previous red flags such as badly formed sentences and incorrect spelling will no longer be enough. The likelihood is that phishing attacks will become more targeted, and use more sophisticated social engineering techniques to gain the recipient’s trust.&nbsp;</p>

<h5 id="cyber-attacks">Cyber attacks&nbsp;</h5>

<p>This refers to when generative AI lowers the bar for entry for hackers to create malware and craft cyber attacks that are more sophisticated and harder to detect.</p>

<p>Generative AI has proved to be highly capable at aiding developers to write effective code: the AI model provides the developer with the majority of the solution, including prerequisites and boilerplate code, leaving the developer only needing to finesse the final details. A hacker using a generative AI model specifically to create malware or craft a cyber attack is likely to more quickly and easily achieve a working attack.</p>

<p>All large commercial generative AI models have filters in place to try to detect if a user is asking the model to create malware. However, these filters can be subverted (refer to prompt injection threats). The expectation from some cyber security experts is that the number and sophistication of cyber attacks is likely to rise due to the use of generative AI, as many more bad actors who previously were excluded from being able to create credible threats are now able to do so.</p>

<p>The unique position of government, and the capabilities and desire of hostile state-sponsored groups, mean that this threat is likely to be a key concern for government cybersecurity teams. The potential for escalating levels of sophisticated cyber attacks fuelled by generative AI is real, although <a rel="external" href="https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/">research by Microsoft and OpenAI</a> has yet to observe any particularly novel or unique attacks resulting from the use of AI. The area is under constant review.</p>

<p>You should expect increased numbers of cyber attacks and take steps to increase your existing cyber security defences. For more information, refer to the <a rel="external" href="https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat">NCSC report on the near-term impact of AI on the cyber threat</a>.</p>

<h5 id="fake-official-correspondence">Fake official correspondence&nbsp;</h5>

<p>This is when generative AI is used to craft convincingly human correspondence which can either be automated and sent at scale to organisations, flooding their usual communications channels, or lead to unfair outcomes when judged against human correspondence.&nbsp;</p>

<p>An example of this kind of threat might be a hacker using generative AI to create thousands of requests for information from a government department, seemingly sent from multiple unique people. Similar to the phishing threat, the ability of LLMs to create convincing and plausible text in an automated way, at scale, makes this type of attack particularly concerning. A hacker could overwhelm an organisation’s normal communications channels, causing an organisation to spend time and money responding to seemingly genuine requests while degrading their ability to cope with real people.&nbsp;</p>

<p>Another example of this sort of threat, at a lower scale, is a fraudster submitting official information that will be used to judge a decision, and where the use of generative AI to create fake answers may prejudice the decision.</p>

<p>Areas of particular concern for the UK government are commercial procurement, recruitment, freedom of information requests, and the processing of claims that require an evidence-based decision.</p>

<p>Mitigations are similar to those for phishing or misinformation attacks – for example, processing all correspondence through another ML model trained to detect AI-generated content. When running services that result in decisions based on evidence provided through official correspondence, you should consider the potential impact on the service of the correspondence being AI generated.</p>

<h3 id="security-opportunities">Security opportunities</h3>

<p>In addition to the threats posed by AI there are also opportunities to improve cyber security through the use of AI. Some of these opportunities are:</p>

<ul>
  <li>threat detection: AI can be used to improve threat detection systems by generating synthetic cyber attack data for training more robust models, or directly detecting anomalies in real-time cyber security data. AI models can also be used to analyse historical cyber security data and identify patterns associated with known threats. These patterns can then be used to detect anomalies in real-time network traffic or system behaviour. When unusual activity occurs, the AI model can trigger an alert to be raised to human operators.&nbsp;</li>
  <li>incident response: AI models trained or fine-tuned on large amounts of historical cyber security data can predict future threats. By recognising subtle changes in patterns, they can be used to anticipate emerging attack vectors. Generative AI can assist in incident response by automating the generation of reports, recommending remediation actions based on past data, or filtering out noise in verbose cyber security logs, allowing human analysts to focus on the most important information.&nbsp;</li>
  <li>security testing: generative AI can create security test cases, improving the efficiency and coverage of security testing. Instead of manually crafting test scenarios, security professionals can use generative AI models to mimic adversary behaviour, simulate various attacks, and analyse existing vulnerabilities, attack patterns and system behaviours. LLMs are good at analysing code, meaning they can also be used to review source code, point out security flaws, and generate secure code snippets based on security best practices.&nbsp;</li>
  <li>enhancing vulnerability management: generative AI can assist in documenting security products. LLMs can be used to process the large amounts of documentation, guidance and online help around different security tools and their features and limitations, providing summarised information and enhanced search capabilities. Internet-enabled LLMs can also provide up-to-date insights, helping prioritise vulnerability patches and updates.</li>
</ul>

<h3 id="scenarios">Scenarios</h3>

<p>The scenarios discussed below build on the security risks identified in this section, and will help you understand how they apply to some of the applications of AI in government.&nbsp;</p>

<p>Each scenario includes descriptions of potential impacts and mitigations. The likelihood and impact of each risk is scored following the approach outlined in the <a rel="external" href="https://owasp.org/www-community/OWASP_Risk_Rating_Methodology">OWASP risk rating methodology</a>. In addition to the impact factors included in the OWASP approach, user harm and misinformation are discussed as significant impact factors.</p>

<p>This list of security threat scenarios is not exhaustive, but you can use the scenarios as a template for assessing the risks associated with different applications of AI.</p>

<ol>
  <li>Perturbation attack: an attacker stealthily modifies the inputs to an AI model to get a desired response: Identity verification using image and video capture technology.</li>
  <li>Insecure AI tool chain: tools used to train, fine-tune, host, serialise and operate AI models are not secure: Machine learning operations (MLOps) tools used with default configuration.</li>
  <li>Prompt injection threats: using prompts that can make the generative AI model behave in unexpected ways: LLM chatbot on a government website.</li>
  <li>Data leakage: responses from the LLM reveal sensitive information, for example personal data: Enterprise AI search tool summarising emails.</li>
  <li>Hallucinations: the LLM responds with information that appears to be truthful but is actually false: Developer uses LLM-generated code without review.</li>
</ol>

<div class="call-to-action">
  <h5 id="identity-verification-using-image-and-video-capture-technology">Identity verification using image and video capture technology</h5>

  <h6 id="scenario">Scenario</h6>

  <p>A government service requires users to prove their identity by capturing an image of an identity document containing their picture, such as a passport or driving licence. A CV AI system then compares this image to a live video clip of the person to verify that the person is who they claim to be. A malicious user uses AI deepfake technology to insert their own image over the genuine photo of another person on a stolen identity document. The CV system is tricked into wrongly verifying that the malicious user’s identity matches the credentials on the other user’s identity document.</p>

  <h6 id="impact">Impact</h6>

  <p>Misidentification of the true user, leading to identity fraud.</p>

  <p>Fraudulent access to a government service.</p>

  <p>Data loss of personal and confidential data about the genuine user.</p>

  <p>Serious security breach if the service provides access to sensitive government information.</p>

  <h6 id="mitigation">Mitigation</h6>

  <p>Ensure the service only uses biometric identity documents. For example, biometric passports contain electronic passport photos which can be securely transferred to the service for verification.&nbsp;</p>

  <p>Use a trusted third-party service to look up and provide reference images for identity documents, rather than relying on images captured by users themselves. For example, the service could use licence images stored in the DVLA system and check this against the live video image.</p>

  <p>Use deepfake detection methods to scan input digital images and video clips which are received by the service.&nbsp;</p>

  <h6 id="risk-rating">Risk rating</h6>

  <p>Likelihood: MEDIUM</p>

  <p>Impact: HIGH</p>

  <h6 id="recommendation">Recommendation</h6>

  <p>In this specific example, use of a biometric passport can prevent the attack. However, if the deepfake were applied to the live video clip of the user instead of the image of the identity document, the system could still be fooled. Access to this type of deepfake technology is becoming increasingly available, meaning that when you build services to receive and process images or video, you must put in place mechanisms to detect if the content has been manipulated by AI.</p>
</div>

<div class="call-to-action">
  <h5 id="mlops-tools-used-with-default-configuration">MLOps tools used with default configuration&nbsp;</h5>

  <h6 id="scenario-1">Scenario</h6>

  <p>A data scientist working in a government organisation wants to experiment with training their own ML model using organisational data. The experiment aims to test whether the model can be used to triage official correspondence, improving efficiency. The data scientist starts by using an open source MLOps tool to host and deploy their ML model in their local environment. The default configuration of the tool exposes a public endpoint with no authentication on the public internet. By default, the tool runs with administrator permissions on the host machine. A hacker discovers the exposed endpoint and sends commands to the MLOps tool, using it to gain a foothold in the organisation’s network.</p>

  <h6 id="impact-1">Impact</h6>

  <p>Serious security breach, which could lead to catastrophic damage to the organisation’s computer systems.</p>

  <p>Data loss, including the organisational data used to train the ML model, and other sensitive data that can be accessed through the tool’s elevated permissions.</p>

  <h6 id="mitigation-1">Mitigation</h6>

  <p>Check the default configuration of all ML tools before deploying them and ensure basic security controls are in place:<br>
- authentication is enabled
- no public-facing endpoints are exposed unless explicitly required
- the principle of least privilege is applied so that tools only run with the minimum permissions they require</p>

  <h6 id="risk-rating-1">Risk rating</h6>

  <p>Likelihood: MEDIUM</p>

  <p>Impact: HIGH</p>

  <h6 id="recommendation-1">Recommendation</h6>

  <p>AI tools should be treated in the same way as all other third-party software. Even when they’re being used for experimentation, secure by design principles should always be applied.</p>
</div>

<div class="call-to-action">
  <h5 id="llm-chatbot-on-a-government-website--full-chat-interface">LLM chatbot on a government website – full chat interface</h5>

  <h6 id="scenario-2">Scenario</h6>

  <p>A chatbot deployed to a government website to assist with queries relating to a particular public service. The chatbot uses a private instance of one of the publicly trained LLMs. The user’s question is combined with system instructions that tell the LLM to only respond to questions relevant to the specific service. The system instructions are combined with the user’s original question and sent to the LLM. A malicious user could craft a specific prompt that circumvents the system instructions and makes the chatbot respond with irrelevant and potentially harmful information.&nbsp;</p>

  <p>This is an example of a direct prompt injection attack.</p>

  <h6 id="impact-2">Impact</h6>

  <p>Actual risk of user harm if a user is tricked into using an unsafe prompt that then results in harmful content being returned and acted on. For example, a user looking for information on how to pay a bill is directed to a fraudulent payment site.</p>

  <p>Reputational damage to the government if a user made public potentially harmful responses received from the chatbot – for example, a user asking for generic information and receiving an inflammatory response.</p>

  <h6 id="mitigation-2">Mitigation</h6>

  <p>Use prompt engineering to attach a meta prompt to any user input to prevent the LLM from responding to malicious input.</p>

  <p>Apply content filters trained to detect likely prompt injections to all prompts sent to the LLM.</p>

  <p>Choose a more robust model: some models have been shown to be more resistant to this kind of attack than others.</p>

  <p>None of these mitigations are sufficient to guarantee that a prompt injection attack would not succeed. Fundamentally, an LLM cannot distinguish between user input and system instructions. Both are processed by the LLM as natural language inputs so there is no way to prevent a user prompt affecting the behaviour of the LLM.</p>

  <h6 id="risk-rating-2">Risk rating</h6>

  <p>Likelihood: HIGH</p>

  <p>Impact:&nbsp;</p>

  <p>LOW – response is returned to a single user with limited repercussions.&nbsp;</p>

  <p>HIGH – response causes actual harm to a user.</p>

  <h6 id="recommendation-2">Recommendation</h6>

  <p>Deploying an LLM chatbot to a public-facing government website comes with a significant risk of a direct prompt injection attack. You should consider the impact of an attack like this in the context of the specific use case. A chatbot deployed in a limited function or in controlled conditions – by restricting the number of users, for example – is far lower risk than one that is more widely available.</p>
</div>

<div class="call-to-action">
  <h5 id="enterprise-ai-search-tool-summarising-emails">Enterprise AI search tool summarising emails</h5>

  <h6 id="scenario-3">Scenario</h6>

  <p>A hacker sends a malicious email attachment to a government recipient who is using an enterprise generative AI tool to assist them. The tool uses a retrieval augmented generation (RAG) pattern, searching the private data the recipient can access and sending relevant data in-context to an LLM. The tool searches the recipient’s inbox, including their unread emails and attachments, for relevant information. The tool passes the prompt injection contained in the attachment to the LLM along with other private data. The prompt injection causes the LLM to respond by summarising the private data in the form of an obfuscated link to a third-party website. When returned to the recipient, the link may, depending on the tools being used, automatically unfurl a preview and instantly exfiltrate the private data to the third-party website.</p>

  <h6 id="impact-3">Impact</h6>

  <p>Data loss: confidential information contained in the user’s emails is transferred to a third party.&nbsp;</p>

  <p>Reputational damage to the department due to loss of data.</p>

  <p>Regulatory breaches with financial consequences.</p>

  <h6 id="mitigation-3">Mitigation</h6>

  <p>Configure the enterprise AI tool so that unread emails and attachments are not included in the initial search.</p>

  <p>Apply filters before the in-context data is added to the prompt to remove likely prompt injections.</p>

  <p>Apply filters to the response generated by the LLM to ensure any links contained in it are only to known resources.</p>

  <p>Ensure network controls are enforced that prevent applications making calls to dangerous URLs.</p>

  <h6 id="risk-rating-3">Risk rating</h6>

  <p>Likelihood: LOW</p>

  <p>Impact: HIGH</p>

  <h6 id="recommendation-3">Recommendation</h6>

  <p>In this scenario, indirect prompt injection in an email attachment can be used to perform data exfiltration without any action required by the user. Similar <a rel="external" href="https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/">data exfiltration techniques have already been shown to work</a> against commercial LLMs. With the increased adoption by government departments of enterprise AI tools, we will likely see more of these novel generative AI-specific cybersecurity threats.</p>
</div>

<div class="call-to-action">
  <h5 id="developer-uses-llm-generated-code">Developer uses LLM-generated code</h5>

  <h6 id="scenario-4">Scenario</h6>

  <p>A developer uses a public LLM to answer coding questions and receives advice to install a specific software package, ArangoDB, from the JavaScript package management system npm. When the LLM was trained, the package did not exist. A hacker has previously interrogated the LLM with common coding questions and identified this hallucination. They then created a malicious package with the fictitious name and registered it with the package management system. When the developer installs the package, they receive the malicious code.</p>

  <h6 id="impact-4">Impact</h6>

  <p>Unauthorised code execution when the software containing the fake package is deployed and run. This could result in significant data loss and other serious consequences.</p>

  <h6 id="mitigation-4">Mitigation</h6>

  <p>Do not rely on the responses of the LLM: double-check all outputs before including them in your code. Check all package dependencies of your code before deployment. Use an automated tool such as a ‘dependabot’ or ‘snyk’ to scan for supply chain vulnerabilities.</p>

  <h6 id="risk-rating-4">Risk rating</h6>

  <p>Likelihood: LOW</p>

  <p>Impact: HIGH</p>

  <h6 id="recommendation-4">Recommendation</h6>

  <p>If developers follow secure coding best practices, the risk should never arise because all dependencies should be checked before deployment. Over-reliance on LLM-generated code without sufficient human oversight is likely to become an increasing risk. Treat all LLM-generated code as inherently insecure and never use it directly in production code without first doing a code review.</p>

  <h6 id="references">References</h6>

  <p><a rel="external" href="https://vulcan.io/blog/ai-hallucinations-package-risk">Can you trust ChatGPT’s package recommendations?</a></p>
</div>

<div class="call-to-action">
  <h4 id="practical-recommendations-20">Practical recommendations</h4>

  <h5 id="applies-to-ai">Applies to AI</h5>

  <ul>
    <li>Design risk-driven security, taking account of the failure modes for the type of AI you’re using – for example, <a rel="external" href="https://owasp.org/www-project-top-10-for-large-language-model-applications/">OWASP Top 10 security risks for LLMs</a> or the <a rel="external" href="https://atlas.mitre.org/matrices/ATLAS">ATLAS Matrix</a>.</li>
    <li>Use a consistent risk rating methodology to assess the impact and likelihood of each risk – for example, the <a rel="external" href="https://owasp.org/www-community/OWASP_Risk_Rating_Methodology">OWASP risk rating methodology</a>.</li>
    <li>Minimise the attack surface by only using the AI capabilities you require – for example, by not sending user input directly to an LLM.</li>
    <li>Defend in depth by adding layers of security – for example, by using PET to prevent data leakage and adding content filters to sanitise output of an AI model.</li>
    <li>Never use private data that needs different levels of user access permissions to train or fine-tune an AI model.</li>
    <li>When building services that receive and process text, images or video, take steps to validate inputs to detect if the content has been generated by AI and could be unreliable, misleading or malicious.</li>
    <li>Review all enterprise access controls before deploying an AI tool to your environment to make sure users can only access the data they have permission to view.&nbsp;</li>
    <li>Never enter any official information directly into public AI applications or APIs unless it’s already publicly available or cleared for publication. Exceptions may apply for specific applications with different data handling terms provided under commercial licences, for example Microsoft Copilot.</li>
    <li>When experimenting with AI tools, pay attention to security and never assume default configurations are secure.</li>
  </ul>

  <h5 id="applies-to-generative-ai">Applies to generative AI</h5>

  <ul>
    <li>Avoid using generative AI where it’s not appropriate or required. Ask yourself if a non-AI solution or a traditional ML model trained for a specific purpose could work just as well.</li>
    <li>Prevent generative AI responses automatically leading to destructive or irreversible actions, such as sending emails or modifying records. In these situations a human must be present to review the action.</li>
    <li>Avoid using links to external resources in LLM responses that will be read by humans. If external links are provided, the response must be filtered to remove malicious URLs.</li>
    <li>Train your users not to trust the outputs of generative AI or rely exclusively on generated responses.</li>
    <li>Treat all LLM-generated code as inherently insecure and never use it directly in production without code review.</li>
    <li>Avoid putting LLM chatbots on public-facing government websites unless the risk of direct prompt injection is acceptable under the specific use case.</li>
    <li>When hosting virtual meetings, organisers should verify the identity of all attendees and state up front that the use of third-party AI meeting transcription tools is not allowed.</li>
  </ul>
</div>

<h2 id="governance">Governance</h2>

<p>To successfully develop an AI programme, you’ll need strong governance processes because of the risks related to lawfulness, security, bias and data. Whether these processes are already built into your existing governance frameworks or implemented as a new governance framework, they should focus on:</p>

<ul>
  <li>continuous improvement through the inclusion of new knowledge, methods and technologies&nbsp;</li>
  <li>identifying and working with important stakeholders representing different organisations and interests, including Civil Society Organisations (CSOs) and sector experts. This will help create a balanced view throughout the life cycle of any AI project or initiatives</li>
  <li>planning for the long-term sustainability of AI initiatives, considering scalability, long-term support, maintenance, ongoing stakeholder involvement and future developments</li>
</ul>

<p>You should manage governance of AI through an AI governance board or AI expert representation on an existing governance board. You can include an ethics committee as part of your governance framework, depending on your operating context. Each has different roles and responsibilities.&nbsp;</p>

<h3 id="ai-governance-board-or-ai-representation-on-an-existing-board">AI governance board or AI representation on an existing board</h3>

<p>The role of an AI governance board or representation on a board is to provide oversight, accountability and strategic guidance to help the organisation or team make informed decisions about AI adoption and use. It covers aspects such as risk management, compliance, assurance, resource allocation, stakeholder engagement, and aligning with business objectives and ethical principles.</p>

<p>An AI governance board helps you make sure your project is on track and that strategic, legal, ethical and operational risks are managed.</p>

<h3 id="ethics-committee">Ethics committee&nbsp;</h3>

<p>The primary focus of an ethics committee is to assess the ethical implications of various actions, projects and decisions about AI within an organisation or programme. It evaluates AI from an ethical standpoint, focusing on values such as fairness, transparency and privacy, and is more specialised than that of an AI governance board.</p>

<p>An ethics committee usually includes legal experts, representatives from other relevant organisations related to the service you’re delivering, community members and stakeholders – all of whom can provide a specialised perspective on ethical matters such as health or security issues.&nbsp;</p>

<p>Before creating an ethics committee, you should consider the ethical, strategic and operational context of your organisation or programme. For example, the department may be too small or the programme too low risk to have a committee like this. It might be sufficient to have an AI governance board or an AI expert representative on a programme board to help you manage ethical considerations. An AI governance board should be able to guide you on whether you need an ethics committee. Refer to the Ethics section for more information.</p>

<h3 id="creating-an-ai-systems-inventory">Creating an AI systems inventory</h3>

<p>To provide a comprehensive view of all deployed AI systems within an organisation or programme, organisations should set up an AI and machine learning (ML) systems inventory. This is in addition to the <a href="https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub">Algorithmic Transparency Recording Standard (ATRS)</a> that all government departments and certain arm’s length bodies must use to ensure public transparency around the algorithmic tools used in their decision-making processes.&nbsp;</p>

<p>A live inventory will help your team, your organisation and your stakeholders understand the scope and scale of AI usage. It does this by providing better oversight and awareness of any potential risks such as data quality, model accuracy, bias, security vulnerabilities and regulatory compliance. An inventory will also be helpful for audit purposes.</p>

<p>The inventory should be regularly kept up to date with:</p>

<ul>
  <li>a description of each system’s purpose, usage and associated risks&nbsp;</li>
  <li>details like data elements, ownership, development and key dates&nbsp;</li>
  <li>use protocols, structures and tools for maintaining an accurate, detailed inventory</li>
</ul>

<p>You should consider sharing your inventory with the <a href="https://www.gov.uk/service-manual/communities/artificial-intelligence-community">AI community of practice</a>. This will enable the community to support your work and connect you with the teams that have developed similar projects across government so that you can share expertise and best practices and possibly reuse existing solutions.</p>

<h3 id="governance-structures-for-teams">Governance structures for teams</h3>

<p>For all programmes or services that use AI systems, teams should:</p>

<ul>
  <li>set out how the AI model will be maintained and managed over time</li>
  <li>develop a comprehensive plan for knowledge transfer, and for training new and existing staff to ensure the model’s sustainable management</li>
  <li>establish clear roles and responsibilities to ensure accountability within teams, including who has the authority to change and modify the code of the AI model</li>
  <li>ensure diversity within the project team by incorporating a range of subject matter expertise, skills and lived experiences</li>
  <li>establish pathways for escalation and identify key points of contact for specific AI-related issues</li>
  <li>adopt a risk prioritisation plan with specific project controls throughout the delivery and post-delivery cycle, such as how you will evaluate data sets for bias</li>
  <li>establish a data reporting mechanism that captures how data flows are managed and maintained throughout the delivery and post-delivery cycle</li>
  <li>set out how the programme or project team will work with and report to their programme board(s) and the ethics committee, if one has been set up</li>
</ul>

<h3 id="managing-risk">Managing risk</h3>

<p>Risk management is part of governance. It helps you to strategically plan and manage your AI project to achieve objectives and respond to challenges in an agile way.&nbsp;</p>

<p>A risk assessment is critical in ensuring that AI projects are only undertaken if the potential benefits outweigh the risks. You should base this threshold on an objective assessment of the project’s potential risks and benefits, defining acceptable levels of risk and ensuring that any possible risks are identified and addressed early in the project life cycle. Relevant laws, regulations and ethical considerations should inform the assessment. If you’re managing AI programmes as part of a portfolio of work, <a rel="external" href="https://assets.publishing.service.gov.uk/media/6453b363c33b460012f5e6bf/Portfolio_Risk_Management_Guidance_Orange_Book_Annex.pdf">The Orange Book (Portfolio Risk Management Guidance (PDF, 1,958KB)</a>) provides a complete overview of risks.&nbsp;</p>

<p>As part of the design of your project, you should conduct a risk assessment to understand the risks and their potential to cause harm to individuals or groups, as well as the likelihood of the AI service being misused or exploited. The impact should be calculated based on the complexity of the AI system, the quality of the data used to train the system, and the potential for human error or malicious intent.</p>

<p>When conducting your assessment, you should consider a number of risks around AI including security, managing bias, legal and operational risks. Consider also that the scale of autonomy of an AI service can increase operational risks. For example, in the case of autonomous vehicles, the Society of Automotive Engineers’ <a rel="external" href="https://www.sae.org/blog/sae-j3016-update">Levels of Driving Automation</a> ranks autonomy on a scale from 0 (no autonomy) to 5 (full automation for all features under all conditions). This scale correlates to the level of risk.&nbsp;</p>

<p>Alongside the risk assessment, you need to create a robust risk management framework that sets out defined roles and responsibilities and includes clear escalation routes to help mitigate risks.</p>

<h4 id="mitigating-risks">Mitigating risks</h4>

<p>You can mitigate some risks related to how the AI service performs by building or establishing programme and technical guardrails (best practices). These will guide the design, implementation and operation of an AI service or application, and are an essential element of delivering great services.</p>

<p>In the case of autonomous AI services that make decisions in areas such as social care or healthcare, the impact of autonomy of an AI service can be mitigated by including human intervention. These decisions need to be made in a controlled environment so as to not reintroduce bias into the AI service.</p>

<p>Whether your AI service is autonomous or includes elements of human intervention, it should be evaluated throughout the all stages of the project life cycle – including design, development and operation. Your risks and mitigation strategy should also cover how your team will manage continuous performance monitoring to prevent biased or inaccurate outputs.</p>

<p>There are also security and data protection risks which are covered in detail in the Security and Data protection and privacy sections.</p>

<h3 id="ai-quality-assurance">AI quality assurance</h3>

<p>AI quality assurance ensures that the AI service meets the service level requirements and provides evidence that the service is fit for purpose. It helps you check that robust techniques have been used to build, test, measure and evaluate AI systems. It also helps organisations communicate that their systems are trustworthy and aligned with relevant regulatory principles. It should be used throughout the AI service life cycle, including during testing and validation in the development phase and monitoring once the AI service is being used.</p>

<p>To meet quality assurance requirements, AI systems must be trustworthy, accountable, transparent and robust. They must ensure safety, respect privacy, mitigate bias, ensure fairness, and be secure and resilient. Given the complexity of AI systems, you may require a toolbox of different products, services and standards to ensure their effectiveness. For example, the Department for Science, Innovation, and Technology (DSIT)’s <a rel="external" href="https://assets.publishing.service.gov.uk/media/65ccf508c96cf3000c6a37a1/Introduction_to_AI_Assurance.pdf">Introduction to AI assurance (PDF, 1,419KB)</a> identifies the key elements of an assurance process – including risk assessment, impact assessment, bias audit, compliance audit, conformity assessment and formal verification.&nbsp;</p>

<h4 id="validation-of-ai">Validation of AI</h4>

<p>Being able to assert the quality of an AI service is critical to ensuring the safety of the system and the reliable accuracy of the service.</p>

<p>You must ensure that any updates to the AI system have a quantitative testing and validation process as part of the change control process. Validation is part of the testing of an AI system and is the ‘confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled’ (source: <a rel="external" href="https://www.iso.org/standard/45481.html">ISO9000:2015</a>).</p>

<p>Deployment of AI systems that are inaccurate, unreliable, or poorly generalised to data beyond their training creates and increases negative AI risks and reduces trustworthiness. You should consider the complexity of your AI systems and identify the different products, services and standards necessary to ensure their effectiveness. To do so, you must make sure that these products and services comply with the required standards as defined by standards development organisations (SDOs), such as the <a rel="external" href="https://www.iso.org/home.html">International Standards Organisation (ISO)</a>.</p>

<h4 id="operational-monitoring">Operational monitoring</h4>

<p>Once you’ve released your AI system for use and it’s operational, you should have ongoing performance monitoring in place. This will ensure your system is operating as expected, and you should be able to provide evidence of this. It will also help you to identify and manage any changes to the model.</p>

<p>Any updates to the model need to go through a managed release process. This will help you mitigate the impact of any process change and clearly document changes made for future reference. You should ensure that the release can be withdrawn and the system reverted to an earlier version if required.</p>

<p>As systems and environments evolve, the current process may diverge sufficiently from the training period of the AI system. This is known as model drift and may require retraining or implementation of a new model within the AI system. Close monitoring is essential so that you can catch this as early as possible and reduce possible disruption to the AI system.</p>

<div class="call-to-action">
  <h4 id="practical-recommendations-21">Practical recommendations</h4>

  <ul>
    <li>Connect with your organisation’s assurance team and review the <a href="https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques">Portfolio of AI assurance techniques</a>.&nbsp;</li>
    <li>Set up an AI governance board or include AI experts on existing governance boards.</li>
    <li>Consider setting up an ethics committee made up of internal stakeholders, cross-government stakeholders, sector experts and external stakeholders like Civil Society Organisations.&nbsp;</li>
    <li>Set up an AI/ML systems inventory to provide a comprehensive view of all deployed AI systems within your department.&nbsp;</li>
    <li>Make sure your programme or project teams have clear governance structures in place.</li>
    <li>Evaluate your AI product throughout the development and project life cycle, identify risks and implement a robust mitigation strategy.</li>
    <li>Use quality assurance techniques to make sure your AI product is trustworthy, accountable, transparent, robust, secure and resilient, and respects privacy, mitigates bias and ensures fairness.</li>
    <li>Make full use of the training resources available, including the courses on <a rel="external" href="https://learn.civilservice.gov.uk/courses/oPEgywmEQumlVAmXlgnRqw">the business value of AI</a> and <a rel="external" href="https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw">understanding AI ethics</a> on Civil Service Learning.</li>
  </ul>
</div>

<h2 id="appendix-example-ai-use-cases-in-the-public-sector">Appendix: example AI use cases in the public sector&nbsp;</h2>

<p>This section includes sample case studies provided by teams that have implemented AI solutions in government departments and public sector organisations. These are real-life examples of AI adoption, presenting the technologies that were deployed, reflections on their capabilities and shortcomings, and discussions of the challenges and risks encountered in each project.&nbsp;</p>

<p>Be aware that these case studies were submitted in spring 2024 and only discuss the aspects of AI projects that each team deemed most relevant and interesting. They are not exhaustive and should not be treated as formal advice. You can find guidance in the Building AI solutions section of this playbook.</p>

<p>You’re encouraged to share your AI project with the <a href="https://www.gov.uk/service-manual/communities/artificial-intelligence-community">AI community of practice</a> to be considered for future updates of this appendix. This will also enable the community to connect you with the teams that have developed similar projects in other departments so that you can share expertise and best practice.</p>

<h2 id="govuk-chat-experimenting-with-generative-ai">GOV.UK Chat: experimenting with generative AI&nbsp;</h2>

<p>GOV.UK Chat is a pilot tool that uses relevant website content to generate responses, aiming to simplify navigation across more than 700,000 pages on GOV.UK and help users find the information they need. The project highlighted the importance of a careful and phased development approach. The experience gained from this prototype led the Government Digital Service (GDS) to focus on enhancing the system’s accuracy with the objective of launching a limited public pilot, provided that accuracy thresholds could be met.</p>

<h3 id="our-tool-and-the-problem-it-solves">Our tool and the problem it solves</h3>

<p>GOV.UK Chat uses a retrieval augmented generation (RAG) approach. This allows users to engage with GOV.UK content through natural language queries. We preferred an RAG approach to fine-tuning a large language model (LLM) because GOV.UK content is updated regularly with edits and new pages published. The initial version of GOV.UK Chat was set up to test how we could improve user interaction with our website. We chose ‘business’ as our focus area because it’s complex for a user, involving many different departments’ policies and content.&nbsp;</p>

<h3 id="project-development">Project development</h3>

<p>The project was managed through a series of phased experiments, each lasting a couple of weeks and focusing on iterative development and evaluation. This approach facilitated controlled experimentation and rapid data gathering for system refinement.</p>

<h3 id="measuring-success">Measuring success</h3>

<p>The efficacy of GOV.UK Chat was evaluated using a multifaceted approach to ensure the reliability and accuracy of its responses. Content designers from GOV.UK and across government assessed the answers generated by the system. This involved exploring how GOV.UK Chat processed user queries, understood the intent behind questions, and retrieved relevant GOV.UK pages to inform its responses.&nbsp;</p>

<p>Experts then evaluated the appropriateness of the questions posed, the system’s interpretation of them, and the factual accuracy and completeness of answers provided.</p>

<h3 id="value-delivered">Value delivered</h3>

<p>Available through specific ‘magic links’, GOV.UK Chat was tested by hundreds of users in a controlled environment. Feedback suggested a preference for GOV.UK Chat over traditional search and navigation methods. This highlighted the convenience of direct question-answering systems, particularly for users with more complex questions.&nbsp;</p>

<p>However, the experiment also raised concerns about the reliability of the information provided, as the system occasionally produced ‘hallucinated’ responses.&nbsp;</p>

<p>Satisfaction surveys were sent out to users after use. Notably, nearly 70% of users found the chatbot’s responses useful and about 65% were satisfied with their experience. The primary value of this experiment is in what we learned from observing user interactions, understanding the types of queries posed, common failure modes, and assessing the feasibility of integrating such a system into GOV.UK more broadly.</p>

<h3 id="technologies">Technologies&nbsp;</h3>

<p>The development of GOV.UK Chat was underpinned by a selection of modern cloud and AI technologies, primarily hosted on Google Cloud. This choice was not exclusive, in that similar setups could be effectively implemented on alternative, functionally equivalent cloud services.&nbsp;</p>

<p>The answer-generation part of the system was powered by direct application programming interface (API) calls to OpenAI, leveraging its advanced generative LLMs for dynamic answer generation. Additionally, we used a <a rel="external" href="https://qdrant.tech/">Qdrant vector store</a> to facilitate efficient data retrieval and management to support the RAG approach.</p>

<h3 id="challenges-and-solutions">Challenges and solutions</h3>

<p>It’s important to acknowledge the rapid evolution of the technology in the generative AI domain. When the GOV.UK Chat system was initially conceived and developed (July 2023), the options for technologies like this were relatively limited.&nbsp;</p>

<p>In our first experiment we used a simplistic approach to retrieval, whereby we returned whole pages. This approach occasionally resulted in errors because a niche search might return lots of very long pages that would exceed the LLM’s token limit, resulting in an error.</p>

<p>In response, we started exploring improvement strategies, specifically through nuanced chunking, alternative embedding models, vectorisation strategies, re-ranking and improved few-shot examples in the prompt. With an answer accuracy of 80%, the experiment suggested that accuracy gains would be required prior to the product being deployed for users on the site.</p>

<p>If we’re to scale GOV.UK Chat into a full live pilot, a major challenge will be quality assurance. The techniques we used in the first stage of the work are highly manual and not practical to scale. We believe a solution will be to build a knowledge base of quality-assessed questions so that a semi-automated quality assurance mechanism can be developed.</p>

<h3 id="example-ethical-legal-and-security-considerations">Example ethical, legal and security considerations</h3>

<p>Safeguards were implemented to protect user privacy and prevent personal data submission. We complied with UK data protection legislation and conducted a thorough data protection impact assessment, removing any personal data from GOV.UK pages accessible to the LLM. Additionally, collaboration with cyber security experts from CDDO, Number 10DS and i.AI through ‘red teaming’ helped identify and mitigate system vulnerabilities, reinforcing our commitment to responsible and secure technology deployment. For more details, refer to the <a rel="external" href="https://insidegovuk.blog.gov.uk/2024/01/18/the-findings-of-our-first-generative-ai-experiment-gov-uk-chat/">GOV.UK Chat blog</a>.</p>

<h2 id="govuk-chat-doing-user-research-for-ai-products">GOV.UK Chat: doing user research for AI products&nbsp;</h2>

<p>This case study focuses on the user research we conducted in the Government Digital Service (GDS) to help the team developing the GOV.UK Chat tool explore user reactions and evaluate the system with experts. As an experimental team, we also wanted to learn about appropriate tools and approaches to develop GOV.UK Chat.</p>

<p>Initially, we focused on testing the accuracy of the tool using surveys, presenting typical user questions and answers for subject matter experts (SMEs) to evaluate. We supported this assessment with a round of online testing with internal users from the content design community to gain feedback on the initial design of the chatbot interface. This approach allowed us to develop the system safely before showing it to business users. Data was collected using a survey, where SMEs evaluated a series of questions and answers without being told which answers were from the large language model (LLM) underpinning GOV.UK Chat and which were human. Accuracy ratings were collected via Likert scales supported by qualitative comments.&nbsp;</p>

<p>We needed to determine if the SMEs agreed about their assigned ratings, as this would tell us about the reliability of the data. We calculated the level of agreement and found it to be high, indicating that SME ratings were consistent. We were then able to compare the ratings for the LLM with the human model answers to see if they were significantly different. We found no significant differences – the accuracy of the LLM was rated on par with the human model answers.</p>

<p>A subsequent round of online testing provided further insights on accuracy from live queries, and helped us examine the tool’s conversational memory. During remote testing, internal users from our content design community interacted with GOV.UK Chat to find information to support a fictitious user who was planning to start a business. After the interaction, participants completed a short survey about their experience. We analysed the test session videos and triangulated this data with their survey responses.</p>

<h3 id="project-development-1">Project development</h3>

<p>We worked with users already in a business, and those about to start one. Our testing was conducted in 2 phases. First, we conducted a qualitative study that combined an interview with a usability test of the GOV.UK Chat prototype. This allowed us to explore users’ attitudes and experiences with AI tools, gain feedback on our tool and further assess accuracy.</p>

<p>In the second phase of testing, we invited 1,000 users to test GOV.UK Chat via a research banner on GOV.UK. Users interacted with the tool, providing message-level feedback on answer usefulness, and completed a short survey about their experience. During both rounds of testing with business users, we repeatedly assessed accuracy by gathering SME ratings of the LLM’s answers.</p>

<p>The scaled-up testing was an important step. Having a large data set for analysis added weight and confidence to how we were assessing accuracy. It also provided an opportunity to examine the types of questions that users might typically ask, and, importantly, how those questions were phrased. This would help us understand to what extent users required support in articulating their questions. Finally, scaled-up testing allowed us to combine our understanding of accuracy with users’ feedback on the experience of using the tool.</p>

<h3 id="value-delivered-1">Value delivered</h3>

<p>Overall, 69% of users thought the tool was useful or very useful. We also achieved an accuracy threshold of 80%.</p>

<h3 id="challenges-and-solutions-1">Challenges and solutions</h3>

<p>Our research approach was inspired by a desire to safely and responsibly test the LLM before showing it to users. However, this was a labour-intensive approach. Each investigation required us to quantify accuracy alongside exploring the user experience, sometimes uncovering broader insights about the propositional value of the tool.</p>

<p>The scaled testing was particularly useful in combining different data sources, such as survey ratings, question types, participant feedback about whether the generated answer was useful, accuracy of answers (evaluated by SMEs), and response times. To not dissuade users from completing the survey, we kept it short and asked a small number of questions.</p>

<p>In future live-scaled testing, we plan to bolster this data with live intercept interviews, speaking to business users who are using GOV.UK and GOV.UK Chat to solve their emerging information needs. This will help us to explore their experience and attitudes to the tool in more detail.&nbsp;</p>

<h2 id="ccs-commercial-agreement-recommendation-system">CCS commercial agreement recommendation system&nbsp;</h2>

<p>The Crown Commercial Service (CCS) built a commercial agreement recommendation system using spend and customer market segmentation data. Our goal was to generate relevant agreement recommendations for customers to help them discover new agreements by considering their past procurement activity and their market sector peers’ activity. The system was inspired by the technologies used by companies such as Netflix to deliver intelligent and personalised recommendations for users based on their historical data.</p>

<h3 id="our-tool-and-the-problem-it-solves-1">Our tool and the problem it solves</h3>

<p>When a customer uses one of our hundreds of agreement lots to procure products and services, the supplier provides data about that transaction. Over time, we collate these submissions into a historical customer-product interaction data set.</p>

<p>A machine learning (ML) recommendation system trained on data like this can provide relevant and detailed recommendations, operate without customer or staff input, keep pace with evolving behaviour, and take into account other useful information.</p>

<p>This recommendation system focuses on discovery, generating recommendations in the form of agreements that are new to each customer, which makes them aware of relevant agreements that they might not have known about or found otherwise.</p>

<h3 id="project-development-2">Project development</h3>

<p>In the absence of an online digital platform ready for recommendation serving, we used this configuration to enable a small team to rapidly develop a pilot solution using historical data that could be served to customers through email marketing or during conversations. Once this platform was ready, new challenges appeared, including how to ensure continuity of recommendation serving and allow for a richer feedback loop by measuring interactions with recommendations in the digital service.</p>

<h3 id="measuring-success-1">Measuring success</h3>

<p>A number of customers were given relevant recommendations and an equivalent number in a control group were not. Acting on these recommendations translated to an increase in the average number of unique agreements used by those customers compared to the control group. This is because we made customers aware of new agreements beyond those which they currently use.</p>

<h3 id="value-delivered-2">Value delivered</h3>

<p>We now have the potential to provide commercial agreement recommendations to many more customers. This was previously limited to customers with dedicated account managers who have specialist customer knowledge. The ultimate goal is to integrate this system into an online digital platform where customers can access their recommendations at their convenience.</p>

<h3 id="technologies-1">Technologies</h3>

<p>The algorithm we used to build our tool is based on <a rel="external" href="https://www.tensorflow.org/recommenders">TensorFlow Recommenders</a> and consists of a ‘two-tower’ neural network (NN). This is a popular architecture for modern recommendation systems. When training, each NN learns the embeddings for the transaction context and candidate products so that the combination of these embeddings correlates with the observed affinity of that context and candidate in the training data. This then allows the embedding of any number of potential candidates, which are ranked by mathematical distance from a given context to get recommendations.</p>

<h3 id="challenges-and-solutions-2">Challenges and solutions</h3>

<p>Offline historical data of customer-product interactions allows the reconstruction of the buying behaviour of a customer through time and the comparison of this to generated recommendations. These simple offline ‘hit’ metrics can be flawed in a discovery context.</p>

<p>In our interaction data set, customers generally use the same products with some variability. A lack of discovery ‘hits’ with what a customer interacted with next is therefore not necessarily a bad thing, as we are specifically trying to predict what a customer has not interacted with yet. This causes challenges when assessing the performance of a discovery-based recommendation system using offline data only.</p>

<h3 id="example-ethical-legal-and-security-considerations-1">Example ethical, legal and security considerations</h3>

<p>As our customer organisations are legal entities and not people, no personal data is used when making recommendations. Additionally, we use objective past behaviour as a guide to produce recommendations for consideration only by customer organisations.</p>

<h2 id="digital-sensitivity-review-at-fcdo-services">Digital Sensitivity Review at FCDO Services&nbsp;</h2>

<p>FCDO Services has developed an AI-powered Digital Sensitivity Review toolset. This enables departments to select, review and transfer digital records consistently and securely for permanent preservation at The National Archives (TNA), meeting the requirements of the <a rel="external" href="https://www.nationalarchives.gov.uk/information-management/legislation/public-records-act/">Public Records Act</a>.</p>

<h3 id="our-toolset-and-the-problem-it-solves">Our toolset and the problem it solves</h3>

<p>The Digital Sensitivity Review toolset uses AI to significantly increase the efficiency, effectiveness and risk reduction for the review and transfer of digital records. Digital records present several risks, as hidden data within records often contain sensitivities. For example, it’s common to insert cropped images into documents, but users may not realise that the original uncropped image is still present and easily recoverable.</p>

<p>As there’s a limited supply of trained sensitivity reviewers, our toolset helps departments to detect issues like this and better cope with the increasing volume and complexity of digital records. Techniques that reduce digital volume, such as ephemeral identification, duplicate identification and clustering, are essential to this process. So we designed specific solutions for each of the stages involved in the transfer of digital records.</p>

<h3 id="project-development-3">Project development</h3>

<p>To develop this project, we established Team Cicero as a collaborative working arrangement in accordance with <a rel="external" href="https://www.iso.org/standard/72798.html">ISO 44001</a>. This enabled working with specialist partners in industry and Sheffield, Glasgow and Loughborough Universities, who are leading cutting-edge research into topics such as lifelong machine learning (ML) techniques.</p>

<p>We defined the target outcome based on the volume and complexity of the unstructured data, commonly called the ‘digital heap’, and established a set of challenges. Integration of the technologies and the users into one system was a consideration from the outset, along with commercial licensing mechanisms. We decided to use off-the-shelf software wherever possible and ensure that the system was modular in design, making it easy to upgrade.&nbsp;</p>

<h3 id="measuring-success-2">Measuring success</h3>

<p>Our 2 key metrics were the number of sensitivity reviewers needed to achieve the target risk appetite for a given volume and complexity without changing records selection policy, and the risk of release of sensitive content.&nbsp;</p>

<p>Without this technology, we would require over 100 additional operations team members, including sensitivity reviewers, to meet the demands of digital review and transfer for one government department. Moreover, to meet the volume of digital records requiring review, we would require a system that worked at least one order of magnitude faster than the equivalent paper review system.</p>

<h3 id="value-delivered-3">Value delivered</h3>

<p>We have been able to process records with 10% of the sensitivity review effort and reduce the risk of release of sensitive content to well below what was achievable without the use of this technology.</p>

<h3 id="challenges-and-solutions-3">Challenges and solutions</h3>

<p>ML techniques require considerable volumes of examples to train the model, but the limited amount of material actually redacted so far is small. Of the records selected for review, typically less than 1% of the documents reviewed have any redaction applied. This becomes a statistical constraint on the use of ML, because sourcing the maximum possible volume of sensitive material is fundamental to maximising the potential of the system.&nbsp;</p>

<p>This problem is exacerbated by the fact that different technologies are more suited to different <a rel="external" href="https://www.legislation.gov.uk/ukpga/2000/36/contents">Freedom of Information Act</a> exemption categories. For example, international relations sensitivities are fundamentally different to those involving personal information. Different departments applying differing policies reduces the availability of statistically significant volumes. Policy examples include the definitions of duplicate, ephemeral and sensitive content.</p>

<h3 id="example-ethical-legal-and-security-considerations-2">Example ethical, legal and security considerations</h3>

<p>Our system is aligned with security considerations required by different departments and we considered several legal and ethical issues. For example, the technologies are currently used only to assist human decisions and our reviewers are fully trained to understand the ethical and legal aspects of their work. The technology augments the reviewer’s ability to process a greater volume of material than would otherwise be possible, but it does not remove or replace the responsibilities of the reviewer.&nbsp;</p>

<p>We have also identified a potential challenge around the ability of departments to keep sensitive content post-transfer, and to provide statistically meaningful volumes for ML, with differing opinions on the legality of doing so. We believe this needs to be discussed further if ML is to reach its full potential in sensitivity reviews.</p>

<h2 id="nhs-user-research-finder">NHS user research finder</h2>

<p>This project focuses on creating a tool to help colleagues find user research completed by other teams across the organisation.</p>

<h3 id="our-tool-and-the-problem-it-solves-2">Our tool and the problem it solves</h3>

<p>Our tool allows users to both upload and search for user research. Users can upload user research into our database, which is then summarised using a commercial large language model (LLM) through an application programming interface (API). The user can review and edit the summary generated by AI before confirming the submission. After a review from the user-centred design ops team, the report summary is then published on the tool.&nbsp;Users can query this database using natural language which is interpreted by the same API.</p>

<p>The tool allows users to enter text search queries such as:</p>

<ul>
  <li>“What do we know about 111 online?”</li>
  <li>“What do we know about records access?”</li>
  <li>“What do GP practices need from patient facing apps?”</li>
</ul>

<p>Users are then presented with research summaries and relevant contacts from the&nbsp;publishing team.&nbsp;</p>

<h3 id="project-development-4">Project development</h3>

<p>We developed this tool over a 13-week period with a commercial partner specialising in AI. It’s currently in private beta testing. We worked using typical agile methodologies and conducted user research and testing throughout.</p>

<h3 id="measuring-success-3">Measuring success</h3>

<p>We’re measuring the success of the tool through a range of key performance indicators (KPIs) including usage, uploads, searches, search queries, satisfaction and completion rate. We’re also conducting user research to measure ease of use and effectiveness at responding to the original user need.&nbsp;</p>

<h3 id="value-delivered-4">Value delivered</h3>

<p>Our tool makes user research more easily findable and visible. If we can refine the summarising capabilities of the tool within the parameters of our information governance, this will save considerable time for user researchers and minimise the risk that relevant user research is neglected or work is duplicated.</p>

<h3 id="challenges-and-solutions-4">Challenges and solutions</h3>

<p>The tool makes use of an API to summarise information using an LLM. Limitations and shortcomings include the fact that the AI currently does not always recognise all the sections within a given research output, which then requires the user to fill in the gaps.&nbsp;</p>

<p>Another unexpected challenge is the way the AI summarises reports. When asked to process the same report several times in succession, the tool will often return varied results. The same is true for the search. Furthermore, when asked for very long, specific or complex search queries, the LLM might return only vaguely related results (instead of nothing).</p>

<p>For example, the query: “Has anyone done any research on how young people (so around the ages of 11 to 16 or 18), feel about accessing healthcare, understand how it works (like how to see a GP or book an appointment, for example), their parents’ involvement in managing their healthcare tasks and when there might be a ‘handing over of the baton’ to the young person to managing it?” returns results about a 111 online discovery with parents of children under 5 years old, as well as general research around adult proxy patients’ user needs.</p>

<p>We are inherently reliant on a third-party API for the tool to function. In early prototyping we attempted to run an in-house LLM. However, the quality of the results was not high enough given the time we had available to develop the prototype. This could be explored further.</p>

<h3 id="example-ethical-legal-and-security-considerations-3">Example ethical, legal and security considerations</h3>

<p>The development of a tool to aggregate and query user research using a commercial LLM for summarisation and search functionalities requires careful consideration of ethics and security. From an ethical perspective, we considered potential accuracy and bias issues of AI-generated summaries. These can impact the integrity of user research.&nbsp;</p>

<p>Privacy and legal concerns are also significant because user research often contains sensitive information. Ensuring the LLM does not compromise participant confidentiality or contravene data protection laws is paramount. The use of a commercial LLM involves checking the security of the systems deployed as well as navigating copyright and intellectual property rights. In particular, the project’s reliance on a third-party API for critical functions introduces dependencies and risks related to data security, service continuity, and control over data processing. This necessitates rigorous vendor assessment and contract management to safeguard organisational and user interests.&nbsp;</p>

<p>Furthermore, the variation in AI-generated outputs and the challenge in handling complex queries underline the need for continuous monitoring, user feedback integration, and iterative improvement to effectively address ethical, privacy and legal concerns.</p>

<h2 id="nhsuk-reviews-an-automated-reviews-moderator">NHS.UK reviews: an automated reviews moderator</h2>

<p>As operators of the NHS.UK website, we need to moderate hundreds of thousands of reviews of NHS services to make sure they meet our guidelines about personal information, abuse, discrimination and other policies.&nbsp;Data scientists from NHS England have automated the review moderation process using natural language processing (NLP) techniques to build bespoke models to implement our policies.&nbsp;</p>

<h3 id="our-tool-and-the-problem-it-solves-3">Our tool and the problem it solves</h3>

<p>We developed an AI tool to automatically identify reviews that contain suicidal and self-harm ideation content; serious issues which should be formal complaints rather than a review; profanity; email addresses; descriptions of a person; names; fully capitalised text; URLs; and text that doesn’t describe an experience. Our AI solution is scalable, trackable and quick. It has a low running cost and improves user experience, as reviews are moderated more quickly and consistently.</p>

<h3 id="project-development-5">Project development</h3>

<p>To deliver this project we deployed a multidisciplinary team. Initially, we developed the tool on a flask app using named entity recognition, part of speech tagging, lookups and simple regex rules. This phase helped to establish a robust architecture and to integrate with the NHS.UK platform where the tool was to be used. Following this, we developed more complicated machine learning (ML) models that required good quality training data, and time to train, test and evaluate.&nbsp;</p>

<h3 id="measuring-success-4">Measuring success</h3>

<p>We measured the success of our AI solution through reductions in solution cost, moderation time, and a reduction in the proportion of unpublishable reviews. Our ML models were also tested through:</p>

<ul>
  <li>confusion matrices: to describe the outcomes of the models against the test data set. We used balanced test data sets and data sets which represented real-world data volumes to give an idea of real-world performance</li>
  <li>clerical review: false positives and false negatives were subjected to a more detailed review. This allowed us to understand edge cases where the models were underperforming, and identify opportunities to fine-tune</li>
  <li>non-functional testing: metrics included latency, throughput and spin-up time for compute. These tests allowed us to understand whether we would see degradation of the service over a range of likely and extreme scenarios, and allowed us to assure the product owner that the solution was fit for use</li>
</ul>

<h3 id="value-delivered-5">Value delivered</h3>

<p>The use of AI technologies removed the need for a third-party moderation service, reducing cost, increasing accuracy, and enabling scalability of moderation through automation. It also decreased the time from review submission to publication, improving patient feedback.&nbsp;The immediate feedback provided by the AI system has resulted in an increased percentage of published reviews, a decrease of rejected reviews, and the ability for the NHS team to easily add moderation rules in future based on requirements and reviewing all comments accordingly.&nbsp;</p>

<h3 id="technologies-2">Technologies</h3>

<p>The auto moderation software was written in Python and hosted on our virtual private cloud service. It consists of a flask app which handles API calls and payloads to hosted ML inference models. The API call passes the content to the model and the model returns a JSON payload which informs the NHS.UK system how to handle the content. The tool uses a combination of regex and NLP methods, and pretrained embedding models like BERT, bag of word and MNET.</p>

<h3 id="challenges-and-solutions-5">Challenges and solutions</h3>

<p>The use of these technologies in the NHS is not widespread and we had to discover how to sign off a solution which contained AI. The compartmentalised nature of the solution helped, as we were able to assign risk to components and get them signed off in isolation before signing off the solution as a whole.</p>

<p>The probabilistic nature of the AI models needed clear explanation and rigorous testing to ensure we were not introducing additional risks to the service.&nbsp;Additionally, one model faced the challenge of insufficient training data as it targeted a less common reason for review rejection – safeguarding concerns. We addressed this by augmenting our data set through the generation of synthetic data using NLP tools, which was then validated by expert moderators to ensure its quality and relevance.</p>

<h3 id="example-ethical-legal-and-security-considerations-4">Example ethical, legal and security considerations</h3>

<p>Our product involves dealing with user feedback that potentially contains personal or sensitive information. This required us to pay particular attention to ethical legal and security issues throughout the project life cycle. Our data processing complies with the Data Protection Act 2018.&nbsp;</p>

<p>We have made provisions for people to request a human review if they are not content with the automated response. This ensures that human operators are involved in the process and can monitor the outcomes of the automated review of comments.</p>

<p>We paid particular attention to ethical concerns. For example, we developed an effective suicidal and self-harm content detector. We undertook a clinical safety exercise to ensure clinical risks were elaborated and appropriately mitigated. In addition, a bias analysis was conducted on the 3 models trained by the team for the auto moderation tool: safeguarding, complaints and ‘not an experience’. The analysis aimed to identify any potential bias in these models by examining the sentiment of misclassified comments, the length of the texts, and the number of spelling mistakes standardised by text length.</p>

<p>Our solutions assurance team assessed the solution with a particular focus on training data coverage and model behaviour, examining the extent to which literacy levels in reviews affected the model outcomes.&nbsp;Our technical review group – a panel of experts with a wide range of expertise – reviewed the solution and made recommendations regarding improved security and solution architecture.</p>
</div>


</div>
</div>
    </div>
  </div>

  <div class="govuk-grid-row">
    
<a class="govuk-link gem-c-back-to-top-link govuk-!-display-none-print" href="#contents">
    <svg class="gem-c-back-to-top-link__icon" xmlns="http://www.w3.org/2000/svg" width="13" height="17" viewBox="0 0 13 17" aria-hidden="true" focusable="false">
      <path fill="currentColor" d="M6.5 0L0 6.5 1.4 8l4-4v12.7h2V4l4.3 4L13 6.4z"></path>
    </svg>
    Back to top
</a>
  </div>
</div>


    </main>
  </div>

      <div class="govuk-width-container">
        
<div data-module="feedback ga4-event-tracker" class="gem-c-feedback govuk-!-display-none-print" data-feedback-module-started="true" data-ga4-event-tracker-module-started="true">
  
<div class="gem-c-feedback__prompt gem-c-feedback__js-show js-prompt" tabindex="-1">
  <div class="gem-c-feedback__prompt-content">
    <div class="gem-c-feedback__prompt-questions js-prompt-questions">
      <div class="gem-c-feedback__prompt-question-answer">
        <h2 class="gem-c-feedback__prompt-question">Is this page useful?</h2>
        <ul class="gem-c-feedback__option-list">
          <li class="gem-c-feedback__option-list-item govuk-visually-hidden" hidden="">
            <a class="gem-c-feedback__prompt-link" role="button" hidden="hidden" aria-hidden="true" href="/contact/govuk">
              Maybe
</a>          </li>
          <li class="gem-c-feedback__option-list-item">
            <button class="govuk-button gem-c-feedback__prompt-link js-page-is-useful" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;Yes&quot;,&quot;section&quot;:&quot;Is this page useful?&quot;,&quot;tool_name&quot;:&quot;Is this page useful?&quot;}">
              Yes <span class="govuk-visually-hidden">this page is useful</span>
</button>          </li>
          <li class="gem-c-feedback__option-list-item">

            <button class="govuk-button gem-c-feedback__prompt-link js-toggle-form js-page-is-not-useful" aria-controls="page-is-not-useful" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;No&quot;,&quot;section&quot;:&quot;Is this page useful?&quot;,&quot;tool_name&quot;:&quot;Is this page useful?&quot;}">
              No <span class="govuk-visually-hidden">this page is not useful</span>
</button>          </li>
        </ul>
      </div>
    </div>

    <div class="gem-c-feedback__prompt-questions gem-c-feedback__prompt-success js-prompt-success" role="alert" hidden="">
      Thank you for your feedback
    </div>

    <div class="gem-c-feedback__prompt-questions gem-c-feedback__prompt-questions--something-is-wrong js-prompt-questions">
      <button class="govuk-button gem-c-feedback__prompt-link js-toggle-form js-something-is-wrong" aria-controls="something-is-wrong" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;Report a problem with this page&quot;,&quot;section&quot;:&quot;Is this page useful?&quot;,&quot;tool_name&quot;:&quot;Is this page useful?&quot;}">
        Report a problem with this page
</button>    </div>
  </div>
</div>

  <form action="/contact/govuk/problem_reports" id="something-is-wrong" class="gem-c-feedback__form js-feedback-form" method="post" hidden="">

  <div class="govuk-grid-row">
    <div class="govuk-grid-column-two-thirds">
      <div class="gem-c-feedback__error-summary gem-c-feedback__js-show js-errors" tabindex="-1" hidden=""></div>

      <input type="hidden" name="url" value="https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html">

      <h2 class="gem-c-feedback__form-heading">Help us improve GOV.UK</h2>
      <p id="feedback_explanation" class="gem-c-feedback__form-paragraph">Don’t include personal or financial information like your National Insurance number or credit card details.</p>

      <div class="govuk-visually-hidden" aria-hidden="true">
        <label for="giraffe">This field is for robots only. Please leave blank</label>
        <input id="giraffe" name="giraffe" type="text" pattern=".{0}" tabindex="-1" autocomplete="off">
      </div>

      <div class="gem-c-textarea govuk-form-group govuk-!-margin-bottom-6">
    
  <label for="textarea-5d1418e1" class="gem-c-label govuk-label">What were you doing?</label>





  <textarea name="what_doing" class="govuk-textarea" id="textarea-5d1418e1" rows="3" spellcheck="true" aria-describedby="feedback_explanation"></textarea>
     
</div>

      <div class="gem-c-textarea govuk-form-group govuk-!-margin-bottom-6">
    
  <label for="textarea-8c6e7e47" class="gem-c-label govuk-label">What went wrong?</label>





  <textarea name="what_wrong" class="govuk-textarea" id="textarea-8c6e7e47" rows="3" spellcheck="true"></textarea>
     
</div>


      

  <button class="gem-c-button govuk-button" type="submit" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;Send&quot;,&quot;section&quot;:&quot;Help us improve GOV.UK&quot;,&quot;tool_name&quot;:&quot;Help us improve GOV.UK&quot;}">Send</button>



      <button class="govuk-button govuk-button--secondary gem-c-feedback__close gem-c-feedback__js-show js-close-form" aria-controls="something-is-wrong">
        Cancel
      </button>
    </div>
  </div>
<input type="hidden" name="javascript_enabled" value="true"><input type="hidden" name="referrer" value="unknown"><input type="hidden" name="timer" value="0"></form>


 <script nonce="">
//<![CDATA[
  document.addEventListener("DOMContentLoaded", function () {
    var input = document.querySelector("#giraffe"),
      form = document.querySelector("#something-is-wrong")

    form.addEventListener("submit", spamCapture);

    function spamCapture(e) {
      if (input.value.length !== 0) return;
      e.preventDefault();
    }
  });

//]]>
</script>
  <div id="page-is-not-useful" class="gem-c-feedback__form gem-c-feedback__form--email gem-c-feedback__js-show js-feedback-form" hidden="">
  <div class="govuk-grid-row">
    <div class="govuk-grid-column-two-thirds" id="survey-wrapper">
      <div class="gem-c-feedback__error-summary js-errors" tabindex="-1" hidden=""></div>

      <h2 class="gem-c-feedback__form-heading">Help us improve GOV.UK</h2>
      <p id="survey_explanation" class="gem-c-feedback__form-paragraph">
        To help us improve GOV.UK, we’d like to know more about your visit today.
        <a href="https://www.smartsurvey.co.uk/s/gov-uk-banner/?c=/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html" class="govuk-link" target="_blank" rel="noopener noreferrer external">Please fill in this survey (opens in a new tab<noscript> and requires JavaScript</noscript>)</a>.
      </p>
      <button class="govuk-button govuk-button--secondary js-close-form" aria-controls="page-is-not-useful">
        Cancel
      </button>
    </div>
  </div>
</div>

</div>
      </div>

      <footer data-module="ga4-link-tracker" class="gem-c-layout-footer govuk-footer" data-ga4-link-tracker-module-started="true">
  <div class="govuk-width-container">
    <svg xmlns="http://www.w3.org/2000/svg" focusable="false" role="presentation" viewBox="0 0 64 60" height="30" width="32" fill="currentcolor" class="govuk-footer__crown">
      <g>
        <circle cx="20" cy="17.6" r="3.7"></circle>
        <circle cx="10.2" cy="23.5" r="3.7"></circle>
        <circle cx="3.7" cy="33.2" r="3.7"></circle>
        <circle cx="31.7" cy="30.6" r="3.7"></circle>
        <circle cx="43.3" cy="17.6" r="3.7"></circle>
        <circle cx="53.2" cy="23.5" r="3.7"></circle>
        <circle cx="59.7" cy="33.2" r="3.7"></circle>
        <circle cx="31.7" cy="30.6" r="3.7"></circle>
        <path d="M33.1,9.8c.2-.1.3-.3.5-.5l4.6,2.4v-6.8l-4.6,1.5c-.1-.2-.3-.3-.5-.5l1.9-5.9h-6.7l1.9,5.9c-.2.1-.3.3-.5.5l-4.6-1.5v6.8l4.6-2.4c.1.2.3.3.5.5l-2.6,8c-.9,2.8,1.2,5.7,4.1,5.7h0c3,0,5.1-2.9,4.1-5.7l-2.6-8ZM37,37.9s-3.4,3.8-4.1,6.1c2.2,0,4.2-.5,6.4-2.8l-.7,8.5c-2-2.8-4.4-4.1-5.7-3.8.1,3.1.5,6.7,5.8,7.2,3.7.3,6.7-1.5,7-3.8.4-2.6-2-4.3-3.7-1.6-1.4-4.5,2.4-6.1,4.9-3.2-1.9-4.5-1.8-7.7,2.4-10.9,3,4,2.6,7.3-1.2,11.1,2.4-1.3,6.2,0,4,4.6-1.2-2.8-3.7-2.2-4.2.2-.3,1.7.7,3.7,3,4.2,1.9.3,4.7-.9,7-5.9-1.3,0-2.4.7-3.9,1.7l2.4-8c.6,2.3,1.4,3.7,2.2,4.5.6-1.6.5-2.8,0-5.3l5,1.8c-2.6,3.6-5.2,8.7-7.3,17.5-7.4-1.1-15.7-1.7-24.5-1.7h0c-8.8,0-17.1.6-24.5,1.7-2.1-8.9-4.7-13.9-7.3-17.5l5-1.8c-.5,2.5-.6,3.7,0,5.3.8-.8,1.6-2.3,2.2-4.5l2.4,8c-1.5-1-2.6-1.7-3.9-1.7,2.3,5,5.2,6.2,7,5.9,2.3-.4,3.3-2.4,3-4.2-.5-2.4-3-3.1-4.2-.2-2.2-4.6,1.6-6,4-4.6-3.7-3.7-4.2-7.1-1.2-11.1,4.2,3.2,4.3,6.4,2.4,10.9,2.5-2.8,6.3-1.3,4.9,3.2-1.8-2.7-4.1-1-3.7,1.6.3,2.3,3.3,4.1,7,3.8,5.4-.5,5.7-4.2,5.8-7.2-1.3-.2-3.7,1-5.7,3.8l-.7-8.5c2.2,2.3,4.2,2.7,6.4,2.8-.7-2.3-4.1-6.1-4.1-6.1h10.6,0Z"></path>
      </g>
    </svg>
      <div class="govuk-footer__navigation">
            <div class="govuk-grid-column-two-thirds govuk-!-display-none-print">
              <h2 class="govuk-footer__heading govuk-heading-m">Services and information</h2>
                <ul class="govuk-footer__list govuk-footer__list--columns-2">
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/benefits">Benefits</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/births-deaths-marriages">Births, death, marriages and care</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/business">Business and self-employed</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/childcare-parenting">Childcare and parenting</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/citizenship">Citizenship and living in the UK</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;6&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/justice">Crime, justice and the law</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;7&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/disabilities">Disabled people</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;8&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/driving">Driving and transport</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;9&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/education">Education and learning</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;10&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/employing-people">Employing people</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;11&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/environment-countryside">Environment and countryside</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;12&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/housing-local-services">Housing and local services</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;13&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/tax">Money and tax</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;14&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/abroad">Passports, travel and living abroad</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;15&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/visas-immigration">Visas and immigration</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;16&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/working">Working, jobs and pensions</a>
                      </li>
                </ul>
            </div>
            <div class="govuk-grid-column-one-third govuk-!-display-none-print">
              <h2 class="govuk-footer__heading govuk-heading-m">Government activity</h2>
                <ul class="govuk-footer__list govuk-footer__list--columns-1">
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/organisations">Departments</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/news-and-communications">News</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/guidance-and-regulation">Guidance and regulation</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/research-and-statistics">Research and statistics</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/policy-papers-and-consultations">Policy papers and consultations</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;6&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/transparency-and-freedom-of-information-releases">Transparency</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;7&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/how-government-works">How government works</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;8&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/get-involved">Get involved</a>
                      </li>
                </ul>
            </div>
      </div>

      <hr class="govuk-footer__section-break govuk-!-display-none-print">
    <div class="govuk-footer__meta">
      <div class="govuk-footer__meta-item govuk-footer__meta-item--grow">
          <h2 class="govuk-visually-hidden">Support links</h2>
          <ul class="govuk-footer__inline-list govuk-!-display-none-print">
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help">Help</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/privacy-notice">Privacy</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/cookies">Cookies</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/accessibility-statement">Accessibility statement</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/contact">Contact</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;6&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/terms-conditions">Terms and conditions</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" lang="cy" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;7&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/cymraeg">Rhestr o Wasanaethau Cymraeg</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;8&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/government/organisations/government-digital-service">Government Digital Service</a>
              </li>
          </ul>
          <svg aria-hidden="true" focusable="false" class="govuk-footer__licence-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 483.2 195.7" height="17" width="41">
            <path fill="currentColor" d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145"></path>
          </svg>
          <span class="govuk-footer__licence-description" data-ga4-track-links-only="" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;section&quot;:&quot;Licence&quot;,&quot;index_section&quot;:&quot;4&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;text&quot;:&quot;Open Government Licence v3.0&quot;,&quot;index_total&quot;:&quot;1&quot;,&quot;type&quot;:&quot;footer&quot;}">
            All content is available under the <a class="govuk-footer__link" href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license">Open Government Licence v3.0</a>, except where otherwise stated
          </span>
      </div>
      <div class="govuk-footer__meta-item" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;section&quot;:&quot;Copyright&quot;,&quot;index_section&quot;:&quot;5&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;text&quot;:&quot;© Crown copyright&quot;,&quot;index_total&quot;:&quot;1&quot;,&quot;type&quot;:&quot;footer&quot;}">
        <a class="govuk-footer__link govuk-footer__copyright-logo" href="https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/crown-copyright/">© Crown copyright</a>
      </div>
    </div>
  </div>
</footer>
    <script src="/assets/static/application-797926e5a7d2db83c4d02a72bd4d31233750842fb58a04eeb1fdd2aab63d21b1.js" type="module"></script>
<script src="/assets/government-frontend/application-7c9db67f200545f0155ed150d5333b474479b0b3eb482c430af7f62aa560e563.js" type="module"></script><script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "FAQPage",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html"
  },
  "name": "Artificial Intelligence Playbook for the UK Government (HTML)",
  "datePublished": "2025-02-10T00:01:07+00:00",
  "dateModified": "2025-02-10T00:00:00+00:00",
  "text": null,
  "publisher": {
    "@type": "Organization",
    "name": "GOV.UK",
    "url": "https://www.gov.uk",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-logo-b8553f688131fad665e52a8c2df7633f9cd1c0fffb9f69703cc68c728e7b3b74.png"
    }
  },
  "image": [
    "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-schema-placeholder-1x1-2672c0fb7a5d5f947d880522c509ebe7f2be090885883cc94418f6860e812e15.png",
    "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-schema-placeholder-4x3-194fde4197f00e669f6f52c182df2ed707bfb2024c9ef39f7a2ed20da62b90eb.png",
    "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-schema-placeholder-16x9-30e6c0e035636ee6b9dc72ae254bcd4a925182805afe7c5b7170cf2394894b28.png"
  ],
  "author": {
    "@type": "Organization",
    "name": "Government Digital Service",
    "url": "https://www.gov.uk/government/organisations/government-digital-service"
  },
  "mainEntity": [
    {
      "@type": "Question",
      "name": "Acknowledgements",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#acknowledgements",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#acknowledgements",
        "text": "\u003cp\u003eThe publication of this playbook has been made possible by the support from a large number of stakeholders.\u003c/p\u003e\u003ch3 id=\"central-government-departments\"\u003eCentral government departments\u003c/h3\u003e\u003cp\u003eCrown Commercial Service (CCS); Cabinet Office (CO), including the Number 10 Data Science (No.10 DS) and i.AI teams; Department for Business and Trade (DBT); Department for Education (DfE); Department for Environment, Food and Rural Affairs (DEFRA); Department for Energy Security and Net Zero (DESNZ); Department of Health and Social Care (DHSC); Department for Levelling Up, Housing and Communities (DLUHC); Department for Science, Innovation and Technology (DSIT), including the Government Office for Science (GO-Science) and the Responsible Technology Adoption Unit (RTA); Department for Transport (DfT); Department for Work and Pensions (DWP); Foreign, Commonwealth and Development Office (FCDO), including FCDO Services; Government Legal Department (GLD); HM Land Registry (HMLR); HM Revenue and Customs (HMRC); HM Treasury (HMT); Home Office (HO); Ministry of Defence (MoD); and Ministry of Justice (MoJ).\u003c/p\u003e\u003ch3 id=\"arms-length-bodies-devolved-administrations-and-public-sector-bodies\"\u003eArm’s length bodies, devolved administrations and public sector bodies\u003c/h3\u003e\u003cp\u003eDriver and Vehicle Licensing Agency (DVLA); Government Communications Headquarters (GCHQ); Government Internal Audit Agency (GIAA); HM Courts and Tribunals Service (HMCTS); Information Commissioner’s Office (ICO); Met Office; National Health Service (NHS); Office for National Statistics (ONS); and the Scottish Government.\u003c/p\u003e\u003ch3 id=\"industry\"\u003eIndustry\u003c/h3\u003e\u003cp\u003eAmazon (AWS), Google, IBM and Microsoft.\u003c/p\u003e\u003ch3 id=\"academic-institutes\"\u003eAcademic institutes\u003c/h3\u003e\u003cp\u003eAlan Turing Institute; BCS, The Chartered Institute for IT; Oxford Internet Institute; Manchester Metropolitan University; and the University of Surrey.\u003c/p\u003e\u003ch3 id=\"user-research\"\u003eUser research\u003c/h3\u003e\u003cp\u003eUser research participants have come from a wide range of departments and have been very generous with their time.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Update log ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#update-log",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#update-log",
        "text": "\u003cp\u003eWe have made the following core updates since the previous edition:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ewe have reviewed and updated the 10 principles and the guidance within the \u003ca href=\"https://www.gov.uk/government/publications/generative-ai-framework-for-hmg\"\u003eGenerative AI Framework\u003c/a\u003e to cater for a wider range of AI technologies beyond generative AI\u003c/li\u003e\n  \u003cli\u003ewe have added new sections on topics such as What is AI?, Fields of AI (including a new diagram), User research for AI, AI business cases, Societal wellbeing and public good, Security opportunities, Governance structures for teams, AI quality assurance and Managing risk. We have also included an Appendix with case studies on recent AI projects developed by public sector organisations\u003c/li\u003e\n  \u003cli\u003ewe have reorganised and expanded the Working collaboratively, Using AI safely and responsibly and Ethics sections, to better articulate the importance of engaging with academia, industry and the broader civil society\u003c/li\u003e\n  \u003cli\u003ewe have moved guidance and training on building AI solutions into separate \u003ca href=\"https://www.gov.uk/government/publications/ai-insights\"\u003eAI insights articles\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg\"\u003ee-learning courses\u003c/a\u003e to allow for a more in-depth discussion\u003c/li\u003e\n\u003c/ul\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Foreword",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#foreword",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#foreword",
        "text": "\u003cp\u003eI am pleased to introduce the AI Playbook for the UK Government, which updates and expands on the \u003ca href=\"https://www.gov.uk/government/publications/generative-ai-framework-for-hmg\"\u003eGenerative AI Framework for HMG\u003c/a\u003e. This updated guidance will help government departments and public sector organisations harness the power of a wider range of AI technologies safely, effectively, and responsibly.\u003c/p\u003e\u003cp\u003eIn January 2025 the government published the \u003ca href=\"https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan#lay-the-foundations\"\u003eAI Opportunities Action Plan\u003c/a\u003e laying out a bold roadmap for maximising AI’s potential to drive growth and deliver real benefits to people across the UK.\u003c/p\u003e\u003cp\u003eThe publication of the AI Playbook highlights the competence and extraordinary work already being done in the AI space across the public sector. Developed collaboratively, with input from many government departments, public sector institutions, academia, and industry, this guidance reflects our commitment to continuously engaging with and learning from wider civil society. \u003c/p\u003e\u003cp\u003eThe AI Playbook will support the public sector in better understanding what AI can and cannot do, and how to mitigate the risks it brings. It will help ensure that AI technologies are deployed in responsible and beneficial ways, safeguarding the security, wellbeing, and trust of the public we serve.\u003c/p\u003e\u003cp\u003eThe potential of AI to transform public services is enormous, giving us an unparalleled opportunity to do things differently and deliver more with less. AI is already helping civil servants spend less time on repetitive tasks, enabling teachers to personalise lessons, and can allow doctors to access life-saving insights faster, through AI-assisted diagnostics. \u003c/p\u003e\u003cp\u003eHowever, our journey with AI is just beginning. The AI Playbook is a launchpad that we will continuously revise and improve to help the UK public sector become a leading responsible user of AI technologies. As technology evolves, so too will our approach, ensuring we remain at the forefront of responsible innovation - always guided by the principle that technology must serve people.\u003c/p\u003e\u003cp\u003eI want to extend my sincere thanks to everyone who contributed their expertise to this AI Playbook, both within and beyond government. I look forward to ongoing collaboration as we continue to learn how to use AI safely, responsibly, and effectively to deliver solutions that are smarter, faster, and more responsive to the collective needs of our society.\u003c/p\u003e\u003cp\u003eFeryal Clark MP\u003c/p\u003e\u003cp\u003eParliamentary Under-Secretary of State for AI and Digital Government\u003c/p\u003e\u003cp\u003eDepartment for Science, Innovation and Technology (DSIT)\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Preface",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#preface",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#preface",
        "text": "\u003cp\u003eWhen we published the \u003ca href=\"https://www.gov.uk/government/publications/generative-ai-framework-for-hmg\"\u003eGenerative AI Framework for HMG\u003c/a\u003e in January 2024, we described it as ‘incomplete’ and ‘dynamic’. We didn’t pretend to have all of the answers in such a fast-moving field, but did aim to provide helpful, practical guidance to public servants on how to put generative AI to work confidently, responsibly and where it matters most.\u003c/p\u003e\u003cp\u003eThe pace of change since we published that first version has not slowed, and interest in generative AI and other forms of AI has grown. The UK government continues to believe that AI has the power to drive productivity, innovation and economic growth. In 2021, the \u003ca href=\"https://www.gov.uk/government/publications/national-ai-strategy\"\u003eNational AI Strategy\u003c/a\u003e set out a 10-year vision for AI, while the 2023 white paper \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper\"\u003eA pro-innovation approach to AI regulation\u003c/a\u003e set out the government’s proposals for implementing a proportionate, future-proof and pro-innovation framework for regulating AI. In 2025, the \u003ca href=\"https://www.gov.uk/government/publications/ai-opportunities-action-plan/ai-opportunities-action-plan#lay-the-foundations\"\u003eAI Opportunities Action Plan\u003c/a\u003e highlighted how the government can leverage AI to boost productivity and improve services.\u003c/p\u003e\u003cp\u003eThis updated version of the framework has been expanded to cover new developments, and we’ve retitled it to encompass all current forms of AI. As well as a general overview, it includes primers on various AI fields for the curious, and links to learning resources for those who want to dive deeper. The AI Playbook now covers the important and emerging discipline of conducting research with the users of AI systems. It addresses emerging cyber threats to those systems, and the ways that attackers are using AI to create new threats.\u003c/p\u003e\u003cp\u003eI would like to echo Minister Clark’s thanks to everyone who has contributed to this playbook. It has been a collective effort of experts from government departments, arm’s length bodies, other public sector organisations, academic institutions and industry partners. As we continue to advance together in the safe, responsible, and effective use of AI, I look forward to even broader collaboration and further contributions from an expanding community.\u003c/p\u003e\u003cp\u003eDavid Knott\u003c/p\u003e\u003cp\u003eGovernment Chief Technology Officer\u003c/p\u003e\u003cp\u003eDepartment for Science, Innovation and Technology (DSIT)\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principles ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principles",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principles",
        "text": "\u003cp\u003eWe have defined 10 common principles to guide the safe, responsible and effective use of artificial intelligence (AI) in government organisations. The white paper \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper\"\u003eA pro-innovation approach to AI regulation\u003c/a\u003e sets out \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a-implementation-of-the-principles-by-regulators\"\u003e5 principles\u003c/a\u003e for regulators to inform AI development in all sectors. This playbook builds on those principles and defines 10 core principles for AI use in government and public sector organisations.\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ePrinciple 1: You know what AI is and what its limitations are\u003c/li\u003e\n  \u003cli\u003ePrinciple 2: You use AI lawfully, ethically and responsibly\u003c/li\u003e\n  \u003cli\u003ePrinciple 3: You know how to use AI securely\u003c/li\u003e\n  \u003cli\u003ePrinciple 4: You have meaningful human control at the right stage\u003c/li\u003e\n  \u003cli\u003ePrinciple 5: You understand how to manage the AI life cycle\u003c/li\u003e\n  \u003cli\u003ePrinciple 6: You use the right tool for the job\u003c/li\u003e\n  \u003cli\u003ePrinciple 7: You are open and collaborative\u003c/li\u003e\n  \u003cli\u003ePrinciple 8: You work with commercial colleagues from the start\u003c/li\u003e\n  \u003cli\u003ePrinciple 9: You have the skills and expertise needed to implement and use AI\u003c/li\u003e\n  \u003cli\u003ePrinciple 10: You use these principles alongside your organisation’s policies and have the right assurance in place\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou can find \u003ca href=\"https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government\"\u003eposters on each of the 10 principles\u003c/a\u003e for you to display in your government organisation.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 1: You know what AI is and what its limitations are",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-1-you-know-what-ai-is-and-what-its-limitations-are",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-1-you-know-what-ai-is-and-what-its-limitations-are",
        "text": "\u003cp\u003eAI is a broad field subject to rapid research and innovation, and many claims have been made about both its promise and risks. You should learn about AI technology to understand what it can and cannot do, and the potential risks it poses.\u003c/p\u003e\u003cp\u003eAI systems currently lack reasoning and contextual awareness and their limitations vary depending on the tools you use and the context in which they operate. AI systems are also not guaranteed to be accurate. You should understand how to use AI tools safely and responsibly, employ techniques to increase the accuracy and correctness of their outputs, and have a process in place to test them. \u003c/p\u003e\u003cp\u003eYou can find more about what AI is and its capabilities in the Understanding AI section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 2: You use AI lawfully, ethically and responsibly",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-2-you-use-ai-lawfully-ethically-and-responsibly",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-2-you-use-ai-lawfully-ethically-and-responsibly",
        "text": "\u003cp\u003eAI solutions bring specific legal and ethical considerations. Your use of AI tools must be lawful and responsible. You should seek legal advice on the development and use of AI and engage with compliance, legal and data protection experts in your organisation early in your journey, including during product development. You may, for example, seek advice on equalities implications, fairness, intellectual property and other legal issues. \u003c/p\u003e\u003cp\u003eYou should seek data protection advice on your use of AI. This may be from your lawyers or your data protection officer. AI systems can process personal data, so you need to consider how you protect this personal data, be compliant with data protection legislation, and minimise the risk of privacy intrusion from the outset. \u003c/p\u003e\u003cp\u003eYou should establish and communicate how you will address ethical concerns throughout your project, from design to deployment, so that diverse and inclusive participation is built into the project life cycle. You should also consider the ethical principles presented in this playbook, and establish robust measures to suit your technological and deployment context. \u003c/p\u003e\u003cp\u003eAI models are trained on data which may include biased or harmful materials. As a result, AI systems may display biases and produce harmful outputs, such as unfair, prejudicial or derogatory representations of groups or individuals. You should consider all potential sources of bias throughout the development life cycle, including unrepresentative data sets and deployment scenarios that have unfair or undesirable impacts.\u003c/p\u003e\u003cp\u003eYou must ensure that AI systems generate a positive impact on stakeholders and civil society at large while minimising potential harms as much as possible. When defining and deploying AI systems, you must understand people’s needs and priorities by conducting user research and engaging with the public as appropriate, including civil society groups, underrepresented individuals, those most likely to experience harm, NGOs, academia and industry. \u003c/p\u003e\u003cp\u003eYou should understand and manage the environmental impact of the AI systems you are planning to use. You should also use AI technologies only when relevant, appropriate and proportionate. Choose the most suitable and sustainable option for your organisation’s needs. \u003c/p\u003e\u003cp\u003eYou can find out more in the Using AI safely and responsibly section. \u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 3: You know how to use AI securely ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-3-you-know-how-to-use-ai-securely",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-3-you-know-how-to-use-ai-securely",
        "text": "\u003cp\u003eWhen building and deploying AI services, you must make sure that they are secure to use and resilient to cyber attacks, as laid out in the \u003ca href=\"https://www.gov.uk/government/publications/government-cyber-security-strategy-2022-to-2030\"\u003eGovernment Cyber Security Strategy\u003c/a\u003e. Your service must comply with the \u003ca rel=\"external\" href=\"https://www.security.gov.uk/guidance/secure-by-design/\"\u003eSecure by Design\u003c/a\u003e principles, which were developed by the Central Digital and Data Office (CDDO), and the government’s \u003ca rel=\"external\" href=\"https://www.security.gov.uk/standards/cyber_standard\"\u003eCyber Security Standard\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eDifferent types of AI are susceptible to different security risks. Some threats – such as data poisoning, perturbation attacks, prompt injections and hallucinations – are specific to AI. However, AI systems can also amplify generic risks such as phishing and cyber attacks. You must understand the risks associated with your use of AI and of adversaries potentially using AI against you.\u003c/p\u003e\u003cp\u003eTo minimise these risks you should build in safeguards and put technical controls in place. These include security testing and, in the case of generative AI, content filtering to detect malicious activity, as well as validation checks to ensure responses are accurate and do not leak data. \u003c/p\u003e\u003cp\u003eYou can find out more in the Security and Data protection and privacy sections.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 4: You have meaningful human control at the right stages",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-4-you-have-meaningful-human-control-at-the-right-stages",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-4-you-have-meaningful-human-control-at-the-right-stages",
        "text": "\u003cp\u003eYou need to monitor the AI’s behaviour and have plans in place to prevent any harmful effects on users. This includes ensuring that humans validate any high-risk decisions influenced by AI and that you have strategies for meaningful intervention.\u003c/p\u003e\u003cp\u003eFor applications where instant responses are required and human review is not possible in real time, such as chatbots, it’s important that you ensure human control at other stages of the AI’s development and deployment.\u003c/p\u003e\u003cp\u003eYou should fully test the product before deployment, and have robust assurance and regular checks of the live tool in place. Since AI models can sometimes produce unwanted or inaccurate results, incorporating feedback from users is crucial. You should have systems in place that allow users to report issues and prompt a human review. \u003c/p\u003e\u003cp\u003eYou can find out more in the Human oversight section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 5: You understand how to manage the full AI life cycle ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-5-you-understand-how-to-manage-the-full-ai-life-cycle",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-5-you-understand-how-to-manage-the-full-ai-life-cycle",
        "text": "\u003cp\u003eAI solutions, like other technology deployments, have a full product life cycle that you need to understand. You should know how to choose the right tool for the job, be able to set it up and have the right resource in place to support day-to-day maintenance of it. You should also know how to update the system and how to securely close it down at the end of its useful life.\u003c/p\u003e\u003cp\u003eYou should understand how to monitor and mitigate for potential drift, bias, and, in the case of generative AI, hallucinations. You should also have a robust testing and monitoring process in place to catch these problems. You should use the \u003ca href=\"https://www.gov.uk/guidance/the-technology-code-of-practice\"\u003eTechnology code of practice\u003c/a\u003e to build a clear understanding of technology deployment life cycles, and understand and use the \u003ca rel=\"external\" href=\"https://www.ncsc.gov.uk/collection/cloud/the-cloud-security-principles\"\u003eNCSC cloud security principles\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eYou should understand the benefits, use cases and other applications that your solution could support across government and the wider public sector. The \u003ca href=\"https://www.gov.uk/government/publications/knowledge-asset-management-in-government\"\u003eRose Book\u003c/a\u003e provides guidance on government-wide knowledge assets and \u003ca href=\"https://www.gov.uk/government/organisations/government-office-for-technology-transfer\"\u003eThe Government Office for Technology Transfer\u003c/a\u003e can provide support and funding to help develop government-wide solutions. If you develop a service, you must use the government \u003ca href=\"https://www.gov.uk/service-manual/service-standard\"\u003eService Standard\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eYou can find out more about development best practices for AI in the Building AI solutions section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 6: You use the right tool for the job",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-6-you-use-the-right-tool-for-the-job",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-6-you-use-the-right-tool-for-the-job",
        "text": "\u003cp\u003eYou should select the most appropriate technology to meet your needs. AI is good at many tasks, but there are a wide range of models and products. You should be open to solutions involving AI because they can allow organisations to develop new or faster approaches to the delivery of public services, can provide a springboard for more creative and innovative thinking about policy and public sector problems, and help your team with time-consuming tasks. However, you should also be open to the conclusion that, sometimes, AI is not the best solution for your problem: it may be more easily solved with more established technologies.\u003c/p\u003e\u003cp\u003eWhen implementing AI solutions, you should select the most appropriate deployment patterns and choose the most suitable model for your use case. You can learn about how to choose the right technologies for your task or project in the Identifying use cases for AI and Use cases to avoid sections.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 7: You are open and collaborative",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-7-you-are-open-and-collaborative",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-7-you-are-open-and-collaborative",
        "text": "\u003cp\u003eThere are many teams across government and the wider public sector using or exploring AI tools in their work. You should make use of existing cross-government communities where there is a space to solve problems collaboratively, such as the \u003ca href=\"https://www.gov.uk/service-manual/communities/artificial-intelligence-community\"\u003eAI community of practice\u003c/a\u003e. You should also engage with other government departments that are trying to address similar issues and reuse ideas, code and infrastructure. \u003c/p\u003e\u003cp\u003eWhere possible, you should engage with the wider civil society including groups, communities, and non-governmental, academic and public representative organisations that have an interest in your project. Collaborating with people both inside and outside government will help you ensure we use AI to deliver tangible benefits to individuals and society as a whole. Make sure you have a clear plan for engaging and communicating with these stakeholders at the start of your work.\u003c/p\u003e\u003cp\u003eYou should be open with the public about where and how algorithms and AI systems are being used in official duties. If you’re a central government department or an arm’s length body within scope, you’re required to use the \u003ca href=\"https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub\"\u003eAlgorithmic Transparency Recording Standard (ATRS)\u003c/a\u003e. This means you must document information about any algorithmic tools you use in decision-making processes and make it clearly accessible to the public. The ATRS is not a requirement for all arm’s length bodies and other public sector institutions yet, but we still encourage you to use it. You should also clearly identify any automated response to the public. For example, a response generated via a chatbot interface should include something like ‘this response has been written by an automated AI chatbot’.\u003c/p\u003e\u003cp\u003eYou can find out more in the Ethics section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 8: You work with commercial colleagues from the start",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-8-you-work-with-commercial-colleagues-from-the-start",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-8-you-work-with-commercial-colleagues-from-the-start",
        "text": "\u003cp\u003eAI is a rapidly developing market, and you should get specific advice from commercial colleagues on the implications for your project. Reach out to them early in your journey to understand how to use AI in line with commercial requirements.\u003c/p\u003e\u003cp\u003eYou should also work with commercial colleagues to ensure that the expectations around the responsible and ethical use of AI are the same between AI systems developed in-house and those procured from a third party. For example, contracts between the public sector and third parties can be drafted to require transparency from the supplier on the different information categories, as set out in the \u003ca href=\"https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub\"\u003eATRS\u003c/a\u003e. You can find out more in the Buying AI section. \u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 9: You have the skills and expertise needed to implement and use AI solutions",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-9-you-have-the-skills-and-expertise-needed-to-implement-and-use-ai-solutions",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-9-you-have-the-skills-and-expertise-needed-to-implement-and-use-ai-solutions",
        "text": "\u003cp\u003eYou should understand the technical and ethical requirements for using AI tools and have them in place within your team. \u003c/p\u003e\u003cp\u003eYou and your team should gain the skills needed to use, design, build and maintain AI solutions, keeping in mind that developing bespoke AI solutions and training your own models require different specialist skills to using pre-trained models accessible through application programming interfaces (APIs). \u003c/p\u003e\u003cp\u003eDecision makers, policy professionals and senior responsible owners (SROs) should gain the skills they need to understand the risks and opportunities of AI, including its potential impact on organisational culture, governance, ethics and strategy.\u003c/p\u003e\u003cp\u003eYou should take the free \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/search?q=Generative+AI\"\u003eAI courses on Civil Service Learning\u003c/a\u003e and proactively keep track of developments in the field. You can find out more in our Acquiring skills and talent section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Principle 10: You use these principles alongside your organisation’s policies and have the right assurance in place",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-10-you-use-these-principles-alongside-your-organisations-policies-and-have-the-right-assurance-in-place",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#principle-10-you-use-these-principles-alongside-your-organisations-policies-and-have-the-right-assurance-in-place",
        "text": "\u003cp\u003eThese principles and this playbook set out a consistent approach for the UK government to use AI tools. While you should use these principles when working with AI, many government organisations have their own governance structures and policies in place. You should follow any organisation-specific policies, especially ones about security and data handling. \u003c/p\u003e\u003cp\u003eYou should understand, monitor and mitigate the risks that using AI tools can bring. Connect with the right assurance teams in your organisation early in the project life cycle for your AI solutions. You should have clearly documented review and escalation processes in place, and have an AI review board or programme-level board. \u003c/p\u003e\u003cp\u003eYou can find out more in the Governance section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Understanding AI ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#understanding-ai",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#understanding-ai",
        "text": "\u003cp\u003eThis section explains what AI is, what its main fields are, the applications of AI and generative AI in government and their limitations. It supports Principle 1: You know what AI is and what its limitations are.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "What is AI?",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#what-is-ai",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#what-is-ai",
        "text": "\u003cp\u003eAI is not new. The term ‘artificial intelligence’ was coined in 1956 during the \u003ca rel=\"external\" href=\"https://home.dartmouth.edu/about/artificial-intelligence-ai-coined-dartmouth\"\u003eDartmouth workshop\u003c/a\u003e, a gathering of scientists intent on exploring the potential of computing to emulate human reasoning. Since then, there have been recurring waves of progress and excitement, followed by periods of waning interest and investment referred to as ‘AI winters’. We use the \u003ca rel=\"external\" href=\"https://oecd.ai/en/wonk/ai-system-definition-update\"\u003edefinition of AI adopted by OECD countries\u003c/a\u003e:\u003c/p\u003e\u003cblockquote\u003e\n  \u003cp class=\"last-child\"\u003eAn AI system is a machine-based system that, for explicit or implicit objectives, infers, from the input it receives, how to generate outputs such as predictions, content, recommendations, or decisions that can influence physical or virtual environments. Different AI systems vary in their levels of autonomy and adaptiveness after deployment.\u003c/p\u003e\n\u003c/blockquote\u003e\u003cp\u003eThe UK government’s paper \u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003eEstablishing a pro-innovation approach to regulating AI\u003c/a\u003e suggests that these systems are ‘adaptable’ because they can find new ways to meet the objectives set by humans, and ‘autonomous’ because, once programmed, they can operate with varying levels of autonomy, including without human control.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Fields of AI",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#fields-of-ai",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#fields-of-ai",
        "text": "\u003cp\u003eAI comprises a complex and evolving set of fields. These include a broad, interconnected range of algorithms, models and processes. Advances in one area typically propagate throughout these networks of technologies, conferring novel behaviours and increases in accuracy and capability to ancestor models.\u003c/p\u003e\u003cp\u003eThe following diagram illustrates the interdependencies between some existing and emerging fields of AI. As you can see, capturing the evolving complexity of AI is a challenging task.\u003c/p\u003e\u003cfigure class=\"image embedded\"\u003e\u003cdiv class=\"img\"\u003e\u003cimg src=\"https://assets.publishing.service.gov.uk/media/67a376fab74b3d9dfe36ca9d/Image_1.jpg\" alt=\"\"\u003e\u003c/div\u003e\u003c/figure\u003e\u003cp\u003eThe complexity of the AI space continues to increase: as AI technologies evolve they often branch out, advancing some research areas while leaving others unchanged. For example, in the field of computer vision (CV) some legacy systems still thrive today due to the simplicity of their behaviours, their low computational and memory requirements, and their stability and well-proven performance. These legacy CV systems fall within the domain of machine learning (ML). However, more recent and capable CV systems, such as those used in self-driving or medical imaging systems, are considerably more complex and belong to CV as part of deep learning (DL).\u003c/p\u003e\u003cp\u003eAs software and algorithms are advancing, so too is hardware infrastructure. Traditional computer processing units (CPUs) can struggle with the large amounts of data and calculations AI requires. New types of hardware have been implemented to train and run AI models faster and more efficiently. For example, the largest modern AI systems (ranging in the hundreds of billions to trillions of parameters) are trained on networks of thousands of graphics processing units (\u003ca rel=\"external\" href=\"https://aws.amazon.com/what-is/gpu/\"\u003eGPUs\u003c/a\u003e), while Tensor Processing Units (\u003ca rel=\"external\" href=\"https://cloud.google.com/tpu/docs/intro-to-tpu\"\u003eTPUs\u003c/a\u003e) and Neural Processing Units (\u003ca rel=\"external\" href=\"https://support.microsoft.com/en-gb/windows/all-about-neural-processing-units-npus-e77a5637-7705-4915-96c8-0c6a975f9db4\"\u003eNPUs\u003c/a\u003e) are increasingly used to optimise ML training.\u003c/p\u003e\u003cp\u003eBelow we provide an overview of the main fields of AI. For a deeper understanding of these fields you can:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003etake the \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg\"\u003efree e-learning courses available\u003c/a\u003e on Civil Service Learning\u003c/li\u003e\n  \u003cli\u003euse the other training opportunities mentioned in the Acquiring skills and talent section\u003c/li\u003e\n  \u003cli\u003erefer to the \u003ca href=\"https://www.gov.uk/government/publications/ai-insights\"\u003eAI insights articles\u003c/a\u003e \u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"neural-networks\"\u003eNeural networks\u003c/h3\u003e\u003cp\u003eNeural networks (NNs), or artificial neural networks (ANNs), are a computational model inspired by biological NNs in the human brain. They were \u003ca rel=\"external\" href=\"https://cs.stanford.edu/people/eroberts/courses/soco/projects/neural-networks/History/history1.html\"\u003einitially created as an aspect of ML\u003c/a\u003e in the 1940s but they may now be considered as components of more elaborate DL systems.\u003c/p\u003e\u003ch4 id=\"how-do-nns-work\"\u003eHow do NNs work?\u003c/h4\u003e\u003cp\u003eAn NN learns through exposure to data in training cycles. During these cycles, the model gradually adjusts the connections between different parts of the network, setting the weights between nodes of adjacent layers. When the model makes an error, the network calculates it as data and checks how far off it was. The error is then adjusted backwards through the network by updating the weights by a very small amount (the \u003ca rel=\"external\" href=\"https://developers.google.com/machine-learning/crash-course/reducing-loss/learning-rate\"\u003elearning rate\u003c/a\u003e) in the direction that will serve to minimise that error. Repeated executions of this process during the training phase tune the network so that it can generalise faithfully on related data it has never seen before. \u003c/p\u003e\u003ch4 id=\"applications-of-nns\"\u003eApplications of NNs\u003c/h4\u003e\u003cp\u003eNNs have a wide range of applications due to their ability to recognise patterns, process new data, make predictions and improve over time. NNs are often deployed in image recognition applications, medical imaging, speech recognition models, autonomous systems, and in many other examples.\u003c/p\u003e\u003ch3 id=\"machine-learning\"\u003eMachine learning\u003c/h3\u003e\u003cp\u003eML is the branch of AI that learns from data. It does this by extracting features from data and learning the relationships between those features. Anywhere there is data, there is an opportunity to learn from it. ML can provide the public sector with unique and quantifiable insights into data that were previously impossible, expensive, or of limited or short-term utility.\u003c/p\u003e\u003ch4 id=\"how-does-ml-work\"\u003eHow does ML work?\u003c/h4\u003e\u003cp\u003eML uses algorithms to analyse data, learn patterns, and then make predictions or decisions based on new data. To work effectively, ML systems must be trained using carefully selected information. This training helps create an optimised model that identifies pertinent features in the data, and weights their relationships to understand how they relate to each other.\u003c/p\u003e\u003cp\u003eML systems can be trained using either ‘labelled’ or ‘unlabelled’ data, or a combination of both. Labelled data is data that has been tagged or categorised in advance. This form of training is usually called ‘supervised learning’ because the model is trained under the supervision of labelled data. Each example in the training data set comes with a correct answer, or label, from which the model is supposed to learn. When unlabelled data is used, the model is programmed to identify patterns, groupings or structures on its own. This is often referred to as ‘unsupervised learning’.\u003c/p\u003e\u003ch4 id=\"applications-of-ml\"\u003eApplications of ML\u003c/h4\u003e\u003cp\u003eThe ability of ML models to learn from data, identify changes, and update based on changes in underlying data enables a host of capabilities that would simply not be possible through traditional linear, deterministic systems. ML models can be used as elements of much larger DL models, or as data processing participants in a sequence of operations, and they underpin many applications including fraud detection, feedback analysis, image processing, summarisation, and more. Modern large language models (LLMs) are also examples of ML systems.\u003c/p\u003e\u003ch3 id=\"deep-learning\"\u003eDeep learning\u003c/h3\u003e\u003cp\u003eDL is a subset of ML that involves more complex model structures and architectures, including sophisticated NNs, to learn complex patterns and representations from large amounts of data. DL has complemented and expanded upon ML, but ML remains the first-choice candidate for simpler tasks and can be easier to implement with less data and lower computational power. DL, while more resource-intensive, excels in handling complex tasks and larger data sets.\u003c/p\u003e\u003ch4 id=\"how-does-dl-work\"\u003eHow does DL work?\u003c/h4\u003e\u003cp\u003eDL models initially detect simple features – edges in an image, for example – and gradually combine them to recognise more complex patterns, such as identifying a face or understanding speech.\u003c/p\u003e\u003ch4 id=\"applications-of-dl\"\u003eApplications of DL\u003c/h4\u003e\u003cp\u003eDL is used in advanced applications like biomedical research and autonomous driving, but also in image and speech recognition (SR), natural language processing (NLP), personalised recommendations, and more.\u003c/p\u003e\u003ch3 id=\"speech-recognition\"\u003eSpeech recognition\u003c/h3\u003e\u003cp\u003eSR, sometimes referred to as automatic speech recognition (ASR), is a field of ML dedicated to processing speech. SR includes both systems that convert speech into text (STT) and new speech-to-speech (S2S) systems. \u003c/p\u003e\u003ch4 id=\"how-does-sr-work\"\u003eHow does SR work?\u003c/h4\u003e\u003cp\u003eComputers only understand numbers, so the challenge with SR is to turn spoken words into numbers while keeping their meaning and context. To do so, SR models first convert speech into numeric representations called spectrograms, which show sound frequencies over time. DL models then analyse these spectrograms to identify sounds, words or sentences. In STT, this process results in written text. In S2S, the recognised words are further processed by another model that translates the text into a different language, which is then turned back into spoken words.\u003c/p\u003e\u003ch4 id=\"applications-of-sr\"\u003eApplications of SR\u003c/h4\u003e\u003cp\u003eSR has expanded considerably over recent years, especially in terms of voice recognition. This technology is now used in banking, personal agents like Siri and Alexa, and general SR functionality built into phones and cars, among other things. Modern SR applications include call analytics, emergency services, meeting summarisation and media subtitling, and more.\u003c/p\u003e\u003ch3 id=\"computer-vision\"\u003eComputer vision\u003c/h3\u003e\u003cp\u003eCV is a field of AI and ML that enables computers to interpret, analyse and understand visual information (images and videos) to perform tasks such as object recognition, facial recognition and scene understanding. Although CV has been researched since the 1960s, it has progressed considerably in recent decades due to a combination of developments in model architecture, hardware performance, and data quality and volumes.\u003c/p\u003e\u003ch4 id=\"how-does-cv-work\"\u003eHow does CV work? \u003c/h4\u003e\u003cp\u003eCV systems are intended to make image data comprehensible and interpretable so that it can be consumed and evaluated appropriately. These evaluations most commonly include classification, object detection, video analysis and image segmentation. \u003c/p\u003e\u003cp\u003eTo perform these tasks, CV ​​uses algorithms to break down images into pixels and process the pixels to detect edges, shapes and colours. The system then uses this information to recognise objects, people or scenes – similarly to how our brains process visual information – allowing computers to identify the content of images.\u003c/p\u003e\u003ch4 id=\"applications-of-cv\"\u003eApplications of CV\u003c/h4\u003e\u003cp\u003eCV has many applications in fields such as facial recognition, quality control, healthcare and medical imaging, surveillance, robotics, and more. \u003c/p\u003e\u003ch3 id=\"natural-language-processing\"\u003eNatural language processing\u003c/h3\u003e\u003cp\u003eNLP is a field within AI that focuses on processing human language. NLP combines computational linguistics and machine learning to analyse large amounts of natural language data and comprehend, interpret, translate, and generate language and language-related data. LLMs, which are in widespread use, are a subset of NLP, with NLP \u003ca rel=\"external\" href=\"https://georgetownlawtechreview.org/natural-language-processing/GLTR-11-2016/\"\u003epreceding them by several decades\u003c/a\u003e.\u003c/p\u003e\u003ch4 id=\"how-does-nlp-work\"\u003eHow does NLP work?\u003c/h4\u003e\u003cp\u003eNLP uses algorithms to convert text into numerical representations that are then processed by an ML model. This process involves a series of steps including noise reduction, tokenisation, stop-words removal, stemming or lemmatisation, vectorisation, and embeddings. The model uses the weights learned during training to adjust the importance of different features in the input data, and considers various elements, including the order and context of words, to understand the overall meaning of the text and produce the required outputs.\u003c/p\u003e\u003ch4 id=\"applications-of-nlp\"\u003eApplications of NLP\u003c/h4\u003e\u003cp\u003eNLP is used for a wide variety of applications and tasks related to language, including machine translation, document classification, sentiment analysis, parts-of-speech tagging, named-entity recognition, text summarisation and conversational AI.\u003c/p\u003e\u003cp\u003eFor government departments, NLP can play an important role in managing and processing large volumes of human language data from sources like emails, letters and online forms, enabling efficient and meaningful analysis.\u003c/p\u003e\u003ch3 id=\"generative-ai\"\u003eGenerative AI\u003c/h3\u003e\u003cp\u003eGenerative AI is a subset of AI capable of generating text, images, video or other forms of output by using probabilistic models trained across various domains. Generative AI learns from large amounts of specially curated training data to discern and replicate complex patterns and structures. The output of generative AI models mimics the characteristics learned from the training data, enabling a range of novel applications. These include personalised content generation, advanced analysis and evaluation, and aiding creative processes.\u003c/p\u003e\u003cp\u003eExamples of publicly accessible generative AI tools are ChatGPT, Claude, Gemini and Dall-E. Generative AI is also becoming increasingly integrated into mainstream products. Examples include the Adobe Photoshop Generative Fill tool, AWS ChatOps Chatbot, Microsoft 365 Copilot and Google Duet AI.\u003c/p\u003e\u003ch4 id=\"how-does-generative-ai-work\"\u003eHow does generative AI work?\u003c/h4\u003e\u003cp\u003eGenerative AI uses large quantities of carefully selected data to train models so they may learn the underlying patterns and structure of data. A well-known example of generative AI is LLMs, which are large neural networks specifically trained on text and natural language data to generate high quality, text-based outputs. However, there are many other generative AI models that are not purely text based.\u003c/p\u003e\u003cp\u003eOnce trained, generative AI models are capable of generating new content consistent with the features and relationships learned from the training data. When a user provides a prompt or input to an LLM, the model evaluates the probability of potential responses based on what it has learned from its training data. It then selects and presents the response that is likeliest to be the right fit for the given prompt.\u003c/p\u003e\u003ch4 id=\"applications-of-generative-ai\"\u003eApplications of generative AI\u003c/h4\u003e\u003cp\u003eThe applications of generative AI go beyond the simple generation of new text or images. For example, generative AI is used in medicine to test molecular structures for drug discovery, and in the financial sector to generate multimodal data surfaces approximating trading and market conditions in order to test safety, security, trading and trade surveillance models. Generative Adversarial Networks (GANs) are also used for creating synthetic data sets to train ML models, as well as in other areas such as \u003ca rel=\"external\" href=\"https://mitsloan.mit.edu/ideas-made-to-matter/deepfakes-explained\"\u003edeepfake\u003c/a\u003e detection, self-driving model ensembles, and text-to-image synthesis. You can explore more applications of generative AI in the Applications of AI in government section.\u003c/p\u003e\u003cp\u003eFor more on how to build, fine-tune and use generative AI solutions, refer to our series of \u003ca href=\"https://www.gov.uk/government/publications/ai-insights\"\u003eAI insights\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"agentic-ai\"\u003eAgentic AI\u003c/h3\u003e\u003cp\u003eAgentic AI refers to autonomous AI systems that can make decisions and perform actions with minimal human intervention. These agents are capable of understanding their environment, identifying the set of tools and functions at their disposal, and using those to take actions to achieve their objectives.\u003c/p\u003e\u003cp\u003eAgent-based systems use Foundation Models (primarily LLMs) to match their capabilities with their objectives. For example, an order processing system may have multiple agents autonomously capturing pricing and market-related data. When a request for an order is raised, the agentic system may utilise those prices or, seeing that they haven’t been updated recently, may use another agent to retrieve the latest price available.This all happens automatically in the background because the system knows which agents can perform each task.\u003c/p\u003e\u003cp\u003eThese systems provide a simpler way to build powerful AI-driven solutions by focusing on the capabilities of each agent and letting the AI model itself figure out the best way to achieve the system’s objectives. The high level of abstraction involved in this technology makes it easier to create more efficient and effective systems that take full advantage of AI.\u003c/p\u003e\u003ch3 id=\"ethics-and-societal-impact\"\u003eEthics and societal impact\u003c/h3\u003e\u003cp\u003eAs research developments have increased the presence of AI in the public sector and everyday life, ethical and societal considerations remain essential to the adoption and use of AI.\u003c/p\u003e\u003cp\u003eApplications of AI in government offer important benefits, such as improving productivity and enhancing access to services. However, AI systems also have limitations and can have two-sided impacts if not used in responsible ways and with the appropriate safeguards in place.\u003c/p\u003e\u003cp\u003eYou can read more on the legal, ethical and security implications of AI in the Using AI safely and responsibly section.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Applications of AI in government",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#applications-of-ai-in-government",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#applications-of-ai-in-government",
        "text": "\u003cp\u003eAI has a broad range of capabilities and has been relevant to the work of government for many years. The ability of generative AI to process language and produce new text, images and code has further enhanced the potential applications of AI technology. \u003c/p\u003e\u003cp\u003eYou can find an overview of how government bodies are using AI – including the central government’s plans for supporting the adoption of AI – in the report on the \u003ca rel=\"external\" href=\"https://www.nao.org.uk/reports/use-of-artificial-intelligence-in-government/\"\u003eUse of artificial intelligence in government (2024)\u003c/a\u003e by the National Audit Office (NAO). \u003c/p\u003e\u003cp\u003eThe following table presents some examples of potential applications of both AI and generative AI technologies in government.\u003c/p\u003e\u003ctable\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eApplication\u003c/td\u003e\n      \u003ctd\u003eAI\u003c/td\u003e\n      \u003ctd\u003eGenerative AI\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eSpeed up the delivery of services\u003c/td\u003e\n      \u003ctd\u003eMachine learning (ML) and optical character recognition (OCR) algorithms can support the processing of thousands of handwritten letters per day, reducing response times.\u003c/td\u003e\n      \u003ctd\u003eCan retrieve relevant organisational information faster to answer digital queries or route email correspondence to the right parts of the business.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eReduce staff workload\u003c/td\u003e\n      \u003ctd\u003eAI technologies for facial recognition and data analytics allow for the automatic control of passports in airports, freeing staff from this task.\u003c/td\u003e\n      \u003ctd\u003eCan suggest first drafts of routine email responses or computer code, acting as an autocomplete tool for algorithms.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003ePerform complicated tasks\u003c/td\u003e\n      \u003ctd\u003eML can analyse very large data sets, identify trends and anomalies in complex historical data, and support data-driven decision making.\u003c/td\u003e\n      \u003ctd\u003eCan help review and summarise huge amounts of information, as well as identify and correct errors in long algorithms.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003ePerform specialist tasks more cost-effectively\u003c/td\u003e\n      \u003ctd\u003ePredictive analytics can identify future needs for resources, optimising budget allocations and planning. ML can detect fraudulent activities in different fields, preventing financial losses and protecting against cyber threats.\u003c/td\u003e\n      \u003ctd\u003eCan summarise documentation containing specialist language like financial or legal terms, or translate a document into several different languages.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eImprove the quality of services\u003c/td\u003e\n      \u003ctd\u003eRecommender systems can help users navigate government web pages and find the information they need. AI can also analyse thousands of feedback messages and suggest service improvements.\u003c/td\u003e\n      \u003ctd\u003eCan improve the readability and accessibility of information on web pages. For example, by simplifying complex language, improving formatting and generating alternative text for images.\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\u003cp\u003eThe Identifying use cases for AI section can help you select appropriate applications of AI tools. For real-life examples of some of these applications, refer to the AI use cases in the public sector appendix.\u003c/p\u003e\u003cp\u003eHowever, AI systems still have limitations. You should make sure that you understand these and build appropriate testing and controls into any AI solutions.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Limitations of AI",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#limitations-of-ai",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#limitations-of-ai",
        "text": "\u003cp\u003eThe capabilities of AI are improving over time. However, they do not provide the answer to every problem. Some of the limitations of AI systems are:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ebias: AI systems lack consciousness and their outputs tend to replicate the bias present in the data they were trained on. For example, their performance can be affected by model bias – an innate deviation in the model giving rise to an error between predicted and actual values – and algorithmic bias – which refers to systematic inequality in outcome from a model. For more information, refer to the \u003ca rel=\"external\" href=\"https://assets.publishing.service.gov.uk/media/60142096d3bf7f70ba377b20/Review_into_bias_in_algorithmic_decision-making.pdf\"\u003eReview into bias in algorithmic decision-making (PDF, 9.9MB)\u003c/a\u003e and the \u003ca rel=\"external\" href=\"https://assets.publishing.service.gov.uk/media/65ccf508c96cf3000c6a37a1/Introduction_to_AI_Assurance.pdf\"\u003eIntroduction to AI assurance (PDF, 1,149KB)\u003c/a\u003e guidance. There is more about the ethical implications of bias in the Fairness, bias and discrimination section\u003c/li\u003e\n  \u003cli\u003edata quantity and quality: AI systems heavily rely on the quality and quantity of data to drive accuracy. Insufficient data can lead to the model failing to generalise effectively, and, similarly, poor quality, biased or noisy data can lead to inaccurate outcomes if not managed correctly\u003c/li\u003e\n  \u003cli\u003eaccuracy: it’s difficult to produce an AI system which provides 100% accurate outputs under all conditions. You must be clear about what objective measures you’re assessing the AI outputs against and any factors that impact them \u003c/li\u003e\n  \u003cli\u003etransparency and explainability: some AI models – for example, deep learning architectures and convolutional or recurrent neural networks – can be so sophisticated that it is challenging to trace how a specific input leads to a specific output\u003c/li\u003e\n  \u003cli\u003ecost and sustainability: depending on the tool you use, AI implementation can be complex, time consuming and have considerable compute costs. Ongoing investment is needed to maintain these systems, update them with new data if required, and ensure their performance remains stable over time. It’s important to consider the sustainability impact of deploying AI systems and check if there are better alternatives\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eGenerative AI also has further specific limitations. You need to be aware of these limitations and have checks and assurance in place when using generative AI in your organisation.\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ehallucination (confabulation): large language models (LLMs) and other probabilistic generative models are vulnerable to creating content that appears plausible but may actually be factually incorrect, as they generate content by returning the most likely output for a given input based on the patterns learnt from the data they were trained on, without an actual understanding of the content\u003c/li\u003e\n  \u003cli\u003elack of critical thinking, personal experience and judgement: although some LLM-based AI systems can give the appearance of reasoning and their outputs may appear as if they come from a person, they are in no way sentient\u003c/li\u003e\n  \u003cli\u003esensitive or inappropriate context: LLMs can generate offensive or inappropriate content if not properly guided, since they can replicate any bias or toxic material present in the data they were trained on\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003edomain expertise: LLMs are not domain experts. They are not a substitute for professional advice, especially in legal, medical or other critical areas where precise and contextually relevant information is essential\u003c/p\u003e\n  \u003c/li\u003e\n  \u003cli\u003edynamic real-time information retrieval: some well-known LLMs such as ChatGPT, Gemini and Copilot are now able to include access to real-time internet data in their results. But there are still many LLMs that do not have real-time access to the internet or data outside their training set\u003c/li\u003e\n\u003c/ul\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Building AI solutions",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#building-ai-solutions",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#building-ai-solutions",
        "text": "\u003cp\u003e​​This section outlines the practical steps you’ll need to take when building AI solutions, including defining the goal, building the team, managing business change, buying AI and building the solution. It supports: \u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ePrinciple 1: You know what AI is and what its limitations are\u003c/li\u003e\n  \u003cli\u003ePrinciple 3: You know how to use AI securely\u003c/li\u003e\n  \u003cli\u003ePrinciple 4: You have meaningful human control at the right stage\u003c/li\u003e\n  \u003cli\u003ePrinciple 5: You understand how to manage the AI life cycle\u003c/li\u003e\n  \u003cli\u003ePrinciple 6: You use the right tool for the job\u003c/li\u003e\n  \u003cli\u003ePrinciple 8: You work with commercial colleagues from the start\u003c/li\u003e\n  \u003cli\u003ePrinciple 9: You have the skills and expertise needed to implement and use AI\u003c/li\u003e\n  \u003cli\u003ePrinciple 10: You use these principles alongside your organisation’s policies and have the right assurance in place\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eHowever, following the guidance in this section is only part of what is needed to build AI solutions. You also need to make sure that you’re using AI safely and responsibly. \u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Building the team",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#building-the-team",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#building-the-team",
        "text": "\u003cp\u003eLike any other digital service, developing AI projects requires more than just technology. You will need to work collaboratively in a multidisciplinary team with diverse expertise.\u003c/p\u003e\u003cp\u003eThe government Service Manual offers \u003ca href=\"https://www.gov.uk/service-manual/the-team/set-up-a-service-team\"\u003estep-by-step guidance on how to set up a service team\u003c/a\u003e for the development and management of digital services. At the outset of your AI project, you need to form a service team and identify key collaborations with other teams and departments.\u003c/p\u003e\u003cp\u003eYour minimum viable AI team must be able to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eidentify \u003ca href=\"https://www.gov.uk/service-manual/user-centred-design/user-needs.html\"\u003euser needs\u003c/a\u003e and \u003ca href=\"https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction#meeting-government-accessibility-requirements\"\u003eaccessibility requirements\u003c/a\u003e \u003c/li\u003e\n  \u003cli\u003emanage and report to stakeholders and other teams, collaborating with different field experts\u003c/li\u003e\n  \u003cli\u003edesign, build, test and iterate AI products, using \u003ca href=\"https://www.gov.uk/service-manual/agile-delivery\"\u003eagile methodologies\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003eensure the responsible development of lawful, ethical, secure and safe-by-design AI services\u003c/li\u003e\n  \u003cli\u003ebe able to collect, process, store and manage data ethically, safely and securely\u003c/li\u003e\n  \u003cli\u003etest with real users and measure the performance of the service\u003c/li\u003e\n  \u003cli\u003esupport the live running of the service, iterate and retire it\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou can use the \u003ca rel=\"external\" href=\"https://ddat-capability-framework.service.gov.uk/\"\u003eGovernment Digital and Data Profession Capability Framework\u003c/a\u003e to identify skill gaps in your team, forecast workforce needs and create effective and consistent job adverts. Be aware that your capability needs will change during the project life cycle: use the Service Manual to understand the \u003ca href=\"https://www.gov.uk/service-manual/the-team/set-up-a-service-team#the-service-team-you-need-in-each-phase\"\u003eroles you will need in the different phases of your project\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eYou will need the right balance between technical and domain expertise, depending on the nature of your project. As well as building a team that contains the right skills, make sure it includes a diversity of groups and viewpoints to help you stay alert to risks of bias and discrimination. Your team should include or be able to collaborate with:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ebusiness leaders and experts who understand the context and impact on users and services\u003c/li\u003e\n  \u003cli\u003edata scientists who understand the relevant data, how to use it effectively, and how to build and train and test models\u003c/li\u003e\n  \u003cli\u003esoftware engineers who can build and integrate solutions\u003c/li\u003e\n  \u003cli\u003euser researchers and designers who can help understand user needs and design compelling experiences\u003c/li\u003e\n  \u003cli\u003elegal, commercial and security colleagues, as well as ethics and data privacy experts, who can help you make your AI solution safe and responsible\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eConsidering the current shortage of AI talent, consider adopting strategies that combine new hires, \u003ca href=\"https://www.gov.uk/service-manual/the-team/working-contractors-third-parties\"\u003eworking with contractors or third parties\u003c/a\u003e, and internal upskilling. \u003c/p\u003e\u003ch3 id=\"acquiring-skills-and-talent\"\u003eAcquiring skills and talent\u003c/h3\u003e\u003cp\u003eThe professional skills required for working in the digital space are outlined in the \u003ca href=\"https://www.gov.uk/government/collections/digital-data-and-technology-profession-capability-framework#it-operations-job-family\"\u003eGovernment Digital and Data Profession Capability Framework\u003c/a\u003e. You can find more information on building a digital and data team in the government \u003ca href=\"https://www.gov.uk/service-manual\"\u003eService Manual\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eAs AI skills are in high demand, their inclusion in the government’s talent strategy is critical. Currently, the Government Digital and Data Profession’s approach involves integrating AI modules and learning materials into existing talent offerings. You can also source AI skills through senior digital secondments and internal collaboration with allied professions such as data engineering. \u003c/p\u003e\u003cp\u003eRaising general awareness of AI throughout the Civil Service and upskilling current digital and data professionals on AI remains crucial. Every civil servant can take up to \u003ca href=\"https://www.gov.uk/guidance/training-and-development-opportunities-in-the-the-civil-service#building-skills-and-competencies\"\u003e5 days of learning per year\u003c/a\u003e and some departments allow more days than others. It’s important that the members of your team have the necessary time and resources to update their knowledge and skills. Data, analytics and digital professionals can also benefit from having sandboxes and spaces to experiment with AI in a safe and secure way.\u003c/p\u003e\u003cp\u003eThe cross-government user research on AI conducted by the Central Digital and Data Office (CDDO) in 2023 and 2024 identified the 5 groups of learners below.\u003c/p\u003e\u003col\u003e\n  \u003cli\u003eBeginners: civil servants who are new to AI and need to be familiar with its concepts, benefits, limitations and risks to be able to work with AI tools safely, responsibly and efficiently. Learning for this audience should focus on having an improved understanding of AI.\u003c/li\u003e\n  \u003cli\u003eTechnical roles outside government digital and data: civil servants in roles such as operational delivery and policy profession who are likely to use AI in their work for tasks such as information retrieval and text or image generation. Learning for this audience should provide the necessary knowledge and skills to make effective and responsible use of appropriate AI tools – with particular attention given to generative AI, its limitations and prompt engineering.\u003c/li\u003e\n  \u003cli\u003eData and analytics professionals: civil servants who work on the collection, organisation, analysis and visualisation of data, with varying levels of expertise in AI. Learning for this audience should advance their understanding of AI with a focus on the implementation and use of AI to facilitate automated data analysis, the synthesis of complex information, and the generation of predictive models. \u003c/li\u003e\n  \u003cli\u003eDigital professionals: civil servants with advanced digital skills who are likely to work on the development of AI solutions in government. Learning for this audience should address the technical aspects and implementation challenges associated with fostering AI innovation, and provide opportunities to collaborate with leading industries and academic institutions. \u003c/li\u003e\n  \u003cli\u003eLeaders: decision makers and senior civil servants who are responsible for creating an AI-ready culture in government. Learning for this audience should provide resources and workshops that are accessible to non-digital and data professionals and focus on the latest trends in AI, including its potential impact on organisational culture, governance, ethics and strategy.\u003c/li\u003e\n\u003c/ol\u003e\u003cp\u003eThe AI Policy Directorate in the Department for Science, Information and Technology (DSIT) has published, in collaboration with \u003ca rel=\"external\" href=\"https://www.ukri.org/councils/innovate-uk/\"\u003eInnovate UK\u003c/a\u003e and the \u003ca rel=\"external\" href=\"https://www.turing.ac.uk/\"\u003eAlan Turing Institute\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://iuk.ktn-uk.org/news/ai-skills-for-business-guidance-feedback-consultation-call-from-the-alan-turing-institute/\"\u003eAI Skills for Business Guidance\u003c/a\u003e. While designed for business, this guidance offers a helpful overview of the knowledge and skills needed to harness the opportunities and navigate the practical challenges of AI.\u003c/p\u003e\u003ch4 id=\"learning-resources\"\u003eLearning resources\u003c/h4\u003e\u003cp\u003eWe’ve launched a series of free online learning resources for all civil servants. The resources cover multiple topics:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\n\u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg\"\u003efundamentals of AI and generative AI\u003c/a\u003e, including their capabilities and limitations\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw\"\u003eunderstanding AI ethics\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/oPEgywmEQumlVAmXlgnRqw\"\u003ethe business value of AI\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003ean overview of the main \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/P1HGnf2xQqeJraSkweLvXg\"\u003egenerative AI tools and applications\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003ecourses on \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/hSsuur6CR-K_-QsXhUxO0Q\"\u003eworking with large language models\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/Bv0vdvyaQ7GuzX2Qgg46oA\"\u003emachine learning and deep learning\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/yBJYpdlUSEmpVWi1YLImUw\"\u003enatural language processing and speech recognition\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/OMT6z8o0S9yhQaIP8dbTQg\"\u003ecomputer vision\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003ea \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/tULvvOtbQaGmjLo9bazHtQ\"\u003etechnical curriculum\u003c/a\u003e with courses leading to certificates in different AI fields \u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eAI courses are freely \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg\"\u003eavailable within Civil Service Learning\u003c/a\u003e, as well as a series of \u003ca rel=\"external\" href=\"https://prospectus.governmentcampus.co.uk/04-artificial-intelligence/\"\u003eAI courses from the Government Campus\u003c/a\u003e which can be accessed through the learning frameworks.\u003c/p\u003e\u003cp\u003eSenior civil servants, grade 7s and grade 6s, can also sign up to the AI course within \u003ca rel=\"external\" href=\"https://cddo.blog.gov.uk/2023/01/11/the-digital-excellence-programme/\"\u003eThe Digital Excellence Programme\u003c/a\u003e. This was designed by CDDO and the Government Skills and Curriculum Unit (GSCU) to support leaders outside of the digital and data profession to become pioneers of the government’s digital transformation.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eMake full use of the training resources available, including those on \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/3TI2CaGVQ8y8Gx06k-0Qsg\"\u003eCivil Service Learning\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eBuild a multidisciplinary team with all the expertise and support you need.\u003c/li\u003e\n    \u003cli\u003eUse multiple talent acquisition strategies and upskill current digital and data professionals in your team.\u003c/li\u003e\n    \u003cli\u003eConsider providing data, analytics and digital professionals with sandboxes and safe spaces to experiment with AI securely.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"working-collaboratively\"\u003eWorking collaboratively\u003c/h3\u003e\u003cp\u003eYou need to work collaboratively to develop effective AI solutions. Your team should establish and maintain relationships with internal and external stakeholders with varying levels of familiarity with AI technologies.\u003c/p\u003e\u003cp\u003eYou must be ready to collaborate with \u003ca href=\"https://www.gov.uk/service-manual/the-team/set-up-a-service-team#working-with-other-teams-in-your-organisation\"\u003eother teams in your organisation\u003c/a\u003e to address occasional knowledge and capability gaps in your team. Building AI solutions requires industry knowledge, whereas scaling and deploying these solutions will require considerations from software and/or technical infrastructure teams. Engaging these teams early on creates clear requirements and parameters for the proposed solutions, and sets a path for success. \u003c/p\u003e\u003cp\u003eTo avoid siloed approaches and duplication of work, look for opportunities for cross-government and industry events that will help your team stay updated on the latest developments and best practices. \u003c/p\u003e\u003cp\u003eFor example, consider joining the \u003ca href=\"https://www.gov.uk/service-manual/communities/artificial-intelligence-community\"\u003eAI community of practice\u003c/a\u003e. This offers monthly meetings for people working on or interested in AI in the public sector. You can sign up to this community by using \u003ca rel=\"external\" href=\"https://www.us14.list-manage.com/subscribe?u=751cf84762295209ed8291813\u0026amp;id=db38e5da8d\"\u003ethe dedicated form\u003c/a\u003e. \u003c/p\u003e\u003cp\u003eDepending on the nature of your project, you should also consider \u003ca href=\"https://www.gov.uk/government/news/whitehall-set-to-bring-in-ai-and-data-experts-under-plans-to-turbocharge-productivity\"\u003eworking with industry experts and academic institutions\u003c/a\u003e to foster knowledge exchange and access to cutting-edge research. Additionally, engaging with broader civil society will help you understand people’s values, concerns, and priorities, ensuring your AI solution meets the needs of the public we serve.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Defining the goal ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#defining-the-goal",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#defining-the-goal",
        "text": "\u003cp\u003eLike all technology, using AI is a means to an end – not an objective in itself. Whether planning your first use of AI or a broader transformation programme, you should be clear on:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ethe goals you want to achieve\u003c/li\u003e\n  \u003cli\u003ethe needs of your users\u003c/li\u003e\n  \u003cli\u003ewhere you can most effectively use AI technologies, and where you should avoid them entirely\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eFor example, you can use AI to automate and streamline processes, optimise the use of resources, foster data-driven decision making, and improve the quality and accessibility of some services. Goals for the use of generative AI may include improved public services or productivity, increased staff satisfaction, cost savings or risk reduction.\u003c/p\u003e\u003cp\u003eThe sections below will help you understand when AI might be the right tool for your project.\u003c/p\u003e\u003ch3 id=\"user-research-for-ai\"\u003eUser research for AI\u003c/h3\u003e\u003cp\u003eUser research (UR) can be critical to the success of your AI project. It is an important mechanism for engaging with people both inside and outside government to understand their needs, values and priorities. It provides key insights into the way the humans who will use the product or service behave, think and feel. This insight helps us ensure we use AI to deliver tangible benefits to individuals and society as a whole. It’s an exciting opportunity to collaborate on the design of an experimental approach, including the development of metrics, which will support the continuous improvement of your AI solution.\u003c/p\u003e\u003cp\u003eDoing UR for an AI project helps you keep the human in the loop and understand the human intelligence that the AI will replicate or imitate. Through UR, you can observe how people complete tasks and solve problems, understand their attitudes, and uncover the cultural issues that may impact the adoption and use of an AI solution. These insights will help you to identify where and how a human needs to be involved. You can find information about \u003ca href=\"https://www.gov.uk/service-manual/user-research\"\u003ehow to do user research\u003c/a\u003e in the government Service Manual.\u003c/p\u003e\u003cp\u003eDeveloping AI products and services involves some specific activities that user research can be particularly useful for. Some of these are critical to your AI project and some supplement the existing data science process. These include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eunderstanding if AI is the right tool: do early UR to observe and speak with people who are involved in processes, whether these are internal users or the general public. This can help you understand users’ problems and aspirations. At this stage, a user researcher can work with a business analyst to find efficiencies in business processes and measure dissatisfaction. This will help establish a baseline of metrics to measure your project outcomes against. UR insights can also help you to work out an appropriate level of model accuracy when working with subjective outputs, and understand biases in the existing human process \u003c/li\u003e\n  \u003cli\u003edefining performance metrics: user researchers can work with analytical colleagues to use insights from research to define model and system metrics and methodologies for measuring ongoing performance \u003c/li\u003e\n  \u003cli\u003epreparing data: understand what data your users use, and how they think and talk about an activity. This will help you find, categorise and summarise appropriate training data. User researchers can help with supervised learning if you need human judgement to produce correct labelling of your training data. To consider how bias can affect labelling, refer to the Fairness, bias and discrimination section\u003c/li\u003e\n  \u003cli\u003esynthesising data: user researchers can help review and assess the appropriateness of generated synthetic data by developing and managing a process for bringing subject matter experts (SMEs) into the loop. If you’re planning to use synthetic data, check the UK Statistic Authority’s \u003ca rel=\"external\" href=\"https://uksa.statisticsauthority.gov.uk/publication/ethical-considerations-relating-to-the-creation-and-use-of-synthetic-data/pages/2/#:~:text=Synthetic%20data%20are%20an%20artificial,re%2Didentification%20is%20almost%20impossible.\"\u003eEthical considerations relating to the creation and use of synthetic data\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003eevaluating your model’s output: as well as methods like statistical evaluation, it’s good practice to test the accuracy and relevance of your model’s outputs with a sample of your users. Methods such as reinforcement learning from human feedback (RLHF) are important to consider when training models – for example, when ranking different outputs. Humans can be particularly helpful when working on tasks that are difficult to specify but easy to judge, such as producing text that lacks bias, toxicity or other harmful content\u003c/li\u003e\n  \u003cli\u003emeasuring usability of the product: observing how users respond to and use the outputs of your AI system will help you understand if they can use it to confidently complete their tasks. Design this testing into the way the AI solution is monitored, so that you can better understand data drift through user behaviour and changes over time\u003c/li\u003e\n  \u003cli\u003eunderstanding attitudes to the product or system: UR will help you understand levels of trust, confidence, and how the solution is used for decision making. These insights are useful to understand why model metrics might look good but service metrics are less positive. Model metrics measure how well your technology is performing, whereas service metrics help you understand if users’ needs and business goals are being met, which can be very different. For example, a machine learning (ML) algorithm could have a very high accuracy rate, but the users of the wider system could misinterpret its output and follow an unexpected course of action or completely ignore its recommendation through lack of trust\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction\"\u003eaccessibility\u003c/a\u003e: it’s particularly important to work out whether any users are being excluded from using your product or service, either intentionally or unintentionally. UR with a realistic sample can help you identify groups that may be excluded and why\u003c/li\u003e\n  \u003cli\u003eidentifying how the system is being used: your AI solution may be used in ways you did not originally intend. Understanding why and what is happening can help you better meet the needs of your users and identify both risks and opportunities for innovation\u003c/li\u003e\n  \u003cli\u003emonitoring the AI solution while in service: building regular UR into how the solution is managed will help you continuously improve your product\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThe skills needed to collaborate on the steps above are within the usual skillset of most experienced user researchers. A user researcher on an AI project will need to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eengage with the technology\u003c/li\u003e\n  \u003cli\u003ebe adaptable in their approach\u003c/li\u003e\n  \u003cli\u003edesign studies with technical and analytical colleagues\u003c/li\u003e\n  \u003cli\u003euse attitudinal methods\u003c/li\u003e\n  \u003cli\u003eunderstand how to do research that supports change management\u003c/li\u003e\n  \u003cli\u003edesign studies to assess how user behaviour changes over time. For example, developing strategies for creating continuous feedback loops \u003c/li\u003e\n  \u003cli\u003ehave experience doing generative research\u003c/li\u003e\n  \u003cli\u003edo concept testing\u003c/li\u003e\n  \u003cli\u003eunderstand privacy and data ethics\u003c/li\u003e\n  \u003cli\u003ebe able to communicate in non-technical language to explain the AI system from a user perspective\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-1\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eConsult the government \u003ca href=\"https://www.gov.uk/service-manual/user-research\"\u003eService Manual’s sections on user research\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eGet your organisation’s user researchers involved in the project from the start so that research and evaluation is designed into the way the AI solution is supported.\u003c/li\u003e\n    \u003cli\u003eWork with user researchers to design into the product or service ways of continuously assessing how effectively the AI model meets user needs. \u003c/li\u003e\n    \u003cli\u003eRead the example use case GOV.UK Chat: doing user research for AI products.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"identifying-use-cases-for-ai\"\u003eIdentifying use cases for AI\u003c/h3\u003e\u003cp\u003eWhen thinking about how your organisation could benefit from AI, consider the possible situations or appropriate use cases. This must be led by business and user needs, pain points and inefficiencies, not what the technology can do.\u003c/p\u003e\u003cp\u003eOnce you’ve identified the challenges and opportunities through user research, focus on use cases that can only be solved by AI or where AI offers significant advantages over existing techniques. \u003c/p\u003e\u003cp\u003eYou can do this by considering whether traditional solutions might be unable to handle the volume, complexity or real-time nature of the task; or whether the problem relates to, for example, advanced pattern detection in large data sets, automating complex, dynamic decision-making processes, or providing personalisation. Evaluate the potential impact of implementing AI solutions on these problems using tools like cost-benefit analysis to consider improvements in efficiency, accuracy or cost reduction.\u003c/p\u003e\u003cp\u003eYou should also assess the feasibility of implementing AI in your team. Are the necessary skills and infrastructure in place to implement and maintain AI solutions? Would you need to train current employees, hire new talent or partner with AI technology providers?\u003c/p\u003e\u003cp\u003eWhen deciding if you’re going to use AI, you should also consider the capabilities and limitations of AI, the use cases to avoid, and discuss your project with technical experts.\u003c/p\u003e\u003cp\u003eTo consider potential use cases of AI in your organisation, check the:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eexamples of applications of AI in government\u003c/li\u003e\n  \u003cli\u003eappendix with case studies on AI projectsrecently developed in the public sector\u003c/li\u003e\n  \u003cli\u003eprojects recorded in the \u003ca href=\"https://www.gov.uk/government/publications/guidance-for-organisations-using-the-algorithmic-transparency-recording-standard/algorithmic-transparency-recording-standard-guidance-for-public-sector-bodies\"\u003eAlgorithmic Transparency Recording Standard (ATRS)\u003c/a\u003e register\u003c/li\u003e\n\u003c/ul\u003e\u003ch4 id=\"use-cases-to-avoid\"\u003eUse cases to avoid \u003c/h4\u003e\u003cp\u003eGiven the current limitations of AI, and their ethical, legal and social implications, there are use cases that are not appropriate and which should be avoided in the public sector. These include but are not limited to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003efully automated decision making: be cautious about any use case involving significant decisions, such as those involving someone’s health or safety. For more detail, refer to the section on Human oversight \u003c/li\u003e\n  \u003cli\u003ehigh-risk or high-impact applications: AI should not be used on its own in high-risk areas which could cause harm to someone’s health, safety, fundamental rights or the environment\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eFollow the principles of using AI safely and responsibly to check if your use case involves significant risks related to bias, fairness, transparency, privacy or human rights.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-2\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eDefine clear goals for your use of AI, and ensure they’re consistent with your organisation’s AI roadmap.\u003c/li\u003e\n    \u003cli\u003eSelect use cases which meet a clear need and fit the capabilities of AI.\u003c/li\u003e\n    \u003cli\u003eFind out what use cases other government organisations are considering.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eUnderstand the limitations of AI and avoid high-risk use cases, considering the principles of using AI safely and responsibly.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"creating-the-ai-support-structure\"\u003eCreating the AI support structure \u003c/h3\u003e\u003cp\u003eTo ensure your organisation adopts AI smoothly, consider how AI will change the way your people and processes work. Check that you have the structures in place to support its adoption. \u003c/p\u003e\u003cp\u003eThese structures do not need to be fully mature before your first project. Your experience in your first AI project will shape the way you organise these structures. \u003c/p\u003e\u003cp\u003eHowever, you should make sure that you have sufficient control over how you use AI and ensure all AI systems operate in a safe and responsible environment. If you do not already have them in place, you should establish the following:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eAI strategy and adoption plan: a clear statement of the way that you plan to use AI within your organisation, including the impact on existing organisation structures and change management plans\u003c/li\u003e\n  \u003cli\u003eAI principles: a simple set of top-level principles which embody your values and goals, and that can be followed by all the people building solutions\u003c/li\u003e\n  \u003cli\u003eAI governance board: a group of senior leaders and experts to set principles and review and authorise uses of AI which fit these principles\u003c/li\u003e\n  \u003cli\u003eAI communication strategy: your approach for engaging with internal and external stakeholders to gain support, share best practice and show transparency\u003c/li\u003e\n  \u003cli\u003eAI sourcing and partnership strategy: a definition of which capabilities you will build within your own organisation, and which you will seek from partners\u003c/li\u003e\n  \u003cli\u003eAI training: resources that your team can use to upskill, based on a learning needs analysis\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou should also consider using:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ea change management team: a small team with access to senior leadership that can shape your approach\u003c/li\u003e\n  \u003cli\u003ea use cases register: a way of capturing use cases and prioritising which you would like to explore first \u003c/li\u003e\n  \u003cli\u003emonitoring systems: to gather feedback and quickly identify emerging risks throughout change processes and for the duration of the system’s life cycle\u003c/li\u003e\n  \u003cli\u003ereview and change processes: to provide staff with sufficient time, information and tools to identify and adapt to risks that emerge during a change process\u003c/li\u003e\n  \u003cli\u003efallback processes: to ensure that critical functionality and services can be maintained if a change must be reverted and/or an AI system terminated\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-3\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eIdentify the support structures you need for your level of AI maturity and adoption, or reuse support structures already in place for other technologies.\u003c/li\u003e\n    \u003cli\u003eDesign an appropriate method of capturing and prioritising opportunities based on feasibility and business value.\u003c/li\u003e\n    \u003cli\u003eDevelop a communication strategy for engaging a wider community of leaders and staff to explain AI, demonstrate activity and reduce resistance to change.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Buying AI ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#buying-ai",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#buying-ai",
        "text": "\u003cp\u003eWhile AI is not new, the AI supply market is evolving rapidly. It’s important that you engage with commercial colleagues to discuss partners, pricing, products and services.\u003c/p\u003e\u003cp\u003eThe Crown Commercial Service (CCS) can guide you through:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eexisting guidance\u003c/li\u003e\n  \u003cli\u003eroutes to market\u003c/li\u003e\n  \u003cli\u003especifying your requirements\u003c/li\u003e\n  \u003cli\u003erunning your procurement\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThey can also help you navigate:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003erunning your procurement in an emerging market\u003c/li\u003e\n  \u003cli\u003eregulation and policy\u003c/li\u003e\n  \u003cli\u003ealigning procurement and ethics\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"ai-business-cases\"\u003eAI business cases \u003c/h3\u003e\u003cp\u003eCreating the business case for your AI project is an important stage in the process of AI adoption as it gives decision makers the opportunity to assess the return of investment in both resources and costs.\u003c/p\u003e\u003cp\u003eBefore writing your business case, you should engage with stakeholders and discuss your project to understand if AI needs to be used, and, if so, which solutions offer the best improvements in productivity.\u003c/p\u003e\u003cp\u003eThere are several resources you can use to write a strong business case. Consider following the Treasury’s \u003ca href=\"https://www.gov.uk/government/publications/the-green-book-appraisal-and-evaluation-in-central-government/the-green-book-2020\"\u003eGreen Book (2022)\u003c/a\u003e guidance to create a fully fledged, five-part business case. This may be a requirement depending on the scale of your project and investment.\u003c/p\u003e\u003cp\u003eYour organisation will likely have investment thresholds. Typically, any investment approaching £10 million will require a five-part business case. You must use the Green Book when that is the case. If the scale of investment is below this threshold, you should strongly consider using the guidance on \u003ca href=\"https://www.gov.uk/government/publications/agile-digital-and-it-projects-clarification-of-business-case-guidance/agile-digital-and-it-projects-clarification-of-business-case-guidance\"\u003eagile business cases\u003c/a\u003e, which was developed by the Government Digital Service (GDS).\u003c/p\u003e\u003cp\u003eWhen working on your business case, note that it’s mandatory to assure all digital and technology spend above £100,000 for anything public facing and £1 million for anything else, through your assurance boards. The process for doing so will be subject to your organisation’s requirements. Follow GDS’s \u003ca href=\"https://www.gov.uk/government/publications/digital-and-technology-spend-control-version-6\"\u003eguidance on getting spend approvals\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIn the following section, we summarise existing guidance on purchasing AI products and services. However, as the field is evolving rapidly this is not exhaustive, and engaging with commercial experts early in your project remains crucial.\u003c/p\u003e\u003ch3 id=\"existing-guidance\"\u003eExisting guidance\u003c/h3\u003e\u003cp\u003eThere is detailed guidance to support the procurement of AI in the public sector. You should familiarise yourself with this guidance and make sure you’re taking steps to align with best practice. The:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/guidelines-for-ai-procurement\"\u003eGuidelines for AI procurement\u003c/a\u003e provide a summary of best practice when buying AI technologies in government, including \u003ca href=\"https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#preparation-and-planning\"\u003epreparation and planning\u003c/a\u003e, \u003ca href=\"https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#publication\"\u003epublication\u003c/a\u003e, \u003ca href=\"https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#selection-evaluation-and-award\"\u003eselection, evaluation and award\u003c/a\u003e and \u003ca href=\"https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#contract-implementation-and-ongoing-management\"\u003econtract implementation and ongoing management\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/the-digital-data-and-technology-playbook\"\u003eDigital, Data and Technology Playbook\u003c/a\u003e provides general guidance on sourcing and contracting for digital and data projects and programmes. All central government departments and their arm’s length bodies are expected to follow this on a ‘comply or explain’ basis. It includes specific guidance on AI and machine learning, as well as Intellectual Property Rights (IPR)\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/the-sourcing-and-consultancy-playbooks\"\u003eSourcing Playbook\u003c/a\u003e defines the commercial process as a whole and includes key policies and guidance for making sourcing decisions for the delivery of public services\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/knowledge-asset-management-in-government\"\u003eKnowledge asset management in government (The Rose Book)\u003c/a\u003e provides guidance on managing and exploiting the wider value of knowledge assets (including software, data and business processes). Annex D contains specific guidance on managing these in procurement\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/digital-and-technology-spend-control-version-6/digital-and-technology-spend-control-version-6\"\u003eDigital and technology spend control version 6\u003c/a\u003e details in which instances your requirements will be subject to functional assurance of central government spending. Spend controls enable the UK government to achieve greater efficiency and better outcomes\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/ppn-0224-improving-transparency-of-ai-use-in-procurement\"\u003eProcurement Policy Note (PPN) 02/24: Improving Transparency of Artificial Intelligence Use in Procurement\u003c/a\u003e provides optional questions to help identify suppliers’ use of AI in response to government procurements and in the delivery of services to government\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"routes-to-market\"\u003eRoutes to market \u003c/h3\u003e\u003cp\u003eConsider the available routes to market and commercial agreements and determine which one is most suitable based on your requirements. \u003c/p\u003e\u003cp\u003eHowever you decide to source your requirement, you must ensure you comply with procurement legislation. This is primarily the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/uksi/2015/102/contents/made\"\u003ePublic Contract Regulations 2015\u003c/a\u003e and the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2023/54/contents/enacted\"\u003eProcurement Act 2023\u003c/a\u003e, which is set to come into effect in 2025. Commercial colleagues will be able to assist with this.\u003c/p\u003e\u003cp\u003eThere are various routes to market to purchase AI systems. Depending on the kind of challenges you’re addressing, you may prefer to use a \u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/news/what-is-a-framework-procurement-essentials\"\u003eframework\u003c/a\u003e or a \u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/news/what-is-a-dynamic-purchasing-system\"\u003eDynamic Purchasing System (DPS)\u003c/a\u003e. CCS offers several frameworks and DPSs for the public sector to procure AI.\u003c/p\u003e\u003cp\u003eA \u003ca href=\"https://www.gov.uk/guidance/public-sector-procurement\"\u003epublic-sector procurement\u003c/a\u003e route also exists, sometimes known as a ‘Find a tender service procurement’ due to the requirement to advertise on that platform. This may be appropriate for bespoke requirements or contractual terms, or where there is no suitable standard offering.\u003c/p\u003e\u003cp\u003eThe table below summarises the differences between a framework agreement and a DPS. There’s more information on the \u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/\"\u003eCCS website\u003c/a\u003e and, on the use of frameworks, in the \u003ca href=\"https://www.gov.uk/government/publications/the-digital-data-and-technology-playbook\"\u003eDigital, Data and Technology Playbook\u003c/a\u003e.\u003c/p\u003e\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"col\"\u003eCategory\u003c/th\u003e\n      \u003cth scope=\"col\"\u003eFramework\u003c/th\u003e\n      \u003cth scope=\"col\"\u003eDPS\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eSupplier access\u003c/td\u003e\n      \u003ctd\u003eSuccessful suppliers are awarded to the framework at launch. \u003cbr\u003e\u003cbr\u003eClosed to new supplier registrations.  \u003cbr\u003e\u003cbr\u003ePrime suppliers can request to add new subcontractors.\u003c/td\u003e\n      \u003ctd\u003eOpen for new supplier registrations at any time.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eStructure\u003c/td\u003e\n      \u003ctd\u003eOften divided into lots by product or service type.\u003c/td\u003e\n      \u003ctd\u003eSuppliers filterable by categories.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eCompliance\u003c/td\u003e\n      \u003ctd\u003eThorough ongoing supplier compliance checks carried out by CCS, meaning buyers have less to do at call-off (excluding G-Cloud).\u003c/td\u003e\n      \u003ctd\u003eBasic compliance checks are carried out by CCS, allowing the buyer to complete these at the call-off.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003ctd\u003eBuying options\u003c/td\u003e\n      \u003ctd\u003eVarious options, including direct award, depending on the agreements.\u003c/td\u003e\n      \u003ctd\u003eFurther competition only.\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e\u003cp\u003eNote that a number of CCS agreements include AI within their scope.\u003c/p\u003e\u003cp\u003eExamples of DPSs include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"http://www.crowncommercial.gov.uk/agreements/rm6200\"\u003eArtificial Intelligence (AI)\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"http://www.crowncommercial.gov.uk/agreements/rm6173\"\u003eAutomation Marketplace\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"http://www.crowncommercial.gov.uk/agreements/rm6094\"\u003eSPARK\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eExamples of frameworks include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"http://www.crowncommercial.gov.uk/agreements/rm6195\"\u003eBig Data and Analytics\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/agreements/RM6098\"\u003eTechnology Products \u0026amp; Associated Services 2\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/agreements/RM6100\"\u003eTechnology Services 3\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/agreements/RM6194\"\u003eBack Office Software\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003e\u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/agreements/RM6292\"\u003eCloud Compute 2\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eIn addition to commercial agreements, CCS has signed a number of \u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/products-and-services/technology/technology-memorandum-of-understanding/\"\u003eMemorandums of Understanding (MoUs) with suppliers\u003c/a\u003e. These MoUs set out preferential pricing and discounts on products and services across the technology landscape, including cloud, software, technology products and services, and networks. You can access MoU savings through any route to market. To get support or find out more, email \u003ca href=\"mailto:info@crowncommercial.gov.uk\"\u003einfo@crowncommercial.gov.uk\u003c/a\u003e. \u003c/p\u003e\u003ch3 id=\"specifying-your-requirements\"\u003eSpecifying your requirements \u003c/h3\u003e\u003cp\u003eWhen buying AI products and services, you’ll need to document your requirements to tell your suppliers what you need. To define what you need, you should engage with subject matter experts (SMEs) as soon as possible, and take time to consider the most appropriate type of AI solution for your project. This might be an off-the-shelf product, an existing technology with bolt-on AI elements (paid or free), outsourcing AI builds (if applicable), or co-creating AI with suppliers. The \u003ca href=\"https://www.gov.uk/government/publications/the-digital-data-and-technology-playbook\"\u003eDigital, Data and Technology Playbook\u003c/a\u003e has guidance on commercial off-the-shelf (COTS) software licensing terms and build versus buy decisions.\u003c/p\u003e\u003cp\u003eWhen drafting requirements for AI, you should:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003estart with your problem statement\u003c/li\u003e\n  \u003cli\u003ehighlight your data strategy and requirements\u003c/li\u003e\n  \u003cli\u003efocus on data quality, bias (mitigation) and limitations \u003c/li\u003e\n  \u003cli\u003eunderline the need for you to understand the supplier’s AI approach\u003c/li\u003e\n  \u003cli\u003econsider strategies to avoid vendor lock-in\u003c/li\u003e\n  \u003cli\u003eapply the \u003ca href=\"https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework\"\u003eData Ethics Framework principles\u003c/a\u003e and consider the appropriate \u003ca rel=\"external\" href=\"https://assets.crowncommercial.gov.uk/wp-content/uploads/Data-ethics-requirements-for-suppliers-template.pdf\"\u003eData ethics requirements and questions (PDF, 62.7KB)\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003emention any integration with associated technologies or services\u003c/li\u003e\n  \u003cli\u003econsider your ongoing support and maintenance requirements\u003c/li\u003e\n  \u003cli\u003econsider data format and provide suppliers with dummy data where possible\u003c/li\u003e\n  \u003cli\u003eprovide guidance on budget to consider hidden costs\u003c/li\u003e\n  \u003cli\u003econsider intellectual property rights and who will have these if new software is developed\u003c/li\u003e\n  \u003cli\u003econsider any acceptable liabilities and appetite for risk, to match against draft terms and conditions, once provided\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eFor more information, read the ‘Selection, evaluation and award’ section of the \u003ca href=\"https://www.gov.uk/government/publications/guidelines-for-ai-procurement/guidelines-for-ai-procurement#selection-evaluation-and-award\"\u003eGuidelines for AI Procurement\u003c/a\u003e and CCS’s guide on \u003ca rel=\"external\" href=\"https://www.crowncommercial.gov.uk/news/how-to-write-a-specification-procurement-essentials\"\u003eHow to write a specification\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eHaving prepared your procurement strategy, defined your requirements and selected your commercial agreement, you can proceed with your procurement. Your commercial team will lead this. \u003c/p\u003e\u003cp\u003eIf you’re using an existing commercial agreement such as a framework or DPS, you’ll conduct a call-off in accordance with the process set out in the relevant commercial agreement. Non-framework or DPS procurements must comply with procurement legislation and relevant policy – for example, the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/uksi/2015/102/contents/made\"\u003ePublic Contract Regulations 2015\u003c/a\u003e, the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2023/54/contents/enacted\"\u003eProcurement Act 2023\u003c/a\u003e and PPNs. \u003c/p\u003e\u003cp\u003eCCS offers buyer guidance tailored to each of its agreements, which describe each step in detail, including completing your order contract and compiling your contract.\u003c/p\u003e\u003ch3 id=\"running-your-procurement-in-an-emerging-market\"\u003eRunning your procurement in an emerging market \u003c/h3\u003e\u003ch4 id=\"commercial-agreements\"\u003eCommercial agreements\u003c/h4\u003e\u003cp\u003eWhile AI is not new, it is an emerging market from a commercial perspective. As well as rapidly evolving technology, there are ongoing changes in the supply base and the products and services it offers. DPSs offer flexibility for new suppliers to join, which often complement these dynamics for buyers. \u003c/p\u003e\u003cp\u003eAny public sector buyers interested in shaping CCS’s longer term commercial agreement portfolio should express their interest by emailing \u003ca href=\"mailto:info@crowncommercial.gov.uk\"\u003einfo@crowncommercial.gov.uk\u003c/a\u003e.\u003c/p\u003e\u003ch4 id=\"regulation-and-policy\"\u003eRegulation and policy\u003c/h4\u003e\u003cp\u003eRegulation and policy will also evolve to keep pace. However, there are already a number of legal and regulatory provisions which are relevant to the use of AI technologies. These include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/consultations/ai-regulation-a-pro-innovation-approach-policy-proposals/outcome/a-pro-innovation-approach-to-ai-regulation-government-response#introduction\"\u003eA pro-innovation approach to AI regulation: government response\u003c/a\u003e: this details the government’s response to the white paper consultation published in March 2023, which set out early steps towards establishing a regulatory regime for AI, including 5 principles to guide responsible AI innovation in all sectors\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques\"\u003ePortfolio of AI assurance techniques\u003c/a\u003e: this portfolio has been developed by the Responsible Technology Adoption Unit (RTAU), initially in collaboration with \u003ca rel=\"external\" href=\"https://www.techuk.org/\"\u003etechUK\u003c/a\u003e. It’s useful for anybody involved in designing, developing, deploying or procuring AI-enabled systems. It shows examples of AI assurance techniques being used in the real world to support the development of trustworthy AI\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance\"\u003eIntroduction to AI assurance\u003c/a\u003e: this guidance has been developed by the Department for Science, Information and Technology (DSIT) to help private sector organisations better understand how to implement AI assurance to ensure the responsible development and deployment of AI systems. It’s designed to be accessible to a range of users, especially those who may not engage with assurance on a day-to-day basis. It introduces them to core assurance definitions and concepts and then how these can be applied to support the development and use of trustworthy AI\u003c/li\u003e\n  \u003cli\u003eAI Management Essentials: DSIT is developing guidance to support private sector organisations to engage in the development of ethical, robust and responsible AI organisational practice. This self-assessment tool will distil key principles from existing AI-related standards and frameworks and provide simple baseline requirements for government suppliers of AI products and services. After testing and consultation, DSIT is planning to work with the Cabinet Office to embed the tool in government procurement frameworks\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThis list is not exhaustive. For further guidance, refer to the Legal considerations section.\u003c/p\u003e\u003ch3 id=\"aligning-procurement-and-ethics\"\u003eAligning procurement and ethics \u003c/h3\u003e\u003cp\u003eIt’s important to consider and factor data ethics into your commercial approach from the outset. \u003c/p\u003e\u003cp\u003eThere’s a range of \u003ca href=\"https://www.gov.uk/guidance/data-ethics-and-ai-guidance-landscape\"\u003eguidance relating to AI and data ethics\u003c/a\u003e to support public servants working with data and/or AI. This guidance collates existing ethical principles, developed by government and public sector bodies. You can also refer to the:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework\"\u003eData ethics framework\u003c/a\u003e: this outlines appropriate and responsible data use in government and the wider public sector. The framework helps public servants understand ethical considerations, address these within their projects, and encourages responsible innovation\u003c/li\u003e\n  \u003cli\u003e\n\u003ca rel=\"external\" href=\"https://assets.crowncommercial.gov.uk/wp-content/uploads/Data-ethics-requirements-for-suppliers-template.pdf\"\u003eData ethics requirements checklist (PDF, 62.7KB)\u003c/a\u003e for suppliers: this will mitigate bias. It will also ensure diversity in development teams, transparency and interpretability, and explainability of the results\u003c/li\u003e\n  \u003cli\u003e\n\u003ca href=\"https://www.gov.uk/government/collections/the-public-sector-contract\"\u003ePublic Sector Contract (PSC)\u003c/a\u003e: this includes provisions related to intellectual property rights, data protection, and equality, diversity and human rights\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-4\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eEngage your commercial colleagues from the outset. Understand and make use of existing guidance.\u003c/li\u003e\n    \u003cli\u003eUnderstand and make use of existing routes to market, including frameworks, DPSs and MoUs.\u003c/li\u003e\n    \u003cli\u003eSpecify clear requirements and plan your procurement carefully.\u003c/li\u003e\n    \u003cli\u003eSeek support from your commercial colleagues to help navigate the evolving market, regulatory and policy landscape.\u003c/li\u003e\n    \u003cli\u003eEnsure that your procurement is aligned to ethical principles.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Using AI safely and responsibly",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#using-ai-safely-and-responsibly",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#using-ai-safely-and-responsibly",
        "text": "\u003cp\u003eThis section outlines the steps you’ll need to take to ensure that you build AI solutions in a safe and responsible way, taking account of legal considerations, ethics, data protection, privacy, security and governance. \u003c/p\u003e\u003cp\u003eMany of these considerations interact with each other, so you should read all of these topics together and seek support from data ethics, privacy, legal and security experts when planning and developing your project. \u003c/p\u003e\u003cp\u003eThis section supports:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ePrinciple 2: You use AI lawfully, ethically and responsibly\u003c/li\u003e\n  \u003cli\u003ePrinciple 3: You know how to use AI securely\u003c/li\u003e\n  \u003cli\u003ePrinciple 4: You have meaningful human control at the right stage\u003c/li\u003e\n  \u003cli\u003ePrinciple 10: You use these principles alongside your organisation’s policies and have the right assurance in place\u003c/li\u003e\n\u003c/ul\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Ethics",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#ethics",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#ethics",
        "text": "\u003cp\u003eThe ethical and responsible use of AI is crucial for:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003emaintaining public trust\u003c/li\u003e\n  \u003cli\u003eprotecting individual rights\u003c/li\u003e\n  \u003cli\u003efostering equitable societal progress\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThis is covered in the white paper \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper\"\u003eA pro-innovation approach to AI regulation\u003c/a\u003e and its \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a-implementation-of-the-principles-by-regulators\"\u003e5 principles\u003c/a\u003e to guide AI development in all sectors.\u003c/p\u003e\u003cp\u003eThe ethical risks and opportunities presented by your use of AI will depend on your context and the nature of your solutions. The key themes you should address are:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003esafety, security and robustness\u003c/li\u003e\n  \u003cli\u003etransparency and explainability\u003c/li\u003e\n  \u003cli\u003efairness, bias and discrimination\u003c/li\u003e\n  \u003cli\u003eaccountability and responsibility\u003c/li\u003e\n  \u003cli\u003econtestability and redress\u003c/li\u003e\n  \u003cli\u003esocietal wellbeing and public good\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThe following sections explore these themes separately. However, there may be overlaps, with the impacts and outputs of one area introducing considerations in others. This means that, in some instances, the promotion of one ethical value may come at the cost of one or more other ethical values. For example, to promote fairness, you may need to collect demographic data to accurately assess the impact of a tool on different groups, which would have a detrimental impact on privacy. You must consider early on whether trade-offs are appropriate, if the benefits outweigh the risks, and that you’re avoiding any unacceptable risks.\u003c/p\u003e\u003cp\u003eWhere possible, you should make specific and robust measurements to assess AI systems. These may, for instance, evaluate the accuracy and quality of a model’s outputs against the protected characteristics listed in the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2010/15/contents\"\u003eEquality Act 2010\u003c/a\u003e. You should select the most appropriate assessments for your technology and use case, being aware that the state of the art is rapidly developing.\u003c/p\u003e\u003cp\u003eAs well as the guidance in this playbook, you can use the \u003ca href=\"https://www.gov.uk/government/publications/data-ethics-framework\"\u003eData Ethics Framework\u003c/a\u003e and the \u003ca rel=\"external\" href=\"https://uksa.statisticsauthority.gov.uk/wp-content/uploads/2022/03/2021_Self-assessment_guidance.pdf\"\u003eUKSA ethics self-assessment (PDF, 449KB)\u003c/a\u003e guidance.\u003c/p\u003e\u003ch3 id=\"safety-security-and-robustness\"\u003eSafety, security and robustness\u003c/h3\u003e\u003cp\u003eYou must build safe, secure and robust AI solutions. This means that your AI systems must be resilient, sustainable and function reliably, even in unforeseen situations or against adversarial attacks. \u003c/p\u003e\u003cp\u003eSafety, security and robustness are important because they promote privacy rights, reduce harm, and uphold all the other ethical principles across the life cycle of your AI system.\u003c/p\u003e\u003cp\u003eSafety refers to a system’s ability to operate without causing harm to people or the environment. This is particularly important in high-risk areas such as healthcare, policing and justice.\u003c/p\u003e\u003cp\u003eSecurity refers to the protection of data, assets and functionality against unauthorised access, misuse or damage.\u003c/p\u003e\u003cp\u003eRobustness refers to the ability of your algorithm or model to maintain its performance and stability under different conditions. A core component of robustness is reliability: to be considered reliable, an AI system should consistently perform as intended across all expected scenarios within the system’s life cycle. You should therefore establish accuracy measures to ascertain whether the system is producing correct outputs. If a reliable system encounters an unexpected event, it should adapt or respond in a consistent way, minimising harm and providing teams with suitable warnings to respond and rectify issues. \u003c/p\u003e\u003cp\u003eBuilding safe, robust and secure AI solutions includes elements relating to privacy. You must understand that considerations of privacy extend beyond the requirements set out in the \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/\"\u003eUK General Data Protection Regulation (UK GDPR)\u003c/a\u003e, the \u003ca href=\"https://www.gov.uk/data-protection\"\u003eData Protection Act 2018\u003c/a\u003e and other relevant legislation. For example, risks relating to group privacy, which consider the rights and interests of a group or collective rather than individuals, may include characteristics or behaviours that, if exposed, could lead to discrimination, stigmatisation or other forms of harm.\u003c/p\u003e\u003cp\u003eWider themes relating to safety, security and robustness are described in the Data protection and privacy and Security sections.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-5\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eEstablish performance metrics for AI systems that measure the accuracy of model outputs. \u003c/li\u003e\n    \u003cli\u003eTest your system in a variety of scenarios, including those capturing extreme or rare potential events.\u003c/li\u003e\n    \u003cli\u003eImplement red teaming processes and record how your model behaves when it encounters unexpected and anomalous scenarios.\u003c/li\u003e\n    \u003cli\u003eMake full use of the training resources available, including the \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw\"\u003ecourses on risks and ethics on Civil Service Learning\u003c/a\u003e.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"transparency-and-explainability\"\u003eTransparency and explainability\u003c/h3\u003e\u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper\"\u003eThe AI regulation white paper\u003c/a\u003e establishes that AI systems should be appropriately transparent and explainable. \u003c/p\u003e\u003cp\u003eTransparency is the communication of appropriate information about an AI system to the right people – for example, information on how, when and for which purposes an AI system is being used. A lack of transparency can lead to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eharmful outcomes\u003c/li\u003e\n  \u003cli\u003epublic distrust\u003c/li\u003e\n  \u003cli\u003ea lack of accountability and the ability to appeal\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou must consider transparency issues before deploying an AI system.\u003c/p\u003e\u003cp\u003eExplainability describes the ability to clarify how an AI system arrives at a given output or decision, such as explaining what factors lead to a loan application being granted or denied. Explainability may be impacted by the technologies used to build a system. You should consider this when designing your system.\u003c/p\u003e\u003cp\u003eEnsuring transparency and explainability can be challenging in the context of AI. Transparency can be limited by proprietary and ‘black box’ commercial tools, while explainability may not be possible for certain forms of machine learning, or may only be achievable at the cost of performance.\u003c/p\u003e\u003cp\u003eWhen checking the transparency of your AI systems, you should consider the:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003etechnical transparency: information about the technical operation of the AI system, such as the code used to create the algorithms and the underlying data sets used to train the model\u003c/li\u003e\n  \u003cli\u003eprocess transparency: information about the design, development and deployment decisions and practices behind your generative AI solutions, and the mechanisms used to demonstrate that the solution is responsible and trustworthy. Putting in place robust reporting mechanisms, process-centred governance frameworks and AI assurance techniques is essential for facilitating process-based transparency\u003c/li\u003e\n  \u003cli\u003eoutcome-based transparency and explainability: the ability to clarify to any user using or impacted by a service that utilises AI how the solution works and which factors influence its decision making and outputs, including individual-level explanations of decisions where this is requested\u003c/li\u003e\n  \u003cli\u003einternal transparency: retention of up-to-date internal records on technology and processes, and process-based transparency information\u003c/li\u003e\n  \u003cli\u003epublic transparency: communication about the department’s use of AI systems, made available to the general public in an open and accessible format\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eAll central government departments and certain arm’s length bodies that are in scope of the \u003ca href=\"https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub\"\u003eAlgorithmic Transparency Recording Standard (ATRS)\u003c/a\u003e must use it to ensure transparency around the algorithmic tools used in decision-making processes by public bodies. We also recommend ATRS for use by other public sector bodies, although they are not required to use it yet. You can refer to additional standards and external resources. These include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ethe UK’s national public sector AI ethics and safety guidance, \u003ca rel=\"external\" href=\"https://zenodo.org/record/3240529#.XSCMBy2ZMXr\"\u003eunderstanding AI ethics and safety\u003c/a\u003e, which outlines a process-based governance framework that can help project teams establish and document proportionate governance actions\u003c/li\u003e\n  \u003cli\u003edata and model cards or fact sheets, which can be used as a reference point when documenting information about AI models and the data sets used in training and testing. A good example of these are Google’s \u003ca rel=\"external\" href=\"https://github.com/PAIR-code/datacardsplaybook/tree/main/templates\"\u003edata cards\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://arxiv.org/abs/1810.03993\"\u003emodel cards\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003ethe \u003ca rel=\"external\" href=\"https://ico.org.uk/\"\u003eInformation Commissioner’s Office (ICO)\u003c/a\u003e, which also offers AI auditing consultation and support to government organisations. Refer to \u003ca rel=\"external\" href=\"https://ico.org.uk/media/for-organisations/documents/4022651/a-guide-to-ai-audits.pdf\"\u003eA guide to ICO Audit: Artificial Intelligence (AI) Audits (PDF, 156KB)\u003c/a\u003e for more information\u003c/li\u003e\n  \u003cli\u003e\n\u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence/\"\u003eExplaining decisions made with AI guidance\u003c/a\u003e, which is the UK’s national AI explainability guidance co-produced by \u003ca rel=\"external\" href=\"https://www.turing.ac.uk/\"\u003eThe Alan Turing Institute\u003c/a\u003e and the ICO. This details 6 types of explanations as well as documentation processes\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-6\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eUse existing standards and recording mechanisms such as the \u003ca href=\"https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub\"\u003eATRS\u003c/a\u003e to communicate information about AI solutions to the general public. ATRS is particularly relevant if your tool is deployed, directly or indirectly, in decision-making processes.\u003c/li\u003e\n    \u003cli\u003eClearly signpost when AI has been used to create content, is interacting with members of the public, or is used in decision-making processes that impact members of the public. \u003c/li\u003e\n    \u003cli\u003ePut in place evaluation and auditing structures, tracking data provenance, design decisions, training scenarios and processes.\u003c/li\u003e\n    \u003cli\u003eImplement transparency and auditing requirements for suppliers.\u003c/li\u003e\n    \u003cli\u003eUse external resources and emerging best practice, such as data cards and model cards for internal transparency.\u003c/li\u003e\n    \u003cli\u003eMake model outputs as explainable as possible, and avoid using less explainable AI methods in areas where explainability is essential.\u003c/li\u003e\n    \u003cli\u003eConsider the use of open source models, which provide more transparency about data sets, code and training processes.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"fairness-bias-and-discrimination\"\u003eFairness, bias and discrimination \u003c/h3\u003e\u003cp\u003eThe white paper \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper\"\u003eA pro-innovation approach to AI regulation\u003c/a\u003e sets out that AI systems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes. \u003c/p\u003e\u003cp\u003eYou must ensure fairness in the development and use of AI solutions to comply with legal and human rights requirements, including consumer and competition law, public and common law, and rules protecting vulnerable people. In the context of AI, fairness has many facets. It means ensuring that a system’s outputs are unprejudiced and do not amplify existing social, demographic or cultural disparities. \u003c/p\u003e\u003cp\u003eThis includes ensuring that:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eAI systems fairly allocate resources or services to all people, across all protected characteristics\u003c/li\u003e\n  \u003cli\u003ecertain subgroups are not disproportionately adversely impacted or harmed – for example, in employment-related decisions\u003c/li\u003e\n  \u003cli\u003ein the context of generative AI systems, different groups are not, for example, represented in harmful, prejudiced or offensive ways\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou should understand that AI systems are designed, developed and deployed by human beings who are bound by the limitations of their contexts and biases. AI models are also trained on data which encodes present and past biases and inequalities of society. These can be present across the AI life cycle. Be aware that generative AI models are particularly vulnerable to bias because they’re trained on vast amounts of unfiltered data scraped from the internet, which is likely to contain a wide range of content reflecting historical and social biases. The wording of prompts may also inadvertently introduce bias.\u003c/p\u003e\u003cp\u003eA well-documented form of algorithmic bias is representational bias, which describes instances in which people are underrepresented, overrepresented or misrepresented in data sets used as training data. This form of biased input data can lead to the generation of harmful stereotypes or abusive content targeted at people from specific genders, sexual orientations and identities, ethnicities, countries of origin, or religions. \u003c/p\u003e\u003cp\u003eAnother form of harm that can result from representational bias in the input data is performance disparities across different social groups. For example, a given system may perform more poorly on certain underrepresented dialects or skin colours. Other systems, such as AI applications that support healthcare, may provide beneficial results primarily to privileged members of society who are more heavily represented in the data.\u003c/p\u003e\u003cp\u003eBias issues may be compounded when protected characteristics are considered in combination. The opacity and complexity of some AI systems can make it difficult to identify exactly where and how biases are introduced. Issues of fairness and bias may manifest when an AI system is implemented and used, even if issues were checked during system testing and validation. This may be due to a variety of reasons – from unexpected interactions by the users with software and hardware, to cultural or personal needs and specificities that were not captured earlier in the development process. For example, users may choose to ignore or only selectively pay attention to recommendations provided by an AI system, which may introduce new forms of bias.\u003c/p\u003e\u003cp\u003eIt’s also important to consider \u003ca href=\"https://www.gov.uk/service-manual/helping-people-to-use-your-service/making-your-service-accessible-an-introduction\"\u003eaccessibility\u003c/a\u003e, ensuring that equal benefits can be achieved by all and considering interactions with assistive technologies. You can find more on \u003ca href=\"https://www.gov.uk/service-manual/helping-people-to-use-your-service/testing-for-accessibility\"\u003edesigning and testing for accessibility\u003c/a\u003e in the government \u003ca href=\"https://www.gov.uk/service-manual\"\u003eService Manual\u003c/a\u003e.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-7\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eImplement bias mitigation and fairness evaluation across the entire AI project life cycle.\u003c/li\u003e\n    \u003cli\u003eComply with human rights law, the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2010/15/contents\"\u003eEquality Act 2010\u003c/a\u003e, the \u003ca href=\"https://www.gov.uk/government/publications/public-sector-equality-duty\"\u003ePublic Sector Equality Duty\u003c/a\u003e and the \u003ca href=\"https://www.gov.uk/government/organisations/equality-and-human-rights-commission\"\u003eEquality and Human Rights Commission\u003c/a\u003e guide to using AI in public services.\u003c/li\u003e\n    \u003cli\u003eReview model outputs and decisions for bias, measuring performance and decisions for different groups of people including intersectional groups.\u003c/li\u003e\n    \u003cli\u003ePut feedback mechanisms in place to allow individuals to report unfair decisions, harmful outputs and accessibility challenges.\u003c/li\u003e\n    \u003cli\u003eBeware of instances in which the use of AI can make legal obligations to the Equality Act 2010 more difficult to uphold – for example, difficulties with making links between abstract algorithmic groupings and protected groups.\u003c/li\u003e\n    \u003cli\u003eAdopt an approach of continuous evaluation to keep pace with changing fairness considerations and societal expectations.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"accountability-and-responsibility\"\u003eAccountability and responsibility \u003c/h3\u003e\u003cp\u003eEnsuring accountability and responsibility in the context of AI means that individuals and organisations can be held responsible for the effects that the AI systems they develop, deploy or use, have on people and society.\u003c/p\u003e\u003cp\u003eYou must think about this at the start of your project to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eencourage mindful creation and usage of AI systems\u003c/li\u003e\n  \u003cli\u003eensure that people who design and deploy AI systems can be held accountable for their outputs and impacts\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eAccountability mechanisms will help you address and remediate issues when an error occurs. This involves establishing which parties are responsible at each stage of the system’s life cycle. Governance mechanisms may help to establish clear guidelines and structures for the development and deployment of AI systems. Refer to the Governance section for more information.\u003c/p\u003e\u003cp\u003eTo establish accountable practices across the AI life cycle, you should consider 3 key elements: answerability, auditability and liability.\u003c/p\u003e\u003ch4 id=\"answerability\"\u003eAnswerability\u003c/h4\u003e\u003cp\u003eYou should establish a chain of human responsibility across the AI project life cycle, including responsibility throughout the supply chain. In cases of harm or errors caused by AI, you need to establish recourse and feedback mechanisms for affected individuals. \u003c/p\u003e\u003cp\u003eIdentifying the specific actors involved in AI systems is vital to answerability. This includes model developers, application developers, policymakers, regulators, system operators and end-users. In each case, you must define their roles and responsibilities, and align these with legal and ethical standards.\u003c/p\u003e\u003ch4 id=\"auditability\"\u003eAuditability\u003c/h4\u003e\u003cp\u003eYou should demonstrate the responsibility and trustworthiness of your development and deployment practices by:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eupholding robust reporting and documentation protocols\u003c/li\u003e\n  \u003cli\u003eretaining traceability throughout the AI project’s life cycle\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eAuditability is the process of documenting every stage of the AI innovation life cycle – from data collection and base model training to implementation, system deployment, updating and retirement – in a way that’s accessible to relevant stakeholders and easily understood.\u003c/p\u003e\u003ch4 id=\"liability\"\u003eLiability\u003c/h4\u003e\u003cp\u003eYou should make sure that all parties involved in the AI project life cycle, from vendors and technical teams to system users, are acting lawfully and understand their respective legal obligations.\u003c/p\u003e\u003cp\u003eAs an end-user, being accountable means that you take responsibility for a system’s outputs and its potential consequences. You should check that outputs are accurate, non-discriminatory, non-harmful, and do not violate existing legal provisions, guidelines, policies or the provider’s terms of use. \u003c/p\u003e\u003cp\u003eYou must also put the necessary oversight and human-in-the-loop processes in place to validate output in situations with high impact or risk. Where these risks are too high, you must reconsider if AI should be used at all. Refer to the Identifying use cases for AI section for more on this.\u003c/p\u003e\u003cp\u003eUltimately, responsibility for any output or decision made or supported by an AI system always rests with the public organisation. Where AI is bought commercially, ensure that vendors understand their responsibilities and liabilities, put the required risk mitigations in place and share all relevant information.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-8\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eFollow existing legal provisions, guidelines and policies as well as the provider’s terms of use when developing, deploying or using AI.\u003c/li\u003e\n    \u003cli\u003eClearly define responsibilities, accountability and liability across all actors involved in the AI life cycle. Where AI is bought commercially, define detailed responsibilities and liability contractually.\u003c/li\u003e\n    \u003cli\u003eNominate a senior responsible owner (SRO) who will be accountable for the use of AI in a specific project.\u003c/li\u003e\n    \u003cli\u003eWhere AI is used in situations of high impact or risk, establish a human-in-the-loop process to oversee and validate outputs and decisions. Make sure that these people can effectively identify risks and intervene, where appropriate.\u003c/li\u003e\n    \u003cli\u003eAs an end-user, assume responsibility for the outputs and decisions made by the AI systems you use. \u003c/li\u003e\n    \u003cli\u003eAdopt a risk-based approach to the use of AI and consider whether it’s appropriate to use AI in high-risk applications.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eUse assurance techniques to evaluate the performance of AI systems. The \u003ca href=\"https://www.gov.uk/government/publications/introduction-to-ai-assurance/introduction-to-ai-assurance#:~:text=2.1%20Introduction,the%20wider%20AI%20governance%20landscape.\"\u003eIntroduction to AI assurance\u003c/a\u003e provides a useful starting point, and the \u003ca href=\"https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques\"\u003ePortfolio of AI assurance techniques\u003c/a\u003e offers real-world examples.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"contestability-and-redress\"\u003eContestability and redress\u003c/h3\u003e\u003cp\u003eThe principles of contestability and redress refer to the mechanisms through which AI systems and their outputs or decisions can be challenged, and how impacted individuals can seek remedy.\u003c/p\u003e\u003cp\u003eContestability and redress are important because they help identify and correct ethical issues in AI systems after deployment. You must design appropriate mechanisms before deployment, and continue to maintain them throughout the full life cycle of your AI system.\u003c/p\u003e\u003cp\u003eThese principles are interlinked with transparency and explainability requirements, as public awareness of the use of algorithms and effective explainability of AI systems are essential for questioning or challenging their use or outputs. \u003c/p\u003e\u003cp\u003eYou should make people aware when an AI system is used, and clearly signpost mechanisms for contestability and redress to impacted individuals. These include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003epublic awareness: to enable users to contest and seek redress about your AI system, you must ensure that they are aware of the presence of the AI system and the function that it plays in the services that they’re interacting with. This includes making users aware of mechanisms for contestability and redress clearly and in a timely fashion\u003c/li\u003e\n  \u003cli\u003emechanisms for appeal: you must establish and promote clear and accessible mechanisms for people to challenge the decisions made by AI systems, and ask wider questions concerning the training, deployment and impacts of AI systems employed by the UK government\u003c/li\u003e\n  \u003cli\u003echange processes: you must ensure that mechanisms are in place to investigate any areas highlighted by users, and make changes to or decommission AI systems if unacceptable risks or harms are identified\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThough contestability and redress mechanisms offer a route to mitigate harms, by themselves they do not sufficiently ensure the responsible use of these technologies, as harms may not be apparent to all who are impacted. Adhering to the principles discussed in the Using AI safely and responsibly section will help you identify risks before harms occur.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-9\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003ePut mechanisms in place for users or impacted individuals to report instances of potential risk or harm relating to AI systems.\u003c/li\u003e\n    \u003cli\u003eNominate one or more SROs to be responsible for monitoring reports and implementing adequate changes throughout the AI system’s life cycle.\u003c/li\u003e\n    \u003cli\u003eEnsure that routes for reporting risks or harms are clearly disclosed, and signposted to users at the point where they are, directly or indirectly, interacting with or impacted by an AI system.\u003c/li\u003e\n    \u003cli\u003eWhere possible, provide impacted individuals with the option to contact the responsible team directly.\u003c/li\u003e\n    \u003cli\u003eFor each project using AI, ensure you allocate adequate resources to respond to messages received through public communication channels and make changes where necessary across the life cycle of the system.\u003c/li\u003e\n    \u003cli\u003eCreate contingency plans to maintain essential services in the event that an unacceptable risk is identified and the use of an AI system needs to be temporarily or permanently stopped.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"societal-wellbeing-and-public-good\"\u003eSocietal wellbeing and public good\u003c/h3\u003e\u003cp\u003eSocietal wellbeing in the context of AI means not only ensuring that AI is developed in a way that minimises and mitigates harms, but also actively promoting ethical applications of AI that solve societal challenges and deliver good for society. \u003c/p\u003e\u003cp\u003eAI may be perceived as an impersonal, distant, or even alienating technology. Government engagement with academia, industry and especially the broader civil society is crucial to dispel fears and foster understanding of the potential benefits of AI technologies. \u003c/p\u003e\u003cp\u003eThe \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper\"\u003eAI regulation white paper\u003c/a\u003e and the \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-ai-opportunities-action-plan-terms-of-reference/artificial-intelligence-ai-opportunities-action-plan-terms-of-reference\"\u003eAI opportunities action plan\u003c/a\u003e highlight the potential for AI to deliver tangible benefits to members of the public and the economy. The applications of AI in government and the potential public benefits that we can achieve using AI are wide-ranging. AI technologies can be used to improve productivity and access to services; advance the effectiveness of health interventions and diagnoses; create and deliver more personalised training; promote sustainability (by supporting the work of conservationists, for example); and even reduce existing inequalities, for example by increasing access to health services for at-risk groups. By engaging in dialogue with civil society and acting as a leading, responsible user of AI, government can help the people of the UK better embrace AI’s potential for positive change.\u003c/p\u003e\u003cp\u003eHowever, AI systems and applications may have two-sided impacts. If not used in responsible ways and with appropriate safeguards in place, AI tools may risk generating harm, entrenching inequalities or undermining democratic processes. When considering the use of an AI system, you should identify and weigh both the potential positive impacts of new technologies as well as any negative impacts or unintended consequences. \u003c/p\u003e\u003cp\u003eYou must ensure that AI systems generate a net positive impact on stakeholders and society at large, while minimising potential harms as much as possible. If the potential negative consequences are too high, you must consider terminating the project.\u003c/p\u003e\u003cp\u003eSome of the elements you should consider when assessing the potential impact of AI tools are:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ejustified trust: this is essential to promote the uptake and long-term adoption of technology. AI can promote justified public trust in the government as a whole if the public understands and recognises that the AI solution has been developed competently, responsibly and ethically\u003c/li\u003e\n  \u003cli\u003epublic benefit: you must ensure that the AI solutions you develop and/or use represent good value for money and benefit the public. This aligns with the UK government’s ambitions to use AI to help solve societal and global challenges – as long as the AI solution is safe, lawful and compatible with other ethical principles\u003c/li\u003e\n  \u003cli\u003eharm minimisation: illustrating public benefit is an important principle when evaluating the ethics of an AI project, but it’s equally important to minimise harms\u003c/li\u003e\n  \u003cli\u003emisinformation and disinformation: both unintentionally and intentionally, AI systems can be used to generate factually incorrect information. Some AI systems may facilitate the spread of misinformation or disinformation through their ability to generate plausible but false content, or by promoting this content online with recommender systems. This risk is particularly prevalent in generative AI systems, which are typically designed to create statistically likely language patterns rather than reliable accounts of reality. The ability of large language models (LLMs) to create text that appears credible and convincing enhances the potential for false or misleading information to be believed\u003c/li\u003e\n  \u003cli\u003esustainability: AI can potentially provide a public benefit by helping meet sustainability goals. However, it can also present risks relating to energy and resource consumption derived from both the training and deployment of some AI technologies. If you plan to use generative AI tools, consider that it’s usually less environmentally sound to train your own model if appropriate pre-trained models are available. Generative AI can be expensive to operate, so it should not be used for tasks that could be undertaken by other available technologies\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eIn addition to the points above, you can consider the \u003ca rel=\"external\" href=\"https://uksa.statisticsauthority.gov.uk/publication/considering-public-good-in-research-and-statistics-ethics-guidance/\"\u003eethics guidance from the Office for National Statistics\u003c/a\u003e to understand and articulate the potential public goods of your project.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-10\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eEngage with broader society – including civil society groups, underrepresented individuals, those most likely to experience harm, NGOs, academia, and more – when defining and deploying AI systems. Understanding their values, needs and priorities will help ensure your AI products deliver tangible benefits to society.\u003c/li\u003e\n    \u003cli\u003eWeigh any positive impacts of using an AI system against potential negative ones.\u003c/li\u003e\n    \u003cli\u003eVerify the information generated by AI systems to ensure it’s accurate.\u003c/li\u003e\n    \u003cli\u003eAssess the potential risks of your AI system being used to generate or spread misinformation or disinformation.\u003c/li\u003e\n    \u003cli\u003eAssess the environmental impact of training and/or deploying your AI system before commencing development. Consider whether the impact represents a reasonable trade-off between benefits and energy consumption, and whether a less energy-intensive system might be able to achieve the same or similar results. Also consider any actions you can take to \u003ca href=\"https://www.gov.uk/guidance/make-your-technology-sustainable\"\u003emake technology sustainable\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eEvaluate the environmental credentials of potential model providers and wider partner organisations – including their use of renewable energy, energy-efficient infrastructure and sustainable practices – and select low carbon emission energy grids.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Legal considerations ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#legal-considerations",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#legal-considerations",
        "text": "\u003cp\u003eDifferent types of AI and use cases will likely create different types of legal issues. You’re not alone and should seek advice from government legal advisers who can help you navigate the design and use of AI in government.\u003c/p\u003e\u003cp\u003eMany of the legal issues that surround AI are not new. For example, the ethical principles discussed in this playbook, such as fairness, discrimination, transparency and bias, have sound foundations in public and other law. The ethical issues that your team identifies are also likely to be legal issues that your lawyers will be able to help guide you through.\u003c/p\u003e\u003cp\u003eThe Lawfulness and purpose limitation section explains how to ensure that personal data is processed lawfully, securely and fairly at all times. Your lawyers can advise you on that.\u003c/p\u003e\u003cp\u003eYou may face procurement and commercial issues when buying AI products. Alongside commercial colleagues, your lawyers can help you navigate those challenges.\u003c/p\u003e\u003cp\u003eWhen you contact your legal team, you should explain your aims for the AI solution, what it will be capable of doing, and any potential risks you’re aware of. This will help you to understand, for example, if you need legislation to achieve what you want to do.\u003c/p\u003e\u003cp\u003eIt will also help to minimise the risk of your work:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ebeing challenged in court\u003c/li\u003e\n  \u003cli\u003ehaving unintended and/or unethical consequences\u003c/li\u003e\n  \u003cli\u003ehaving a negative impact on the people you want it to benefit\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"example-legal-issues\"\u003eExample legal issues\u003c/h3\u003e\u003cp\u003eThese are designed to help you understand when you might want to consider getting legal advice. They should not be read as real legal advice and their application to any given scenario will depend on the specific facts. You should always consult your organisation’s lawyer.\u003c/p\u003e\u003ch3 id=\"data-protection\"\u003eData protection\u003c/h3\u003e\u003cp\u003eData protection is a legal issue, with potentially serious consequences if the government gets it wrong. \u003c/p\u003e\u003cp\u003eAlthough your organisation will likely have a data protection officer, and there may also be data protection experts in your team, your legal team can help you unpick some of the difficult data protection issues that are created by AI.\u003c/p\u003e\u003cp\u003eRefer to the Data protection and privacy section for more information.\u003c/p\u003e\u003ch3 id=\"contractual-issues\"\u003eContractual issues\u003c/h3\u003e\u003cp\u003eYour lawyers will help you draw up the contracts and other agreements for the procurement or licensing of AI tools. There may be special considerations for these contracts. For example, how to: \u003c/p\u003e\u003cul\u003e\n  \u003cli\u003edeal with intellectual property\u003c/li\u003e\n  \u003cli\u003eensure the level of transparency needed to help buyers understand their systems\u003c/li\u003e\n  \u003cli\u003etransfer a project to new or successor suppliers \u003c/li\u003e\n  \u003cli\u003eassist with the defence against any legal challenge\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eContracts for technology services may need to incorporate procedures for system errors and outages that recognise the potential consequences of performance failures. \u003c/p\u003e\u003cp\u003eIt’s important that you consider appropriate contractual terms early on because this may, in part, drive decisions on the appropriate route to market. Refer to the Buying AI section for more information.\u003c/p\u003e\u003ch3 id=\"intellectual-property-including-copyright\"\u003eIntellectual property, including copyright\u003c/h3\u003e\u003cp\u003eThe potential intellectual property issues with AI have been much discussed. Your lawyers can help you navigate these. \u003c/p\u003e\u003cp\u003eFor example, you should consider at the outset: \u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ewhich parties will own which parts of any intellectual property generated during the project\u003c/li\u003e\n  \u003cli\u003ewhich parties will have ongoing rights to use any intellectual property that is generated (and on what basis)\u003c/li\u003e\n  \u003cli\u003ehow the balance of risk and liability should be determined between the parties, as this will be relevant to any claims for infringement of third party intellectual property\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"equality-issues\"\u003eEquality issues\u003c/h3\u003e\u003cp\u003eLawyers can help you navigate the equality issues raised by the use of AI in government – for example, obligations arising under the Equality Act 2010 and the Public Sector Equality Duty. Conducting an assessment of the equality impacts of your use of AI can also be one way to guard against bias, which is particularly important in the context of AI.\u003c/p\u003e\u003cp\u003eIf approached early, before contracts are signed, your legal advisers can help you ensure the government is fulfilling its responsibilities to the public to assess the impacts of the technology it’s using.\u003c/p\u003e\u003ch3 id=\"public-law-principles\"\u003ePublic law principles\u003c/h3\u003e\u003cp\u003ePublic law principles explain how public bodies should act rationally, fairly, lawfully and in compatibility with human rights. These are guidelines for public bodies on how to act within the law. \u003c/p\u003e\u003cp\u003eMany of these public law principles overlap with the ethical principles set out in this guidance. As a result, your lawyers will likely be able to guide you on how to apply the ethical principles based on their knowledge of public law, the court cases that have occurred and the detail of the judgments.\u003c/p\u003e\u003cp\u003eFor example, public law involves a principle of procedural fairness. This is not so much about the decision that is eventually reached but about how a decision is arrived at. The transparency and explainability of the AI tool may well be key in being able to demonstrate that the procedure was fair. Similarly, an inability to determine how AI tools have arrived at their decisions or outputs may introduce risk into the decision-making process. \u003c/p\u003e\u003cp\u003ePublic law also considers rationality. Rationality may be relevant in testing the choice of an AI system, considering the features used in a system, and considering the outcomes of the system and the metrics used to test those outcomes. \u003c/p\u003e\u003cp\u003eIf you’re considering using AI in decision making, public law can also guide you. For example, it can help you determine whether a particular decision should be delegated to a decision maker, rather than letting an AI tool make an automated decision. When operating in a regulated environment, such as a procurement process, automated decision making or assessments could be subject to legal challenge if procedural fairness, lack of bias and rationality cannot be evidenced. \u003c/p\u003e\u003ch3 id=\"human-rights\"\u003eHuman rights\u003c/h3\u003e\u003cp\u003ePublic authorities must act in a way that is compatible with human rights. It’s possible that AI systems (especially those involving the use of personal data) may in some way affect at least one of an individual’s rights, as set out in the \u003ca rel=\"external\" href=\"https://www.coe.int/en/web/human-rights-convention\"\u003eEuropean Convention on Human Rights (ECHR)\u003c/a\u003e. Examples of the rights most likely to be impacted are Article 8 (right to a private and family life) and Article 10 (freedom of expression).\u003c/p\u003e\u003ch3 id=\"legislation\"\u003eLegislation\u003c/h3\u003e\u003cp\u003eSometimes, in order to do something, a public authority needs a legislative framework. Your lawyers will be able to advise you whether your use of AI is within the current legal framework or needs new legislation.\u003c/p\u003e\u003cp\u003eFor example, the legislative framework might not allow the process you’re automating to be delegated to a machine, or it might provide for a decision to be made by a particular person.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-11\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eEnsure you engage legal professionals at the outset of your AI project. They can help you navigate legal complexities and identify potential legal risks associated with data protection, contractual agreements, intellectual property, equality issues, and compliance with public law principles.\u003c/li\u003e\n    \u003cli\u003eGiven the potential consequences of mishandling personal data, collaborate with legal experts to ensure you comply with data protection regulations and understand how to mitigate risks associated with data privacy and security.\u003c/li\u003e\n    \u003cli\u003eWork with legal experts to develop robust contracts and agreements for procuring or licensing AI tools, considering issues such as intellectual property rights, transparency levels, liability distribution, and procedures for addressing system errors or failures.\u003c/li\u003e\n    \u003cli\u003eSeek legal advice to determine whether your AI project aligns with existing legislative frameworks or requires new legislation. Understanding legislative constraints helps mitigate the risk of legal challenges and ensures you comply with legislative requirements.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Data protection and privacy",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#data-protection-and-privacy",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#data-protection-and-privacy",
        "text": "\u003cp\u003eAI-driven technologies offer significant benefits but they also pose potential risk of harm to individuals and groups if they’re not implemented with specific focus on protecting individuals’ \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/data-protection-fee/legal-definitions-fees/#data\"\u003epersonal data\u003c/a\u003e and right to privacy. \u003c/p\u003e\u003cp\u003eBe aware that organisations developing and deploying AI systems must consider the principles of data protection outlined in the \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/\"\u003eUK General Data Protection Regulation (UK GDPR)\u003c/a\u003e and the \u003ca href=\"https://www.gov.uk/data-protection\"\u003eData Protection Act 2018\u003c/a\u003e, and minimise the risk of privacy intrusion from the outset.\u003c/p\u003e\u003cp\u003eThe UK data protection law applies irrespective of the type of technology used, so its basic principles of compliance will also apply to any AI system. The data protection principles most relevant to the use of AI are:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eaccountability: your organisation has clear ownership of risk and responsibility for mitigations and compliance\u003c/li\u003e\n  \u003cli\u003elawfulness: you have an applicable lawful basis for processing personal data and ensure the processing is lawful under data protection or any other regulation\u003c/li\u003e\n  \u003cli\u003epurpose limitation: you define why you’re processing personal data and only process data for that purpose\u003c/li\u003e\n  \u003cli\u003etransparency and individual rights: you’re open about what it uses personal data for, and your users can exercise their information rights\u003c/li\u003e\n  \u003cli\u003efairness: you avoid processing personal data in ways that are detrimental, unexpected or misleading\u003c/li\u003e\n  \u003cli\u003edata minimisation: you develop systems that process only the data that is needed for the task at hand\u003c/li\u003e\n  \u003cli\u003estorage limitation: you don’t accumulate large amounts of personal data for unjustifiably long periods\u003c/li\u003e\n  \u003cli\u003ehuman oversight: you build in human oversight to automated decision making\u003c/li\u003e\n  \u003cli\u003eaccuracy: you have steps in place to ensure the accuracy of AI-generated responses and data related to individuals\u003c/li\u003e\n  \u003cli\u003esecurity: you implement appropriate technical and organisational mitigations to protect sensitive and personal data\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eData protection and privacy considerations require specialist expertise, so it’s crucial to involve relevant data protection, legal and other information governance professionals in AI projects from the outset to follow data protection by design principles.\u003c/p\u003e\u003ch3 id=\"accountability\"\u003eAccountability \u003c/h3\u003e\u003cp\u003eAccountability is a key principle that establishes ownership of risk, responsibility for mitigations, compliance with legislation, the ability to demonstrate compliance, and high standards for privacy.\u003c/p\u003e\u003cp\u003eOrganisations should take the following steps when planning AI solutions:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003emake a strategic decision on how any use of AI technology fits with your existing risk tolerance\u003c/li\u003e\n  \u003cli\u003ereview your risk governance model to establish clear ownership of AI risks at a senior level\u003c/li\u003e\n  \u003cli\u003eimplement measures to mitigate these risks and test their effectiveness\u003c/li\u003e\n  \u003cli\u003emake sure you identify residual risks and align them with your organisation’s risk threshold\u003c/li\u003e\n  \u003cli\u003ebe collaborative, work transparently and demonstrate how you mitigate risks\u003c/li\u003e\n  \u003cli\u003edue to the evolving nature of AI technologies and new regulations, ensure you conduct regular reviews, with a view to making further iterations\u003c/li\u003e\n  \u003cli\u003eengage with internal data protection, privacy and legal experts from the outset\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e\u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-by-design-and-default/#:~:text=Data%20protection%20by%20design%20is%20ultimately%20an%20approach%20that%20ensures,and%20then%20throughout%20the%20lifecycle.\"\u003eData protection by design\u003c/a\u003e is an important component of the UK GDPR risk-based approach. It requires you to integrate data protection safeguards into personal data processing activities throughout the AI product life cycle. \u003c/p\u003e\u003cp\u003eThis will ensure that you implement appropriate technical and organisational measures to protect \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/data-protection-fee/legal-definitions-fees/#:~:text=compliance%20with%20them.-,Data%20subject,to%20whom%20personal%20data%20relates\"\u003edata subject\u003c/a\u003e rights, and comply with the \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-protection-principles/a-guide-to-the-data-protection-principles/\"\u003edata protection principles\u003c/a\u003e defined in the UK GDPR and Data Protection Act 2018.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-12\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eEstablish ownership of AI risks at a senior level.\u003c/li\u003e\n    \u003cli\u003eIntegrate oversight of AI into your governance processes.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eTake a risk-based approach, defining risk appetite and following \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-by-design-and-default/\"\u003eprinciples of data protection by design and by default\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"lawfulness-and-purpose-limitation\"\u003eLawfulness and purpose limitation\u003c/h3\u003e\u003cp\u003eBefore implementing AI solutions, you need to undertake a \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/guide-to-accountability-and-governance/accountability-and-governance/data-protection-impact-assessments/\"\u003edata protection impact assessment (DPIA)\u003c/a\u003e. This involves an assessment of data protection and privacy risks, and the implementation of appropriate technical and organisational measures to sufficiently mitigate them.\u003c/p\u003e\u003cp\u003eArticle 35(3)(a) of the UK GDPR requires you to undertake a DPIA if your use of AI involves any of the following:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003esystematic and extensive evaluation of personal data aspects based on automated processing, including profiling, on which decisions are made that produce legal or similarly significant effects\u003c/li\u003e\n  \u003cli\u003elarge-scale processing of special categories of personal data\u003c/li\u003e\n  \u003cli\u003esystematic monitoring of publicly accessible areas on a large scale\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThe \u003ca href=\"https://www.gov.uk/government/organisations/information-commissioner-s-office\"\u003eInformation Commissioner’s Office (ICO)\u003c/a\u003e also requires a DPIA if your processing of personal data involves the use of innovative technologies. In your DPIA, you should take all of the actions below.\u003c/p\u003e\u003col\u003e\n  \u003cli\u003eDescribe the purpose of personal data processing activities.\u003c/li\u003e\n  \u003cli\u003eAssess the necessity and proportionality of personal data processing.\u003c/li\u003e\n  \u003cli\u003eIdentify all \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/personal-information-what-is-it/what-is-personal-information-a-guide/\"\u003epersonal data\u003c/a\u003e, including \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/special-category-data/#scd1\"\u003especial category data\u003c/a\u003e, that is being processed, including sources and flows of data.\u003c/li\u003e\n  \u003cli\u003eIdentify the \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/\"\u003evalid lawful basis\u003c/a\u003e under Article 6 (and any \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/special-category-data/#scd3\"\u003eadditional special conditions\u003c/a\u003e under Article 9 for special category data) of the UK GDPR.\u003c/li\u003e\n  \u003cli\u003eIdentify your organisation’s role and obligations as a data controller and whether any data processors are involved.\u003c/li\u003e\n  \u003cli\u003eIdentify the stages when AI processes and automated decisions may have an impact on individuals.\u003c/li\u003e\n  \u003cli\u003eSeek and document the views of individuals whose personal data is being processed. This includes finding out whether data subjects are aware that this processing is taking place.\u003c/li\u003e\n  \u003cli\u003eIdentify the stages when any human is involved in the decision-making process.\u003c/li\u003e\n  \u003cli\u003eConsider any potential detriment to individuals due to bias or inaccuracy. \u003c/li\u003e\n  \u003cli\u003e Document measures and safeguards put in place, and any residual levels of risk posed by the processing.\u003c/li\u003e\n\u003c/ol\u003e\u003cp\u003eThe purpose for which data is collected and used has a significant effect on whether individuals perceive it as being invasive to privacy. A clear and well-defined articulation of the purpose from the outset will guide your deliberations about an applicable, lawful basis and the minimum personal data that is absolutely necessary to deploy the AI service. \u003c/p\u003e\u003cp\u003eAI systems often reuse personal data for new purposes that are different from those for which it was originally collected. This may cause tension with the purpose limitation of the UK GDPR. Repurposing of personal data is only legitimate if a new purpose is ‘compatible’ with the purpose for which the data was originally collected. \u003c/p\u003e\u003cp\u003eYou should consider the following criteria when repurposing personal data:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ewhether the new purpose aligns with the data subjects’ expectations\u003c/li\u003e\n  \u003cli\u003ewhat type of personal data is involved\u003c/li\u003e\n  \u003cli\u003ewhat potential impact it will have on data subjects’ interests\u003c/li\u003e\n  \u003cli\u003ewhether the data controller will need to adopt additional safeguards to ensure fairness and transparency\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThe DPIA process should identify personal data processing at each stage of the AI life cycle – from design to data acquisition and preparation, training, testing, deployment and monitoring. Although it’s common to characterise AI with large volumes of data, AI systems are able to directly perceive and evaluate their environment, and adapt to the data received. You should not underestimate AI’s interactive qualities, such as its ability to collect new data in real time from touchscreens and audiovisual inputs, and adapt its responses and subsequent functions based on these inputs.\u003c/p\u003e\u003cp\u003eWhen mapping personal data flows, you should identify the geographic location of each distinct processing activity because the processing of data outside the United Kingdom will increase the risk of losing the protection of UK data protection laws. Data controllers may need to bring in additional safeguards, such as \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/international-transfers/international-data-transfer-agreement-and-guidance/\"\u003einternational data transfer agreements\u003c/a\u003e if personal data is being processed in jurisdictions where the data protection regime is not deemed to be adequate and transfers of personal data are restricted under Article 46 of the UK GDPR.\u003c/p\u003e\u003cp\u003eIf your assessment indicates that there’s a high risk to the data protection rights of individuals, and that you’re unable to sufficiently reduce these risks despite mitigating actions, you must consult the ICO before you can start processing personal data.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-13\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eWhen building your team, seek support from data compliance professionals – including data protection, legal and privacy experts.\u003c/li\u003e\n    \u003cli\u003eIdentify data processing operations and their purpose, and map personal data sources and flows.\u003c/li\u003e\n    \u003cli\u003eDetermine whether personal data is necessary for each activity, and whether you’re processing special category data or children’s data.\u003c/li\u003e\n    \u003cli\u003eIdentify the applicable lawful basis of your data processing and assess data protection and privacy risk through \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/\"\u003eDPIAs\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/legitimate-interests/\"\u003elegitimate interest assessments\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eIf data protection and privacy risks remain high even after mitigations, \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/do-we-need-to-consult-the-ico/\"\u003econsult with the ICO\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eIdentify any processing outside the UK to take additional safeguards to protect personal data in jurisdictions where the data protection regime may not be adequate. \u003c/li\u003e\n    \u003cli\u003eAssess any changes in the purpose of your AI system and make sure your AI system remains compliant and lawful.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"transparency-and-individual-rights\"\u003eTransparency and individual rights\u003c/h3\u003e\u003cp\u003eIn addition to the ethical reasons for seeking transparency, organisations need to be transparent about how they process personal data in an AI system so that individuals can effectively exercise the rights granted to them by the UK GDPR.\u003c/p\u003e\u003cp\u003eThe UK GDPR requires data controllers to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eprovide information to users in a concise, transparent, intelligible and easily accessible form using clear and plain language\u003c/li\u003e\n  \u003cli\u003ebe transparent about the purpose for processing personal data, retention periods and third parties involved in the processing activity\u003c/li\u003e\n  \u003cli\u003ebe transparent about the existence of automated decision making, providing meaningful information about the logic involved, and about the significance and envisaged consequences for the data subject of processing in this way\u003c/li\u003e\n  \u003cli\u003eprovide a clear explanation of the results these systems produce\u003c/li\u003e\n  \u003cli\u003euphold individuals’ rights, including the right of access to the personal data that you hold on them, and have a simple and clear process to exercise their right to correction and to object to the processing of their personal data at any time\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThe transparency principle applies to personal data collected from all sources, including the interactive qualities of AI systems that have the ability to collect new data, which may include text and audiovisual inputs. For example, if you’re using facial recognition technology for public area monitoring, you need to be transparent by clearly informing data subjects. You can do this with clear signage and information on relevant data controllers, what information is collected, the purpose and legal basis of processing, and for how long the data is kept.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-14\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eExplain your system in plain English.\u003c/li\u003e\n    \u003cli\u003eBe transparent about the purpose for processing personal data, retention periods and third parties involved in the processing activity.\u003c/li\u003e\n    \u003cli\u003eBe transparent about the existence and nature of automated decision making, using the \u003ca href=\"https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub\"\u003eAlgorithmic Transparency Recording Standard (ATRS)\u003c/a\u003e where required or on a voluntary basis as best practice.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eProvide a clear explanation of the results these systems produce, following guidance such as the ICO’s \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/explaining-decisions-made-with-artificial-intelligence/\"\u003eExplaining decisions made with AI\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"fairness\"\u003eFairness \u003c/h3\u003e\u003cp\u003eFairness in processing is another principle under the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted\"\u003eUK GDPR\u003c/a\u003e which applies to AI systems that process personal data. In the context of data protection legislation, fairness means that ‘you should only process personal data in ways that people would reasonably expect and not use it in any way that could have unjustified adverse effects on them’.\u003c/p\u003e\u003cp\u003e\u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/\"\u003eDPIAs\u003c/a\u003e are the main tool to help you consider the risks to the rights and freedoms of individuals, including the potential for any significant social or economic disadvantage. DPIAs also help demonstrate whether your processing is necessary to achieve your purpose, proportionate and fair.\u003c/p\u003e\u003cp\u003eThe Responsible Technology Adoption Unit (RTA) in the Department for Science, Information and Technology (DSIT) published the results of its \u003ca href=\"https://www.gov.uk/government/publications/public-attitudes-to-data-and-ai-tracker-survey-wave-3\"\u003ePublic Attitudes to Data and AI survey\u003c/a\u003e in December 2023. This report found that people’s comfort with the use of AI greatly depends on the specific context. Perceptions of the need for AI governance also vary considerably by sector, with a substantial proportion of the public prioritising careful management of AI used in healthcare, by the military, or in banking and finance.\u003c/p\u003e\u003cp\u003eYou must make sure that AI systems do not process personal data in ways that are unduly detrimental, unexpected or misleading to the individuals concerned. If AI systems infer data about people, you need to ensure that the system is accurate and is not discriminatory. You need to uphold the ‘right to be informed’ for individuals whose personal data is used at any stage of the development and deployment of AI systems. This is part of fulfilling the transparency and fairness principles. \u003c/p\u003e\u003cp\u003eData protection aims to protect individuals’ rights and freedoms with regard to the processing of their personal data, not just their information rights. This includes the right to privacy but also the right to non-discrimination. For example, computer vision technologies such as facial recognition have raised concerns due to the risk of errors in matching faces. This technology has proven to be less accurate when used on women and people of colour, producing biased results. Ultimately, this can create discrimination, raising fundamental rights concerns because of the disadvantage to some individuals whose facial images are captured and processed.\u003c/p\u003e\u003cp\u003ePeople’s facial images constitute biometric data. This is personal data because it’s the result of specific technical processing related to physical, physiological or behavioural characteristics of a natural person, which can confirm the unique identification of the person. Facial images may fall into the \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/lawful-basis/a-guide-to-lawful-basis/lawful-basis-for-processing/special-category-data/#scd1\"\u003especial categories of personal data\u003c/a\u003e because they’re likely to reveal sensitive characteristics such as racial or ethnic origin, and so require enhanced protection and additional safeguards. \u003c/p\u003e\u003cp\u003eBiometric data is also considered special category data when processed for the purposes of identification. You must ensure that the technologies used to capture and process this data are overt, accurate, proportionate, fair and deploy a narrow ‘zone of recognition’. For example, if someone walks past a camera and their image does not meet the threshold for a potential match, their data needs to be promptly deleted.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-15\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eIdentify the risks to the rights and freedoms of individuals through \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/accountability-and-governance/data-protection-impact-assessments-dpias/\"\u003eDPIAs\u003c/a\u003e and assess whether your processing is necessary, proportionate and fair to achieve your purpose.\u003c/li\u003e\n    \u003cli\u003eUse the ICO’s \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/ai-and-data-protection-risk-toolkit/\"\u003eAI data protection and risk toolkit\u003c/a\u003e to reduce the risks to individuals’ rights and freedoms.\u003c/li\u003e\n    \u003cli\u003eMitigate risks using the ICO’s \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/artificial-intelligence/guidance-on-ai-and-data-protection/how-do-we-ensure-fairness-in-ai/what-about-fairness-bias-and-discrimination\"\u003eguidance on fairness in AI systems\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eProvide users with clear reassurance that you’re upholding their right to privacy, including simple processes to exercise their rights in clear \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/the-right-to-be-informed/what-methods-can-we-use-to-provide-privacy-information/\"\u003eprivacy notices\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eAddress any objections from users – for example, related to solely automated decisions, or where there’s a significant legal impact – by implementing safeguards such as meaningful human intervention, or an effective process to obtain and consider individuals’ views and corrections of factual errors.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"data-minimisation\"\u003eData minimisation\u003c/h3\u003e\u003cp\u003eThe data minimisation principle requires you to identify the minimum amount of personal data you need to fulfil your purpose, and to only process that information and no more. This does not mean that AI tools should not process personal data, but if you can achieve the same outcome by processing less personal data then, by definition, the data minimisation principle requires you to do so. \u003c/p\u003e\u003cp\u003eRetaining data that is not strictly necessary is a risk to the individuals from whom the data is derived. Excluding irrelevant data prevents algorithms from identifying correlations that lack significance or are coincidental. There are a number of techniques that you can adopt to develop AI systems that process only the data you need, while still remaining functional. \u003c/p\u003e\u003cp\u003eFor example, you can consider using privacy-enhancing technologies (PETs) to offer stronger protections and preserve data privacy while enabling effective use of data. Some PETs provide new tools for anonymisation, and some enable collaborative analysis on privately held data sets, allowing data to be used without disclosing copies of the data. PETs are multi-purpose: you can use them to reinforce data governance choices, or as tools for data collaboration and greater accountability through audits. A data-focused example solution is to create ‘synthetic data’. This is an artificial data set that does not include any actual data on ‘real’ individuals but mirrors in characteristics and proportional relationships all statistical aspects of the original data set.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-16\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eJustify your use of personal data, using your DPIA to think about the problem you’re solving so that you settle with the minimum personal data that’s required. Less personal data means less risk.\u003c/li\u003e\n    \u003cli\u003eReduce the risk of individuals being identified through the processing of their personal data by using appropriate de-identification techniques (such as redaction, pseudonymisation and encryption).\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eRefer to the ICO guidance on \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-sharing/privacy-enhancing-technologies/\"\u003eprivacy-enhancing technologies (PETs)\u003c/a\u003e.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"storage-limitation\"\u003eStorage limitation\u003c/h3\u003e\u003cp\u003eThe UK GDPR states that you should only hold personal data as long as you can reasonably justify it for the purpose of your processing, and that you should not retain personal data longer than you need it. Think through:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ewhat personal data the technology will hold\u003c/li\u003e\n  \u003cli\u003ewhy you have it and what it’s used for\u003c/li\u003e\n  \u003cli\u003ewhether you can justify keeping it for that period of time\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou should map all personal data flows through every stage of development, testing and deployment, and utilise data minimisation, anonymisation techniques and eventual deletion to irreversibly transform or remove personal data.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-17\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eUse data minimisation and \u003ca rel=\"external\" href=\"https://ico.org.uk/media/about-the-ico/documents/4018606/chapter-2-anonymisation-draft.pdf\"\u003eanonymisation techniques (PDF, 325KB)\u003c/a\u003e as needed to remove or irreversibly transform personal data where possible.\u003c/li\u003e\n    \u003cli\u003eBe transparent about the length of personal data retention in privacy notices.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e\u003ch3 id=\"human-oversight\"\u003eHuman oversight \u003c/h3\u003e\u003cp\u003eAlthough it is possible to use AI systems for automated decision making where the system makes a decision automatically without any human involvement, this may infringe the UK GDPR. Article 22 currently prohibits decision(s) based solely on automated processing that have \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/automated-decision-making-and-profiling/what-does-the-uk-gdpr-say-about-automated-decision-making-and-profiling/#:~:text=by%20automated%20means.-,What%20types%20of%20decision%20have%20a%20legal%20or%20similarly%20significant,law%2C%20such%20as%20housing%20benefit.\"\u003elegal or similarly significant consequences\u003c/a\u003e for individuals. Services using AI that affect a person’s legal status or their legal rights must only use AI to support decisions that must be made by a human decision maker. \u003c/p\u003e\u003cp\u003eAI systems need to introduce deliberation processes into all stages of the life cycle so that the abilities of humans and machines are combined to reach the best results when performing tasks. However, the human input needs to be ‘meaningful’. Several factors determine how much human involvement there should be in AI systems, such as the complexity of the output, its potential impact, and the amount of specialist human knowledge (for example, legal and medical) required.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-18\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eDesign, document and assess the stages when meaningful human review processes are incorporated and what additional information will be taken into consideration when making the final decision.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003eUse the ICO guidance on \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/individual-rights/automated-decision-making-and-profiling/what-does-the-uk-gdpr-say-about-automated-decision-making-and-profiling/\"\u003eautomated decision making under UK GDPR\u003c/a\u003e for more clarity on types of decisions that have a legal or similarly significant effect.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"accuracy\"\u003eAccuracy \u003c/h3\u003e\u003cp\u003eAccuracy in the context of data protection requires that personal data is not factually incorrect or misleading, and, where necessary, is corrected, deleted and kept up to date without delay.\u003c/p\u003e\u003cp\u003eYou should not treat AI outputs as factual information about the individual, but instead consider these as a ‘statistically informed guess’. You also need to factor in the possibility of outputs being incorrect and the impact this may have on any decisions. \u003c/p\u003e\u003cp\u003eYou need to make it explicit that the outputs of your AI systems are statistically informed guesses rather than facts, including information about the source of the data and how the inference has been generated.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-19\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eTest AI outputs against existing knowledge and expertise during training and testing. \u003c/li\u003e\n    \u003cli\u003eBe transparent that outputs are statistically informed guesses rather than facts. \u003c/li\u003e\n    \u003cli\u003eDocument the source of the data and the AI system used to generate the conclusion.\u003c/li\u003e\n    \u003cli\u003eImplement processes to consider individuals’ feedback, views and corrections of factual errors.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Security",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#security",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#security",
        "text": "\u003cp\u003eCyber security is a primary concern for all government services, as laid out in the \u003ca href=\"https://www.gov.uk/government/publications/government-cyber-security-strategy-2022-to-2030\"\u003eGovernment Cyber Security Strategy\u003c/a\u003e. When building and deploying new services, including AI systems, the government has a responsibility to make sure these are secure to use and also resilient to cyber attacks. To meet this requirement, your service must comply with the government’s \u003ca rel=\"external\" href=\"https://www.security.gov.uk/guidance/secure-by-design/\"\u003eSecure by Design principles\u003c/a\u003e before it can be deployed. \u003c/p\u003e\u003cp\u003eThere are some security risks that apply uniquely to AI and/or generative AI technologies. This section takes you through some of these risks to help you keep AI solutions in government secure. \u003c/p\u003e\u003cp\u003eTo learn more about AI security, you’re encouraged to join the \u003ca href=\"mailto:x-gov-genai-security-group@digital.cabinet-office.gov.uk\"\u003ecross-government AI security group\u003c/a\u003e that brings together security practitioners, data scientists and AI experts. Please note that only those with GOV.UK email addresses can currently join this group.\u003c/p\u003e\u003ch3 id=\"how-to-deploy-ai-securely\"\u003eHow to deploy AI securely \u003c/h3\u003e\u003cp\u003eDepending on how AI systems are used, they can present different security challenges and varying levels of risk that must be managed. This section covers some of the approaches that you need to take for:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003epublic AI applications and web services\u003c/li\u003e\n  \u003cli\u003eembedded AI applications\u003c/li\u003e\n  \u003cli\u003epublic AI application programming interfaces (APIs)\u003c/li\u003e\n  \u003cli\u003eprivately hosted open source AI models\u003c/li\u003e\n  \u003cli\u003eworking with your organisational data\u003c/li\u003e\n  \u003cli\u003eopen-source vs closed-source models\u003c/li\u003e\n\u003c/ul\u003e\u003ch4 id=\"public-ai-applications-and-web-services\"\u003ePublic AI applications and web services\u003c/h4\u003e\u003cp\u003eA simple way to implement an AI solution is to use publicly available commercial applications – such as Google Gemini or ChatGPT in the case of generative AI. While you might think that these public tools are more secure, you should consider that you cannot easily control the data input to the models: you must rely on educating users on what data they can and cannot enter into these services.\u003c/p\u003e\u003cp\u003eYou also have no control over the outputs from these models, and you’re subject to their commercial licence agreements and privacy statements. For example, \u003ca rel=\"external\" href=\"https://help.openai.com/en/articles/5722486-how-your-data-is-used-to-improve-model-performance\"\u003eOpenAI\u003c/a\u003e will use the prompt data you enter directly into the ChatGPT website to improve their models, although individual users can opt out. When using public AI applications, you must not enter official information unless it has been published or is cleared for publication.\u003c/p\u003e\u003ch4 id=\"embedded-ai-applications\"\u003eEmbedded AI applications\u003c/h4\u003e\u003cp\u003eMany vendors include AI features and capabilities directly within their products, for example Slack GPT and Microsoft Copilot. While this guidance applies at a high level to each of these applications, they come with their own unique security concerns. Before adopting any of these products it’s important to understand the underlying architecture of the solution, and what mitigations the vendor has put in place for the inherent risks associated with AI.\u003c/p\u003e\u003cp\u003eIn addition to embedded applications, there are also many AI-powered plugins or extensions to other software. For example, Visual Studio Code has a large ecosystem of community-built extensions, many of which offer AI functionality. You must take extreme caution before installing any unverified extensions as these can pose a security risk.\u003c/p\u003e\u003cp\u003eThere has also been a proliferation of AI transcription tools that are capable of joining virtual meetings and transcribing meeting notes. These present a serious risk of data leakage as they silently upload meeting recordings to an AI service for transcription and analysis. When hosting virtual meetings, organisers should verify the identity of all attendees and state up front that the use of third-party meeting transcription tools is not allowed.\u003c/p\u003e\u003cp\u003eYou should always speak with your security team to discuss your requirements before deploying any embedded AI applications, extensions or plugins.\u003c/p\u003e\u003ch4 id=\"public-ai-apis\"\u003ePublic AI APIs\u003c/h4\u003e\u003cp\u003eMany public AI applications offer the ability to access their services through APIs. By using the API you can integrate AI capabilities into your own applications, intercept the data being sent to the AI model, and also process the responses before returning them to the user.\u003c/p\u003e\u003cp\u003eFor example, when integrating a large language model (LLM) through an API you can include privacy-enhancing technology (PET) to prevent data leakage, add content filters to sanitise the prompts and responses, and log and audit all interactions with the model. Be aware that PETs come with their own limitations, therefore selection of the PET should be proportionate to the sensitivity of the data.\u003c/p\u003e\u003cp\u003eRefer to the Information Commissioner’s Office (ICO)’s guidance on \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/uk-gdpr-guidance-and-resources/data-sharing/privacy-enhancing-technologies/\"\u003eprivacy-enhancing technologies (PETs)\u003c/a\u003e and the Responsible Technology Adoption Unit (RTA)’s \u003ca href=\"https://www.gov.uk/government/publications/privacy-enhancing-technologies-adoption-guide\"\u003ePET adoption guide\u003c/a\u003e for more information. Consider also that your data is still passed over to the provider when you use an API, although retention policies tend to be more flexible for API use. For example, OpenAI only \u003ca rel=\"external\" href=\"https://openai.com/enterprise-privacy#api-platform-faq\"\u003eretains prompt data sent to the API for 30 days\u003c/a\u003e.\u003c/p\u003e\u003ch4 id=\"privately-hosted-ai-models\"\u003ePrivately hosted AI models\u003c/h4\u003e\u003cp\u003eInstead of using a public AI offering, the alternative is to host your own AI model. This could be a model taken from one of the many publicly available pre-trained open source models or it could be a model you have built and trained yourself. By running a model in your own private cloud infrastructure, you ensure that data never leaves an environment that you own.\u003c/p\u003e\u003cp\u003eIn the case of generative AI, the models you can run in this way are not on the scale of the publicly available ones, but can still provide acceptable results for certain applications. The advantage is that you have complete control over the model and the data it consumes. The disadvantage is that you’re responsible for ensuring the model is secure and up to date. Consider that you must also maintain the infrastructure to host your model, which brings additional costs along with the specialist skills you’ll need in machine learning (ML) operations.\u003c/p\u003e\u003ch4 id=\"managed-machine-learning-model-hosting-platform\"\u003eManaged machine learning model hosting platform\u003c/h4\u003e\u003cp\u003eAn alternative approach to setting up the infrastructure to host your own model from scratch, is to use a fully managed ML model hosting platform. For example, \u003ca rel=\"external\" href=\"https://aws.amazon.com/bedrock/\"\u003eAmazon Bedrock\u003c/a\u003e and \u003ca rel=\"external\" href=\"http://watsonx.ai\"\u003eIBM watsonx.ai\u003c/a\u003e allow you to host different open source or commercially available AI models and compare their performance, while \u003ca rel=\"external\" href=\"https://learn.microsoft.com/en-gb/azure/ai-services/openai/overview\"\u003eMicrosoft Azure OpenAI service\u003c/a\u003e offers access to the OpenAI GPT models but running in a private instance with zero-day retention policies.\u003c/p\u003e\u003ch4 id=\"running-ai-models-locally\"\u003eRunning AI models locally\u003c/h4\u003e\u003cp\u003eMany open source AI models are capable of being run locally on a single machine. This is often attractive because it allows the models to be run in isolation, with limited or no network access. This type of deployment is not recommended for most production services, but might be appropriate for ad-hoc or one-off applications where performance of the model is not paramount. \u003c/p\u003e\u003ch4 id=\"training-ai-models\"\u003eTraining AI models\u003c/h4\u003e\u003cp\u003eIn addition to where your AI model runs, you should also consider how it was trained because this is important from a security perspective. For example, many of the publicly available generative AI models were trained using data from the public internet. This means that they could include data that is personally identifiable, inaccurate, illegal or harmful, any of which could present a security risk.\u003c/p\u003e\u003cp\u003eIt’s possible to train an AI model using your own data, and for many specific and limited tasks this is often the most appropriate approach because it gives you complete control of the training data. For generative AI models, the cost of doing this for larger and more capable systems is prohibitive and the amount of private data required to produce acceptable performance of a large model is beyond the capacity of most organisations. You should assume that any data you use to train your model could be extracted by an attacker. There’s more about this in the Data leakage section.\u003c/p\u003e\u003ch4 id=\"working-with-your-organisational-data\"\u003eWorking with your organisational data\u003c/h4\u003e\u003cp\u003eA key application of AI is working with your organisation’s private data. By enabling the model to access, understand and use this data, insights and knowledge can be provided to users that are specific to their subject domain and will provide more reliable results. For a standard ML model, you can train them directly with your private data set. For generative AI models, you can fine-tune them or use approaches like retrieval augmented generation (RAG) to augment the model with your private data.\u003c/p\u003e\u003cp\u003eIf you use your own data with an AI model, you immediately increase the data security risk and you need to apply additional security controls to stop data leakage and privacy violations.\u003c/p\u003e\u003cp\u003eQuestions you should consider when using your own private data with an AI model include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ewhere is your data being sent and how is it being processed?\u003c/li\u003e\n  \u003cli\u003eis your data being used to train future models?\u003c/li\u003e\n  \u003cli\u003ehow long is your data being retained? \u003c/li\u003e\n  \u003cli\u003eis your data being logged and who has access to those logs and for what purpose?\u003c/li\u003e\n\u003c/ul\u003e\u003ch4 id=\"open-source-vs-closed-source-models\"\u003eOpen-source vs closed-source models\u003c/h4\u003e\u003cp\u003eNeither open-source or closed-source AI models are inherently less secure than the other. A fully open-source model may expose not only the model code, but also the weights of its parameters and the data used to train the model. While this increases transparency, it also potentially presents a greater risk, as knowing the weights and the training data could allow an attacker to create attacks carefully tailored to the specific model.\u003c/p\u003e\u003cp\u003eOne benefit of fully open-source models is that they allow you to inspect the source code and model architecture, enabling security experts to audit the code for vulnerabilities. Despite this, because of its complexity, even an open source generative AI model remains mostly opaque and hard to analyse. \u003c/p\u003e\u003ch3 id=\"security-risks\"\u003eSecurity risks\u003c/h3\u003e\u003cp\u003eAI security risks are divided into 2 main categories: the risk of using AI and the risk of adversaries using AI against you. \u003c/p\u003e\u003ch4 id=\"using-ai\"\u003eUsing AI\u003c/h4\u003e\u003cp\u003eThere are many resources you can use to explore the risks of using AI:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ethe National Cyber Security Centre (NCSC) has published a set of \u003ca rel=\"external\" href=\"https://www.ncsc.gov.uk/collection/machine-learning\"\u003eprinciples around securing machine learning (ML) solutions\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003eMicrosoft has compiled a list of \u003ca rel=\"external\" href=\"https://learn.microsoft.com/en-us/security/engineering/failure-modes-in-machine-learning\"\u003eML failure modes\u003c/a\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003ca rel=\"external\" href=\"https://atlas.mitre.org/matrices/ATLAS\"\u003eMITRE Adversarial Threat Landscape for AI Systems (ATLAS) matrix\u003c/a\u003e is an open source knowledge base of techniques used to attack AI systems\u003c/li\u003e\n  \u003cli\u003e\u003ca href=\"https://www.gov.uk/government/publications/international-scientific-report-on-the-safety-of-advanced-ai\"\u003eInternational Scientific Report on the Safety of Advanced AI has an analysis of risks posed by general purpose advanced AI systems\u003c/a\u003e\u003c/li\u003e\n  \u003cli\u003ethe Open Worldwide Application Security Project (OWASP) has done significant work to identify the \u003ca rel=\"external\" href=\"https://owasp.org/www-project-top-10-for-large-language-model-applications/\"\u003eunique risks posed by LLMs\u003c/a\u003e – these risks focus on the use of LLMs but many of them will also apply to other types of generative AI models and more widely to AI in general\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eFrom a combination of these sources, we can draw out some of the most common vulnerabilities and discuss them in context of AI applications in government.\u003c/p\u003e\u003ch5 id=\"data-and-model-poisoning\"\u003eData and model poisoning\u003c/h5\u003e\u003cp\u003eThis is when data used to train an AI model has been tampered with, leading the model to produce incorrect or harmful output.\u003c/p\u003e\u003cp\u003eAttackers can target the data used to train an AI model to introduce vulnerabilities, backdoors or biases that compromise the model’s security and behaviour. \u003c/p\u003e\u003cp\u003eFor a traditional ML model that uses a limited amount of training data for a specific task, this type of attack can be prevented by training the model yourself on a known data set that you control. \u003c/p\u003e\u003cp\u003eFor frontier generative AI models, the barrier to entry for training a model from scratch is high and fine-tuning an existing model is much easier and cheaper. There are many open source models that are easy to fine-tune (for image generation, for example), and these can and are used to produce specific types of outputs, some of which are harmful or illegal. \u003c/p\u003e\u003cp\u003eWhen using an AI model – particularly a specialised, fine-tuned model – from a third-party source, it’s difficult to ascertain if it has been tampered with. Poisoned models may appear to be functioning as expected until a specific prompt triggers the malicious behaviour.\u003c/p\u003e\u003cp\u003eSupply chain vulnerabilities of this kind are not unique to AI. For example, when software libraries are hacked, all downstream systems that depend on those libraries are affected – a notable example of this was the \u003ca rel=\"external\" href=\"https://www.bleepingcomputer.com/news/security/dev-corrupts-npm-libs-colors-and-faker-breaking-thousands-of-apps/\"\u003eFaker NPM hack\u003c/a\u003e. Many automated tools exist for detecting, tracking and fixing security issues with open source software, but the current tooling for doing this with open source AI models is much more limited. Popular open source AI model site \u003ca rel=\"external\" href=\"https://huggingface.co/\"\u003eHugging Face\u003c/a\u003e does have some \u003ca rel=\"external\" href=\"https://huggingface.co/docs/hub/security-malware\"\u003emalware scanning tooling\u003c/a\u003e, but this is not capable of determining if a given AI model has been trained on poisoned data. \u003c/p\u003e\u003cp\u003eAI model hosting platforms like Microsoft’s \u003ca rel=\"external\" href=\"https://azure.microsoft.com/en-gb/products/ai-studio/\"\u003eAzure AI\u003c/a\u003e, Amazon’s \u003ca rel=\"external\" href=\"https://aws.amazon.com/bedrock/\"\u003eBedrock\u003c/a\u003e and IBM’s \u003ca rel=\"external\" href=\"http://watsonx.ai\"\u003ewatsonx.ai\u003c/a\u003e allow developers to use commercial models and other third-party AI models. These services do not make any guarantees about the security and integrity of the third-party models that they’re capable of hosting.\u003c/p\u003e\u003cp\u003eTraining data can also be poisoned indirectly through the introduction of malicious data into known collections of open data that are used to train or fine-tune frontier generative AI models. This is likely to become an increasing threat as hackers learn which publicly available data sets (for example, Wikipedia or Reddit) have been used to train generative AI models, and target these to poison future versions of the models.\u003c/p\u003e\u003cp\u003eThe impacts of an AI model trained or fine-tuned with poisoned data are wide ranging, including direct security threats to the organisation running the model and biased or harmful outputs to the users of the model. Poisoned models could push people to particular products or subvert confidence in government services. \u003c/p\u003e\u003cp\u003eTo help detect and prevent data poisoning, you’ll need to make sure your users and developers are trained on the risks and aware that the results from AI models can be false or biased. Outputs of models should be tested against known good responses and should be systematically tested for biases. ML hosting platforms often include evaluation tools that can measure and test the performance of an ML model, such as \u003ca rel=\"external\" href=\"https://cloud.google.com/vertex-ai/docs/evaluation/introduction\"\u003eGoogle’s Vertex AI\u003c/a\u003e or \u003ca rel=\"external\" href=\"https://learn.microsoft.com/en-us/azure/machine-learning/prompt-flow/concept-model-monitoring-generative-ai-evaluation-metrics?view=azureml-api-2\"\u003eMicrosoft’s Prompt Flow\u003c/a\u003e. Improved explainability of the models themselves would also help, enabling the output to be traced back to the source training data. For more information, refer to the Transparency and explainability section.\u003c/p\u003e\u003ch5 id=\"data-leakage\"\u003eData leakage\u003c/h5\u003e\u003cp\u003eThis is when responses from an AI model reveal confidential information, such as personal data. \u003c/p\u003e\u003cp\u003eAI models are trained using data. In the case of generative AI, data is commonly taken from the public internet and will contain personal data and other confidential information. AI models can suffer from data leakage, depending on their intended mode of operation. For example, a model that is being used as a classifier to rank or sort data into groups based on criteria would necessarily be less likely to leak training data than a generative AI model, as its outputs are confined to the specific classification problem. However, if an AI model has been trained or fine-tuned with private data that has different levels of security controls based on the user who should be seeing it, for example documents that are restricted to a specific group of people, there is currently no way to preserve these controls when training the model.\u003c/p\u003e\u003cp\u003eGenerative AI models can also be \u003ca rel=\"external\" href=\"https://arxiv.org/abs/2311.17035\"\u003emade to reveal their original training data\u003c/a\u003e through their responses, meaning that any outputs from a generative AI model could potentially contain confidential information. For generative AI, a way to preserve user access controls is to use ‘in-context learning’. A search is carried out first on the private data a user is permitted to see, and the retrieved results are passed in context to the generative AI model. This type of approach is known as retrieval augmented generation (RAG), and is used in many commercial generative AI tools (such as Microsoft Copilot). \u003c/p\u003e\u003cp\u003eIn-context learning has limitations and can degrade the performance of a generative AI model in certain applications. RAG tools are susceptible to indirect prompt injection, either in the information retrieved by the initial search or through the user prompt, meaning that security controls could be circumvented and private data could still be leaked.\u003c/p\u003e\u003cp\u003eYou should make sure your AI model only has access to the data the user of the model should be able to access.\u003c/p\u003e\u003ch5 id=\"insecure-ai-tool-chain\"\u003eInsecure AI tool chain\u003c/h5\u003e\u003cp\u003eThis refers to when tools used to train, fine-tune, host, serialise and operate AI models are not secure. \u003c/p\u003e\u003cp\u003eSpecialised tools built to support AI models have been found to lack basic security features. For example, the Pickle format used to serialise ML models \u003ca rel=\"external\" href=\"https://huggingface.co/docs/hub/security-pickle\"\u003ehas serious security flaws\u003c/a\u003e. This may be because the tools were developed at pace by AI researchers and data scientists not following secure coding practices. AI tools often have elevated access rights to the systems they’re running on, making the impact of a security breach even worse. This is not a new risk, as any developer tooling can be insecure, but AI tools appear to be particularly prone to security issues. It’s easier for a hacker to target the tool chain for an AI model than the model itself.\u003c/p\u003e\u003cp\u003eYou should make sure your cybersecurity team has approved the tools you use to support your AI models. This includes checking that the tools implement user authentication and follow the principle of least privilege, meaning they are not running with administrator permissions to your system.\u003c/p\u003e\u003ch5 id=\"exacerbates-previously-existing-risks\"\u003eExacerbates previously existing risks\u003c/h5\u003e\u003cp\u003eThis refers to when the use of AI exacerbates previously existing risks, such as poor data management, insufficient security classification, insecure storage of credentials, and more. \u003c/p\u003e\u003cp\u003eAn example of this sort of risk is over-privileged access. This happens when an AI tool is used to enhance enterprise-wide search capabilities. A user may not be aware of sensitive data that they currently have access to on a government system, but when they use an AI-enhanced search tool, the power of the tool exposes the lack of access controls and brings back sensitive data that the user was unaware of being able to see. The AI tool is not creating this issue: the problem already exists, but the AI tool is making it worse. \u003c/p\u003e\u003cp\u003eIn line with the advice of vendors, you should review all enterprise access controls before deploying an AI tool to your system. This should be an ongoing exercise because no system is static. It’s essential that you’re able to continually monitor and review access controls when deploying AI applications across your organisation.\u003c/p\u003e\u003ch5 id=\"perturbation-attack\"\u003ePerturbation attack\u003c/h5\u003e\u003cp\u003eThis is when an attacker stealthily modifies the inputs to an AI model to get a desired response. \u003c/p\u003e\u003cp\u003eAn example of this type of threat is a computer vision (CV) system for medical diagnostics trained to distinguish between abnormal and normal scans. The system can be fooled when presented with an image containing specific amounts of noise, causing it to classify a scan incorrectly. Mitigations include adversarial training of the model with noisy images to improve robustness against this type of attack.\u003c/p\u003e\u003ch5 id=\"prompt-injection\"\u003ePrompt injection \u003c/h5\u003e\u003cp\u003eThis is when hackers use prompts that can make the generative AI model behave in unexpected ways. \u003c/p\u003e\u003cp\u003ePrompt injection is a type of perturbation attack specifically targeted at generative AI systems that use text prompts to generate new content (text, image, audio, video). Developers lay down rules about how a model should behave and respond as system instructions, which are provided to the model along with the user prompt. Fundamentally, a generative AI model cannot distinguish between the user prompt and these system instructions because both are just seen as input to the model. A hacker can exploit this flaw by crafting special prompts that circumvent the system instructions, causing the model to respond in an unintended way. \u003c/p\u003e\u003cp\u003eThe potential impact of prompt injections ranges from very mild, like a user making a banking chatbot tell jokes in the style of a pirate, to much more serious. For example, a hacker might trick a generative AI model designed to send alerts to patients about medical appointments into sending fake messages about non-existing appointments.\u003c/p\u003e\u003cp\u003ePrompt injections come in 2 forms: \u003c/p\u003e\u003cul\u003e\n  \u003cli\u003edirect, which means the user who is interacting with the generative AI model crafts the prompt injection themselves\u003c/li\u003e\n  \u003cli\u003eindirect, when other information that is being sent to a generative AI model is tampered with to include a prompt injection. For example, an email attachment can include a prompt and when a generative AI model that is tasked with summarising emails reads the attachment, the prompt injection is triggered\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eGenerative AI has the ability to take natural language inputs and have a machine act on them. A common pattern for using LLMs in this way is called ReAct (\u003ca rel=\"external\" href=\"https://arxiv.org/abs/2210.03629\"\u003eReason-Act\u003c/a\u003e): the LLM is prompted to reason about how to perform a task, and the response from the model is processed and used to automate calls to different services that perform actions. Hackers can subvert this approach to make the model perform different actions, which significantly limits the utility of generative AI in fully automated solutions. To make sure the model is doing the right thing, there must be a human present to review the action before carrying it out. This is why the majority of commercial applications of generative AI are in the space of human assistants (‘copilots’). \u003c/p\u003e\u003cp\u003eWork is underway to address the prompt injection issue and a number of mitigations are already available. These include:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003efiltering prompts before they’re sent to the model (by sending the prompts through another ML model trained to detect likely prompt injections)\u003c/li\u003e\n  \u003cli\u003efiltering the outputs of the model before they’re returned to the user\u003c/li\u003e\n  \u003cli\u003emore \u003ca rel=\"external\" href=\"https://arxiv.org/html/2312.14197v1\"\u003especulative work\u003c/a\u003e around fine-tuning models to better distinguish between user input and system prompts\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eTo defend against prompt injection, you should log all prompts sent to a model and carry out ongoing audits to determine if prompt injection is happening, blocking users you find who are responsible.\u003c/p\u003e\u003ch5 id=\"hallucinations\"\u003eHallucinations\u003c/h5\u003e\u003cp\u003eHallucinations are when the generative AI model responds with information that appears to be truthful but is actually false.\u003c/p\u003e\u003cp\u003eCounterintuitively for a machine, generative AI is better at creative tasks than fact retrieval. This is because all generative AI models predict and generate content by determining the most likely subsequent pattern based on previous training (for example, an LLM will predict the next most likely word). The models are therefore very good at generating plausible predictions that look correct but may not actually be correct. The risk is that overreliance by human operators on the outputs of generative AI models results in misinformation, miscommunication, legal issues and security vulnerabilities. \u003c/p\u003e\u003cp\u003eFundamentally, generative AI models cannot be trusted to produce factual content. Any generative AI services that output generated content directly to the public – for example, an LLM-powered chatbot giving advice on a government website – would be prone to hallucination and could lead to someone being misled about a government service, policy or point of law. \u003c/p\u003e\u003cp\u003eA \u003ca rel=\"external\" href=\"https://bc.ctvnews.ca/air-canada-s-chatbot-gave-a-b-c-man-the-wrong-information-now-the-airline-has-to-pay-for-the-mistake-1.6769454\"\u003elegal case in Canada\u003c/a\u003e found an organisation that owned a site with a hallucinating chatbot financially responsible for the bad advice it dispensed. In the worst case, hallucination could even lead to direct harm if a user acted on faulty advice. For example, a user being advised not to seek medical attention when they needed to. \u003c/p\u003e\u003cp\u003eIn addition to this direct risk, there’s also a significant indirect risk if officials are relying on generative AI as a primary information source when providing the public with guidance, advising ministers or informing policy decisions. \u003c/p\u003e\u003cp\u003eYou should make sure your users are trained not to uncritically trust the outputs of generative AI or to rely exclusively on their responses. Specifically around cybersecurity, if security practitioners in government become overly reliant on the advice of generative AI assistants, they may become less effective at spotting novel attacks and may even be misled into following bad advice and exposing systems to increased cybersecurity risks.\u003c/p\u003e\u003ch4 id=\"adversaries-using-ai\"\u003eAdversaries using AI\u003c/h4\u003e\u003ch5 id=\"misinformation\"\u003eMisinformation\u003c/h5\u003e\u003cp\u003eThis is when AI is used to create realistic synthetic media, leading to the spread of misinformation, undermining trust in digital media and manipulating public opinion. \u003c/p\u003e\u003cp\u003eAdversaries could use AI to interfere in electoral processes and spread misinformation. What makes this threat more potent is the ease with which bad actors can produce content for multiple audiences in many languages, with translations reflecting nuance and common parlance to make them more credible.\u003c/p\u003e\u003cp\u003eThe technical ease with which generative AI models can be integrated with social media or other platforms also means that bad actors could spread misinformation automatically and at scale. At present, misinformation is the most pressing risk that AI (particularly generative AI) presents to governments, specifically relating to the integrity of democratic elections. There have already been a number of instances of \u003ca rel=\"external\" href=\"https://www.bbc.co.uk/news/world-us-canada-68064247\"\u003esuspected AI-generated fake media\u003c/a\u003e being deployed in the US. With the release of new, more powerful generative AI models capable of generating realistic video content, such as OpenAI’s \u003ca rel=\"external\" href=\"https://openai.com/sora\"\u003evideo generation model Sora\u003c/a\u003e, this risk is only going to increase.\u003c/p\u003e\u003cp\u003eBig tech companies have committed to address the issue by including \u003ca rel=\"external\" href=\"https://arstechnica.com/ai/2023/07/openai-google-will-watermark-ai-generated-content-to-hinder-deepfakes-misinfo/\"\u003ewatermarks in the generated AI content\u003c/a\u003e their models create, but so far this has not become widespread. \u003ca rel=\"external\" href=\"https://www.bbc.co.uk/news/technology-66618852\"\u003eGoogle has announced a tool\u003c/a\u003e that can detect and watermark AI-generated content. The \u003ca rel=\"external\" href=\"https://c2pa.org/\"\u003eC2PA open standard\u003c/a\u003e for embedding metadata into media content, which allows the source and provenance of the content to be verified, is also gaining some traction.\u003c/p\u003e\u003cp\u003eWhen building services that receive and process digital content (text, images or even video), you’ll need to consider the impact of that content being generated by AI and therefore being unreliable, misleading or malicious. For more information about the ethical implications of misinformation, refer to the Societal wellbeing and public good section. \u003c/p\u003e\u003ch5 id=\"phishing\"\u003ePhishing\u003c/h5\u003e\u003cp\u003eThis is when generative AI is used to craft more convincing phishing emails and messages that can be tailored to specific user groups, leading to an increase in internet fraud.\u003c/p\u003e\u003cp\u003eLLMs make it easy for fraudsters to create convincing phishing emails and messages in different languages, even those they do not speak. LLMs can be easily automated to produce unique, targeted and personalised phishing emails at scale, making detection much harder. There is already \u003ca rel=\"external\" href=\"https://www.prnewswire.com/news-releases/fido-alliance-study-reveals-growing-demand-for-password-alternatives-as-ai-fuelled-phishing-attacks-rise-301957007.html\"\u003esome evidence\u003c/a\u003e that the amount of phishing emails and messages is rising, with the likely cause being the advent of generative AI. \u003c/p\u003e\u003cp\u003eGovernment is likely to see an increase in phishing emails and social engineering attacks as a result of generative AI. The risk of cyber security breaches through targeted, socially engineered attacks driven by generative AI could become more acute, as it may become easier to identify likely targets by using generative AI to trawl across social networks and other public resources, looking for contact details for government employees in sensitive roles.\u003c/p\u003e\u003cp\u003eTo detect scams of this sort, you’ll need more sophisticated counter measures – for example, using another specially trained ML model to detect and block phishing emails produced by generative AI. \u003c/p\u003e\u003cp\u003eYou’ll also need to educate users about how to detect AI-produced fake messages, because previous red flags such as badly formed sentences and incorrect spelling will no longer be enough. The likelihood is that phishing attacks will become more targeted, and use more sophisticated social engineering techniques to gain the recipient’s trust. \u003c/p\u003e\u003ch5 id=\"cyber-attacks\"\u003eCyber attacks \u003c/h5\u003e\u003cp\u003eThis refers to when generative AI lowers the bar for entry for hackers to create malware and craft cyber attacks that are more sophisticated and harder to detect.\u003c/p\u003e\u003cp\u003eGenerative AI has proved to be highly capable at aiding developers to write effective code: the AI model provides the developer with the majority of the solution, including prerequisites and boilerplate code, leaving the developer only needing to finesse the final details. A hacker using a generative AI model specifically to create malware or craft a cyber attack is likely to more quickly and easily achieve a working attack.\u003c/p\u003e\u003cp\u003eAll large commercial generative AI models have filters in place to try to detect if a user is asking the model to create malware. However, these filters can be subverted (refer to prompt injection threats). The expectation from some cyber security experts is that the number and sophistication of cyber attacks is likely to rise due to the use of generative AI, as many more bad actors who previously were excluded from being able to create credible threats are now able to do so.\u003c/p\u003e\u003cp\u003eThe unique position of government, and the capabilities and desire of hostile state-sponsored groups, mean that this threat is likely to be a key concern for government cybersecurity teams. The potential for escalating levels of sophisticated cyber attacks fuelled by generative AI is real, although \u003ca rel=\"external\" href=\"https://www.microsoft.com/en-us/security/blog/2024/02/14/staying-ahead-of-threat-actors-in-the-age-of-ai/\"\u003eresearch by Microsoft and OpenAI\u003c/a\u003e has yet to observe any particularly novel or unique attacks resulting from the use of AI. The area is under constant review.\u003c/p\u003e\u003cp\u003eYou should expect increased numbers of cyber attacks and take steps to increase your existing cyber security defences. For more information, refer to the \u003ca rel=\"external\" href=\"https://www.ncsc.gov.uk/report/impact-of-ai-on-cyber-threat\"\u003eNCSC report on the near-term impact of AI on the cyber threat\u003c/a\u003e.\u003c/p\u003e\u003ch5 id=\"fake-official-correspondence\"\u003eFake official correspondence \u003c/h5\u003e\u003cp\u003eThis is when generative AI is used to craft convincingly human correspondence which can either be automated and sent at scale to organisations, flooding their usual communications channels, or lead to unfair outcomes when judged against human correspondence. \u003c/p\u003e\u003cp\u003eAn example of this kind of threat might be a hacker using generative AI to create thousands of requests for information from a government department, seemingly sent from multiple unique people. Similar to the phishing threat, the ability of LLMs to create convincing and plausible text in an automated way, at scale, makes this type of attack particularly concerning. A hacker could overwhelm an organisation’s normal communications channels, causing an organisation to spend time and money responding to seemingly genuine requests while degrading their ability to cope with real people. \u003c/p\u003e\u003cp\u003eAnother example of this sort of threat, at a lower scale, is a fraudster submitting official information that will be used to judge a decision, and where the use of generative AI to create fake answers may prejudice the decision.\u003c/p\u003e\u003cp\u003eAreas of particular concern for the UK government are commercial procurement, recruitment, freedom of information requests, and the processing of claims that require an evidence-based decision.\u003c/p\u003e\u003cp\u003eMitigations are similar to those for phishing or misinformation attacks – for example, processing all correspondence through another ML model trained to detect AI-generated content. When running services that result in decisions based on evidence provided through official correspondence, you should consider the potential impact on the service of the correspondence being AI generated.\u003c/p\u003e\u003ch3 id=\"security-opportunities\"\u003eSecurity opportunities\u003c/h3\u003e\u003cp\u003eIn addition to the threats posed by AI there are also opportunities to improve cyber security through the use of AI. Some of these opportunities are:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ethreat detection: AI can be used to improve threat detection systems by generating synthetic cyber attack data for training more robust models, or directly detecting anomalies in real-time cyber security data. AI models can also be used to analyse historical cyber security data and identify patterns associated with known threats. These patterns can then be used to detect anomalies in real-time network traffic or system behaviour. When unusual activity occurs, the AI model can trigger an alert to be raised to human operators. \u003c/li\u003e\n  \u003cli\u003eincident response: AI models trained or fine-tuned on large amounts of historical cyber security data can predict future threats. By recognising subtle changes in patterns, they can be used to anticipate emerging attack vectors. Generative AI can assist in incident response by automating the generation of reports, recommending remediation actions based on past data, or filtering out noise in verbose cyber security logs, allowing human analysts to focus on the most important information. \u003c/li\u003e\n  \u003cli\u003esecurity testing: generative AI can create security test cases, improving the efficiency and coverage of security testing. Instead of manually crafting test scenarios, security professionals can use generative AI models to mimic adversary behaviour, simulate various attacks, and analyse existing vulnerabilities, attack patterns and system behaviours. LLMs are good at analysing code, meaning they can also be used to review source code, point out security flaws, and generate secure code snippets based on security best practices. \u003c/li\u003e\n  \u003cli\u003eenhancing vulnerability management: generative AI can assist in documenting security products. LLMs can be used to process the large amounts of documentation, guidance and online help around different security tools and their features and limitations, providing summarised information and enhanced search capabilities. Internet-enabled LLMs can also provide up-to-date insights, helping prioritise vulnerability patches and updates.\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"scenarios\"\u003eScenarios\u003c/h3\u003e\u003cp\u003eThe scenarios discussed below build on the security risks identified in this section, and will help you understand how they apply to some of the applications of AI in government. \u003c/p\u003e\u003cp\u003eEach scenario includes descriptions of potential impacts and mitigations. The likelihood and impact of each risk is scored following the approach outlined in the \u003ca rel=\"external\" href=\"https://owasp.org/www-community/OWASP_Risk_Rating_Methodology\"\u003eOWASP risk rating methodology\u003c/a\u003e. In addition to the impact factors included in the OWASP approach, user harm and misinformation are discussed as significant impact factors.\u003c/p\u003e\u003cp\u003eThis list of security threat scenarios is not exhaustive, but you can use the scenarios as a template for assessing the risks associated with different applications of AI.\u003c/p\u003e\u003col\u003e\n  \u003cli\u003ePerturbation attack: an attacker stealthily modifies the inputs to an AI model to get a desired response: Identity verification using image and video capture technology.\u003c/li\u003e\n  \u003cli\u003eInsecure AI tool chain: tools used to train, fine-tune, host, serialise and operate AI models are not secure: Machine learning operations (MLOps) tools used with default configuration.\u003c/li\u003e\n  \u003cli\u003ePrompt injection threats: using prompts that can make the generative AI model behave in unexpected ways: LLM chatbot on a government website.\u003c/li\u003e\n  \u003cli\u003eData leakage: responses from the LLM reveal sensitive information, for example personal data: Enterprise AI search tool summarising emails.\u003c/li\u003e\n  \u003cli\u003eHallucinations: the LLM responds with information that appears to be truthful but is actually false: Developer uses LLM-generated code without review.\u003c/li\u003e\n\u003c/ol\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch5 id=\"identity-verification-using-image-and-video-capture-technology\"\u003eIdentity verification using image and video capture technology\u003c/h5\u003e\n\n  \u003ch6 id=\"scenario\"\u003eScenario\u003c/h6\u003e\n\n  \u003cp\u003eA government service requires users to prove their identity by capturing an image of an identity document containing their picture, such as a passport or driving licence. A CV AI system then compares this image to a live video clip of the person to verify that the person is who they claim to be. A malicious user uses AI deepfake technology to insert their own image over the genuine photo of another person on a stolen identity document. The CV system is tricked into wrongly verifying that the malicious user’s identity matches the credentials on the other user’s identity document.\u003c/p\u003e\n\n  \u003ch6 id=\"impact\"\u003eImpact\u003c/h6\u003e\n\n  \u003cp\u003eMisidentification of the true user, leading to identity fraud.\u003c/p\u003e\n\n  \u003cp\u003eFraudulent access to a government service.\u003c/p\u003e\n\n  \u003cp\u003eData loss of personal and confidential data about the genuine user.\u003c/p\u003e\n\n  \u003cp\u003eSerious security breach if the service provides access to sensitive government information.\u003c/p\u003e\n\n  \u003ch6 id=\"mitigation\"\u003eMitigation\u003c/h6\u003e\n\n  \u003cp\u003eEnsure the service only uses biometric identity documents. For example, biometric passports contain electronic passport photos which can be securely transferred to the service for verification. \u003c/p\u003e\n\n  \u003cp\u003eUse a trusted third-party service to look up and provide reference images for identity documents, rather than relying on images captured by users themselves. For example, the service could use licence images stored in the DVLA system and check this against the live video image.\u003c/p\u003e\n\n  \u003cp\u003eUse deepfake detection methods to scan input digital images and video clips which are received by the service. \u003c/p\u003e\n\n  \u003ch6 id=\"risk-rating\"\u003eRisk rating\u003c/h6\u003e\n\n  \u003cp\u003eLikelihood: MEDIUM\u003c/p\u003e\n\n  \u003cp\u003eImpact: HIGH\u003c/p\u003e\n\n  \u003ch6 id=\"recommendation\"\u003eRecommendation\u003c/h6\u003e\n\n  \u003cp\u003eIn this specific example, use of a biometric passport can prevent the attack. However, if the deepfake were applied to the live video clip of the user instead of the image of the identity document, the system could still be fooled. Access to this type of deepfake technology is becoming increasingly available, meaning that when you build services to receive and process images or video, you must put in place mechanisms to detect if the content has been manipulated by AI.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch5 id=\"mlops-tools-used-with-default-configuration\"\u003eMLOps tools used with default configuration \u003c/h5\u003e\n\n  \u003ch6 id=\"scenario-1\"\u003eScenario\u003c/h6\u003e\n\n  \u003cp\u003eA data scientist working in a government organisation wants to experiment with training their own ML model using organisational data. The experiment aims to test whether the model can be used to triage official correspondence, improving efficiency. The data scientist starts by using an open source MLOps tool to host and deploy their ML model in their local environment. The default configuration of the tool exposes a public endpoint with no authentication on the public internet. By default, the tool runs with administrator permissions on the host machine. A hacker discovers the exposed endpoint and sends commands to the MLOps tool, using it to gain a foothold in the organisation’s network.\u003c/p\u003e\n\n  \u003ch6 id=\"impact-1\"\u003eImpact\u003c/h6\u003e\n\n  \u003cp\u003eSerious security breach, which could lead to catastrophic damage to the organisation’s computer systems.\u003c/p\u003e\n\n  \u003cp\u003eData loss, including the organisational data used to train the ML model, and other sensitive data that can be accessed through the tool’s elevated permissions.\u003c/p\u003e\n\n  \u003ch6 id=\"mitigation-1\"\u003eMitigation\u003c/h6\u003e\n\n  \u003cp\u003eCheck the default configuration of all ML tools before deploying them and ensure basic security controls are in place:\u003cbr\u003e\n- authentication is enabled\n- no public-facing endpoints are exposed unless explicitly required\n- the principle of least privilege is applied so that tools only run with the minimum permissions they require\u003c/p\u003e\n\n  \u003ch6 id=\"risk-rating-1\"\u003eRisk rating\u003c/h6\u003e\n\n  \u003cp\u003eLikelihood: MEDIUM\u003c/p\u003e\n\n  \u003cp\u003eImpact: HIGH\u003c/p\u003e\n\n  \u003ch6 id=\"recommendation-1\"\u003eRecommendation\u003c/h6\u003e\n\n  \u003cp\u003eAI tools should be treated in the same way as all other third-party software. Even when they’re being used for experimentation, secure by design principles should always be applied.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch5 id=\"llm-chatbot-on-a-government-website--full-chat-interface\"\u003eLLM chatbot on a government website – full chat interface\u003c/h5\u003e\n\n  \u003ch6 id=\"scenario-2\"\u003eScenario\u003c/h6\u003e\n\n  \u003cp\u003eA chatbot deployed to a government website to assist with queries relating to a particular public service. The chatbot uses a private instance of one of the publicly trained LLMs. The user’s question is combined with system instructions that tell the LLM to only respond to questions relevant to the specific service. The system instructions are combined with the user’s original question and sent to the LLM. A malicious user could craft a specific prompt that circumvents the system instructions and makes the chatbot respond with irrelevant and potentially harmful information. \u003c/p\u003e\n\n  \u003cp\u003eThis is an example of a direct prompt injection attack.\u003c/p\u003e\n\n  \u003ch6 id=\"impact-2\"\u003eImpact\u003c/h6\u003e\n\n  \u003cp\u003eActual risk of user harm if a user is tricked into using an unsafe prompt that then results in harmful content being returned and acted on. For example, a user looking for information on how to pay a bill is directed to a fraudulent payment site.\u003c/p\u003e\n\n  \u003cp\u003eReputational damage to the government if a user made public potentially harmful responses received from the chatbot – for example, a user asking for generic information and receiving an inflammatory response.\u003c/p\u003e\n\n  \u003ch6 id=\"mitigation-2\"\u003eMitigation\u003c/h6\u003e\n\n  \u003cp\u003eUse prompt engineering to attach a meta prompt to any user input to prevent the LLM from responding to malicious input.\u003c/p\u003e\n\n  \u003cp\u003eApply content filters trained to detect likely prompt injections to all prompts sent to the LLM.\u003c/p\u003e\n\n  \u003cp\u003eChoose a more robust model: some models have been shown to be more resistant to this kind of attack than others.\u003c/p\u003e\n\n  \u003cp\u003eNone of these mitigations are sufficient to guarantee that a prompt injection attack would not succeed. Fundamentally, an LLM cannot distinguish between user input and system instructions. Both are processed by the LLM as natural language inputs so there is no way to prevent a user prompt affecting the behaviour of the LLM.\u003c/p\u003e\n\n  \u003ch6 id=\"risk-rating-2\"\u003eRisk rating\u003c/h6\u003e\n\n  \u003cp\u003eLikelihood: HIGH\u003c/p\u003e\n\n  \u003cp\u003eImpact: \u003c/p\u003e\n\n  \u003cp\u003eLOW – response is returned to a single user with limited repercussions. \u003c/p\u003e\n\n  \u003cp\u003eHIGH – response causes actual harm to a user.\u003c/p\u003e\n\n  \u003ch6 id=\"recommendation-2\"\u003eRecommendation\u003c/h6\u003e\n\n  \u003cp\u003eDeploying an LLM chatbot to a public-facing government website comes with a significant risk of a direct prompt injection attack. You should consider the impact of an attack like this in the context of the specific use case. A chatbot deployed in a limited function or in controlled conditions – by restricting the number of users, for example – is far lower risk than one that is more widely available.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch5 id=\"enterprise-ai-search-tool-summarising-emails\"\u003eEnterprise AI search tool summarising emails\u003c/h5\u003e\n\n  \u003ch6 id=\"scenario-3\"\u003eScenario\u003c/h6\u003e\n\n  \u003cp\u003eA hacker sends a malicious email attachment to a government recipient who is using an enterprise generative AI tool to assist them. The tool uses a retrieval augmented generation (RAG) pattern, searching the private data the recipient can access and sending relevant data in-context to an LLM. The tool searches the recipient’s inbox, including their unread emails and attachments, for relevant information. The tool passes the prompt injection contained in the attachment to the LLM along with other private data. The prompt injection causes the LLM to respond by summarising the private data in the form of an obfuscated link to a third-party website. When returned to the recipient, the link may, depending on the tools being used, automatically unfurl a preview and instantly exfiltrate the private data to the third-party website.\u003c/p\u003e\n\n  \u003ch6 id=\"impact-3\"\u003eImpact\u003c/h6\u003e\n\n  \u003cp\u003eData loss: confidential information contained in the user’s emails is transferred to a third party. \u003c/p\u003e\n\n  \u003cp\u003eReputational damage to the department due to loss of data.\u003c/p\u003e\n\n  \u003cp\u003eRegulatory breaches with financial consequences.\u003c/p\u003e\n\n  \u003ch6 id=\"mitigation-3\"\u003eMitigation\u003c/h6\u003e\n\n  \u003cp\u003eConfigure the enterprise AI tool so that unread emails and attachments are not included in the initial search.\u003c/p\u003e\n\n  \u003cp\u003eApply filters before the in-context data is added to the prompt to remove likely prompt injections.\u003c/p\u003e\n\n  \u003cp\u003eApply filters to the response generated by the LLM to ensure any links contained in it are only to known resources.\u003c/p\u003e\n\n  \u003cp\u003eEnsure network controls are enforced that prevent applications making calls to dangerous URLs.\u003c/p\u003e\n\n  \u003ch6 id=\"risk-rating-3\"\u003eRisk rating\u003c/h6\u003e\n\n  \u003cp\u003eLikelihood: LOW\u003c/p\u003e\n\n  \u003cp\u003eImpact: HIGH\u003c/p\u003e\n\n  \u003ch6 id=\"recommendation-3\"\u003eRecommendation\u003c/h6\u003e\n\n  \u003cp\u003eIn this scenario, indirect prompt injection in an email attachment can be used to perform data exfiltration without any action required by the user. Similar \u003ca rel=\"external\" href=\"https://embracethered.com/blog/posts/2023/google-bard-data-exfiltration/\"\u003edata exfiltration techniques have already been shown to work\u003c/a\u003e against commercial LLMs. With the increased adoption by government departments of enterprise AI tools, we will likely see more of these novel generative AI-specific cybersecurity threats.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch5 id=\"developer-uses-llm-generated-code\"\u003eDeveloper uses LLM-generated code\u003c/h5\u003e\n\n  \u003ch6 id=\"scenario-4\"\u003eScenario\u003c/h6\u003e\n\n  \u003cp\u003eA developer uses a public LLM to answer coding questions and receives advice to install a specific software package, ArangoDB, from the JavaScript package management system npm. When the LLM was trained, the package did not exist. A hacker has previously interrogated the LLM with common coding questions and identified this hallucination. They then created a malicious package with the fictitious name and registered it with the package management system. When the developer installs the package, they receive the malicious code.\u003c/p\u003e\n\n  \u003ch6 id=\"impact-4\"\u003eImpact\u003c/h6\u003e\n\n  \u003cp\u003eUnauthorised code execution when the software containing the fake package is deployed and run. This could result in significant data loss and other serious consequences.\u003c/p\u003e\n\n  \u003ch6 id=\"mitigation-4\"\u003eMitigation\u003c/h6\u003e\n\n  \u003cp\u003eDo not rely on the responses of the LLM: double-check all outputs before including them in your code. Check all package dependencies of your code before deployment. Use an automated tool such as a ‘dependabot’ or ‘snyk’ to scan for supply chain vulnerabilities.\u003c/p\u003e\n\n  \u003ch6 id=\"risk-rating-4\"\u003eRisk rating\u003c/h6\u003e\n\n  \u003cp\u003eLikelihood: LOW\u003c/p\u003e\n\n  \u003cp\u003eImpact: HIGH\u003c/p\u003e\n\n  \u003ch6 id=\"recommendation-4\"\u003eRecommendation\u003c/h6\u003e\n\n  \u003cp\u003eIf developers follow secure coding best practices, the risk should never arise because all dependencies should be checked before deployment. Over-reliance on LLM-generated code without sufficient human oversight is likely to become an increasing risk. Treat all LLM-generated code as inherently insecure and never use it directly in production code without first doing a code review.\u003c/p\u003e\n\n  \u003ch6 id=\"references\"\u003eReferences\u003c/h6\u003e\n\n  \u003cp\u003e\u003ca rel=\"external\" href=\"https://vulcan.io/blog/ai-hallucinations-package-risk\"\u003eCan you trust ChatGPT’s package recommendations?\u003c/a\u003e\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-20\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003ch5 id=\"applies-to-ai\"\u003eApplies to AI\u003c/h5\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eDesign risk-driven security, taking account of the failure modes for the type of AI you’re using – for example, \u003ca rel=\"external\" href=\"https://owasp.org/www-project-top-10-for-large-language-model-applications/\"\u003eOWASP Top 10 security risks for LLMs\u003c/a\u003e or the \u003ca rel=\"external\" href=\"https://atlas.mitre.org/matrices/ATLAS\"\u003eATLAS Matrix\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eUse a consistent risk rating methodology to assess the impact and likelihood of each risk – for example, the \u003ca rel=\"external\" href=\"https://owasp.org/www-community/OWASP_Risk_Rating_Methodology\"\u003eOWASP risk rating methodology\u003c/a\u003e.\u003c/li\u003e\n    \u003cli\u003eMinimise the attack surface by only using the AI capabilities you require – for example, by not sending user input directly to an LLM.\u003c/li\u003e\n    \u003cli\u003eDefend in depth by adding layers of security – for example, by using PET to prevent data leakage and adding content filters to sanitise output of an AI model.\u003c/li\u003e\n    \u003cli\u003eNever use private data that needs different levels of user access permissions to train or fine-tune an AI model.\u003c/li\u003e\n    \u003cli\u003eWhen building services that receive and process text, images or video, take steps to validate inputs to detect if the content has been generated by AI and could be unreliable, misleading or malicious.\u003c/li\u003e\n    \u003cli\u003eReview all enterprise access controls before deploying an AI tool to your environment to make sure users can only access the data they have permission to view. \u003c/li\u003e\n    \u003cli\u003eNever enter any official information directly into public AI applications or APIs unless it’s already publicly available or cleared for publication. Exceptions may apply for specific applications with different data handling terms provided under commercial licences, for example Microsoft Copilot.\u003c/li\u003e\n    \u003cli\u003eWhen experimenting with AI tools, pay attention to security and never assume default configurations are secure.\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003ch5 id=\"applies-to-generative-ai\"\u003eApplies to generative AI\u003c/h5\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eAvoid using generative AI where it’s not appropriate or required. Ask yourself if a non-AI solution or a traditional ML model trained for a specific purpose could work just as well.\u003c/li\u003e\n    \u003cli\u003ePrevent generative AI responses automatically leading to destructive or irreversible actions, such as sending emails or modifying records. In these situations a human must be present to review the action.\u003c/li\u003e\n    \u003cli\u003eAvoid using links to external resources in LLM responses that will be read by humans. If external links are provided, the response must be filtered to remove malicious URLs.\u003c/li\u003e\n    \u003cli\u003eTrain your users not to trust the outputs of generative AI or rely exclusively on generated responses.\u003c/li\u003e\n    \u003cli\u003eTreat all LLM-generated code as inherently insecure and never use it directly in production without code review.\u003c/li\u003e\n    \u003cli\u003eAvoid putting LLM chatbots on public-facing government websites unless the risk of direct prompt injection is acceptable under the specific use case.\u003c/li\u003e\n    \u003cli\u003eWhen hosting virtual meetings, organisers should verify the identity of all attendees and state up front that the use of third-party AI meeting transcription tools is not allowed.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Governance",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#governance",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#governance",
        "text": "\u003cp\u003eTo successfully develop an AI programme, you’ll need strong governance processes because of the risks related to lawfulness, security, bias and data. Whether these processes are already built into your existing governance frameworks or implemented as a new governance framework, they should focus on:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003econtinuous improvement through the inclusion of new knowledge, methods and technologies \u003c/li\u003e\n  \u003cli\u003eidentifying and working with important stakeholders representing different organisations and interests, including Civil Society Organisations (CSOs) and sector experts. This will help create a balanced view throughout the life cycle of any AI project or initiatives\u003c/li\u003e\n  \u003cli\u003eplanning for the long-term sustainability of AI initiatives, considering scalability, long-term support, maintenance, ongoing stakeholder involvement and future developments\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou should manage governance of AI through an AI governance board or AI expert representation on an existing governance board. You can include an ethics committee as part of your governance framework, depending on your operating context. Each has different roles and responsibilities. \u003c/p\u003e\u003ch3 id=\"ai-governance-board-or-ai-representation-on-an-existing-board\"\u003eAI governance board or AI representation on an existing board\u003c/h3\u003e\u003cp\u003eThe role of an AI governance board or representation on a board is to provide oversight, accountability and strategic guidance to help the organisation or team make informed decisions about AI adoption and use. It covers aspects such as risk management, compliance, assurance, resource allocation, stakeholder engagement, and aligning with business objectives and ethical principles.\u003c/p\u003e\u003cp\u003eAn AI governance board helps you make sure your project is on track and that strategic, legal, ethical and operational risks are managed.\u003c/p\u003e\u003ch3 id=\"ethics-committee\"\u003eEthics committee \u003c/h3\u003e\u003cp\u003eThe primary focus of an ethics committee is to assess the ethical implications of various actions, projects and decisions about AI within an organisation or programme. It evaluates AI from an ethical standpoint, focusing on values such as fairness, transparency and privacy, and is more specialised than that of an AI governance board.\u003c/p\u003e\u003cp\u003eAn ethics committee usually includes legal experts, representatives from other relevant organisations related to the service you’re delivering, community members and stakeholders – all of whom can provide a specialised perspective on ethical matters such as health or security issues. \u003c/p\u003e\u003cp\u003eBefore creating an ethics committee, you should consider the ethical, strategic and operational context of your organisation or programme. For example, the department may be too small or the programme too low risk to have a committee like this. It might be sufficient to have an AI governance board or an AI expert representative on a programme board to help you manage ethical considerations. An AI governance board should be able to guide you on whether you need an ethics committee. Refer to the Ethics section for more information.\u003c/p\u003e\u003ch3 id=\"creating-an-ai-systems-inventory\"\u003eCreating an AI systems inventory\u003c/h3\u003e\u003cp\u003eTo provide a comprehensive view of all deployed AI systems within an organisation or programme, organisations should set up an AI and machine learning (ML) systems inventory. This is in addition to the \u003ca href=\"https://www.gov.uk/government/collections/algorithmic-transparency-recording-standard-hub\"\u003eAlgorithmic Transparency Recording Standard (ATRS)\u003c/a\u003e that all government departments and certain arm’s length bodies must use to ensure public transparency around the algorithmic tools used in their decision-making processes. \u003c/p\u003e\u003cp\u003eA live inventory will help your team, your organisation and your stakeholders understand the scope and scale of AI usage. It does this by providing better oversight and awareness of any potential risks such as data quality, model accuracy, bias, security vulnerabilities and regulatory compliance. An inventory will also be helpful for audit purposes.\u003c/p\u003e\u003cp\u003eThe inventory should be regularly kept up to date with:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003ea description of each system’s purpose, usage and associated risks \u003c/li\u003e\n  \u003cli\u003edetails like data elements, ownership, development and key dates \u003c/li\u003e\n  \u003cli\u003euse protocols, structures and tools for maintaining an accurate, detailed inventory\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eYou should consider sharing your inventory with the \u003ca href=\"https://www.gov.uk/service-manual/communities/artificial-intelligence-community\"\u003eAI community of practice\u003c/a\u003e. This will enable the community to support your work and connect you with the teams that have developed similar projects across government so that you can share expertise and best practices and possibly reuse existing solutions.\u003c/p\u003e\u003ch3 id=\"governance-structures-for-teams\"\u003eGovernance structures for teams\u003c/h3\u003e\u003cp\u003eFor all programmes or services that use AI systems, teams should:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eset out how the AI model will be maintained and managed over time\u003c/li\u003e\n  \u003cli\u003edevelop a comprehensive plan for knowledge transfer, and for training new and existing staff to ensure the model’s sustainable management\u003c/li\u003e\n  \u003cli\u003eestablish clear roles and responsibilities to ensure accountability within teams, including who has the authority to change and modify the code of the AI model\u003c/li\u003e\n  \u003cli\u003eensure diversity within the project team by incorporating a range of subject matter expertise, skills and lived experiences\u003c/li\u003e\n  \u003cli\u003eestablish pathways for escalation and identify key points of contact for specific AI-related issues\u003c/li\u003e\n  \u003cli\u003eadopt a risk prioritisation plan with specific project controls throughout the delivery and post-delivery cycle, such as how you will evaluate data sets for bias\u003c/li\u003e\n  \u003cli\u003eestablish a data reporting mechanism that captures how data flows are managed and maintained throughout the delivery and post-delivery cycle\u003c/li\u003e\n  \u003cli\u003eset out how the programme or project team will work with and report to their programme board(s) and the ethics committee, if one has been set up\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"managing-risk\"\u003eManaging risk\u003c/h3\u003e\u003cp\u003eRisk management is part of governance. It helps you to strategically plan and manage your AI project to achieve objectives and respond to challenges in an agile way. \u003c/p\u003e\u003cp\u003eA risk assessment is critical in ensuring that AI projects are only undertaken if the potential benefits outweigh the risks. You should base this threshold on an objective assessment of the project’s potential risks and benefits, defining acceptable levels of risk and ensuring that any possible risks are identified and addressed early in the project life cycle. Relevant laws, regulations and ethical considerations should inform the assessment. If you’re managing AI programmes as part of a portfolio of work, \u003ca rel=\"external\" href=\"https://assets.publishing.service.gov.uk/media/6453b363c33b460012f5e6bf/Portfolio_Risk_Management_Guidance_Orange_Book_Annex.pdf\"\u003eThe Orange Book (Portfolio Risk Management Guidance (PDF, 1,958KB)\u003c/a\u003e) provides a complete overview of risks. \u003c/p\u003e\u003cp\u003eAs part of the design of your project, you should conduct a risk assessment to understand the risks and their potential to cause harm to individuals or groups, as well as the likelihood of the AI service being misused or exploited. The impact should be calculated based on the complexity of the AI system, the quality of the data used to train the system, and the potential for human error or malicious intent.\u003c/p\u003e\u003cp\u003eWhen conducting your assessment, you should consider a number of risks around AI including security, managing bias, legal and operational risks. Consider also that the scale of autonomy of an AI service can increase operational risks. For example, in the case of autonomous vehicles, the Society of Automotive Engineers’ \u003ca rel=\"external\" href=\"https://www.sae.org/blog/sae-j3016-update\"\u003eLevels of Driving Automation\u003c/a\u003e ranks autonomy on a scale from 0 (no autonomy) to 5 (full automation for all features under all conditions). This scale correlates to the level of risk. \u003c/p\u003e\u003cp\u003eAlongside the risk assessment, you need to create a robust risk management framework that sets out defined roles and responsibilities and includes clear escalation routes to help mitigate risks.\u003c/p\u003e\u003ch4 id=\"mitigating-risks\"\u003eMitigating risks\u003c/h4\u003e\u003cp\u003eYou can mitigate some risks related to how the AI service performs by building or establishing programme and technical guardrails (best practices). These will guide the design, implementation and operation of an AI service or application, and are an essential element of delivering great services.\u003c/p\u003e\u003cp\u003eIn the case of autonomous AI services that make decisions in areas such as social care or healthcare, the impact of autonomy of an AI service can be mitigated by including human intervention. These decisions need to be made in a controlled environment so as to not reintroduce bias into the AI service.\u003c/p\u003e\u003cp\u003eWhether your AI service is autonomous or includes elements of human intervention, it should be evaluated throughout the all stages of the project life cycle – including design, development and operation. Your risks and mitigation strategy should also cover how your team will manage continuous performance monitoring to prevent biased or inaccurate outputs.\u003c/p\u003e\u003cp\u003eThere are also security and data protection risks which are covered in detail in the Security and Data protection and privacy sections.\u003c/p\u003e\u003ch3 id=\"ai-quality-assurance\"\u003eAI quality assurance\u003c/h3\u003e\u003cp\u003eAI quality assurance ensures that the AI service meets the service level requirements and provides evidence that the service is fit for purpose. It helps you check that robust techniques have been used to build, test, measure and evaluate AI systems. It also helps organisations communicate that their systems are trustworthy and aligned with relevant regulatory principles. It should be used throughout the AI service life cycle, including during testing and validation in the development phase and monitoring once the AI service is being used.\u003c/p\u003e\u003cp\u003eTo meet quality assurance requirements, AI systems must be trustworthy, accountable, transparent and robust. They must ensure safety, respect privacy, mitigate bias, ensure fairness, and be secure and resilient. Given the complexity of AI systems, you may require a toolbox of different products, services and standards to ensure their effectiveness. For example, the Department for Science, Innovation, and Technology (DSIT)’s \u003ca rel=\"external\" href=\"https://assets.publishing.service.gov.uk/media/65ccf508c96cf3000c6a37a1/Introduction_to_AI_Assurance.pdf\"\u003eIntroduction to AI assurance (PDF, 1,419KB)\u003c/a\u003e identifies the key elements of an assurance process – including risk assessment, impact assessment, bias audit, compliance audit, conformity assessment and formal verification. \u003c/p\u003e\u003ch4 id=\"validation-of-ai\"\u003eValidation of AI\u003c/h4\u003e\u003cp\u003eBeing able to assert the quality of an AI service is critical to ensuring the safety of the system and the reliable accuracy of the service.\u003c/p\u003e\u003cp\u003eYou must ensure that any updates to the AI system have a quantitative testing and validation process as part of the change control process. Validation is part of the testing of an AI system and is the ‘confirmation, through the provision of objective evidence, that the requirements for a specific intended use or application have been fulfilled’ (source: \u003ca rel=\"external\" href=\"https://www.iso.org/standard/45481.html\"\u003eISO9000:2015\u003c/a\u003e).\u003c/p\u003e\u003cp\u003eDeployment of AI systems that are inaccurate, unreliable, or poorly generalised to data beyond their training creates and increases negative AI risks and reduces trustworthiness. You should consider the complexity of your AI systems and identify the different products, services and standards necessary to ensure their effectiveness. To do so, you must make sure that these products and services comply with the required standards as defined by standards development organisations (SDOs), such as the \u003ca rel=\"external\" href=\"https://www.iso.org/home.html\"\u003eInternational Standards Organisation (ISO)\u003c/a\u003e.\u003c/p\u003e\u003ch4 id=\"operational-monitoring\"\u003eOperational monitoring\u003c/h4\u003e\u003cp\u003eOnce you’ve released your AI system for use and it’s operational, you should have ongoing performance monitoring in place. This will ensure your system is operating as expected, and you should be able to provide evidence of this. It will also help you to identify and manage any changes to the model.\u003c/p\u003e\u003cp\u003eAny updates to the model need to go through a managed release process. This will help you mitigate the impact of any process change and clearly document changes made for future reference. You should ensure that the release can be withdrawn and the system reverted to an earlier version if required.\u003c/p\u003e\u003cp\u003eAs systems and environments evolve, the current process may diverge sufficiently from the training period of the AI system. This is known as model drift and may require retraining or implementation of a new model within the AI system. Close monitoring is essential so that you can catch this as early as possible and reduce possible disruption to the AI system.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"practical-recommendations-21\"\u003ePractical recommendations\u003c/h4\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eConnect with your organisation’s assurance team and review the \u003ca href=\"https://www.gov.uk/guidance/cdei-portfolio-of-ai-assurance-techniques\"\u003ePortfolio of AI assurance techniques\u003c/a\u003e. \u003c/li\u003e\n    \u003cli\u003eSet up an AI governance board or include AI experts on existing governance boards.\u003c/li\u003e\n    \u003cli\u003eConsider setting up an ethics committee made up of internal stakeholders, cross-government stakeholders, sector experts and external stakeholders like Civil Society Organisations. \u003c/li\u003e\n    \u003cli\u003eSet up an AI/ML systems inventory to provide a comprehensive view of all deployed AI systems within your department. \u003c/li\u003e\n    \u003cli\u003eMake sure your programme or project teams have clear governance structures in place.\u003c/li\u003e\n    \u003cli\u003eEvaluate your AI product throughout the development and project life cycle, identify risks and implement a robust mitigation strategy.\u003c/li\u003e\n    \u003cli\u003eUse quality assurance techniques to make sure your AI product is trustworthy, accountable, transparent, robust, secure and resilient, and respects privacy, mitigates bias and ensures fairness.\u003c/li\u003e\n    \u003cli\u003eMake full use of the training resources available, including the courses on \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/oPEgywmEQumlVAmXlgnRqw\"\u003ethe business value of AI\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://learn.civilservice.gov.uk/courses/f7Sf3JPkTQiwYgr2qatDEw\"\u003eunderstanding AI ethics\u003c/a\u003e on Civil Service Learning.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Appendix: example AI use cases in the public sector ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#appendix-example-ai-use-cases-in-the-public-sector",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#appendix-example-ai-use-cases-in-the-public-sector",
        "text": "\u003cp\u003eThis section includes sample case studies provided by teams that have implemented AI solutions in government departments and public sector organisations. These are real-life examples of AI adoption, presenting the technologies that were deployed, reflections on their capabilities and shortcomings, and discussions of the challenges and risks encountered in each project. \u003c/p\u003e\u003cp\u003eBe aware that these case studies were submitted in spring 2024 and only discuss the aspects of AI projects that each team deemed most relevant and interesting. They are not exhaustive and should not be treated as formal advice. You can find guidance in the Building AI solutions section of this playbook.\u003c/p\u003e\u003cp\u003eYou’re encouraged to share your AI project with the \u003ca href=\"https://www.gov.uk/service-manual/communities/artificial-intelligence-community\"\u003eAI community of practice\u003c/a\u003e to be considered for future updates of this appendix. This will also enable the community to connect you with the teams that have developed similar projects in other departments so that you can share expertise and best practice.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "GOV.UK Chat: experimenting with generative AI ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#govuk-chat-experimenting-with-generative-ai",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#govuk-chat-experimenting-with-generative-ai",
        "text": "\u003cp\u003eGOV.UK Chat is a pilot tool that uses relevant website content to generate responses, aiming to simplify navigation across more than 700,000 pages on GOV.UK and help users find the information they need. The project highlighted the importance of a careful and phased development approach. The experience gained from this prototype led the Government Digital Service (GDS) to focus on enhancing the system’s accuracy with the objective of launching a limited public pilot, provided that accuracy thresholds could be met.\u003c/p\u003e\u003ch3 id=\"our-tool-and-the-problem-it-solves\"\u003eOur tool and the problem it solves\u003c/h3\u003e\u003cp\u003eGOV.UK Chat uses a retrieval augmented generation (RAG) approach. This allows users to engage with GOV.UK content through natural language queries. We preferred an RAG approach to fine-tuning a large language model (LLM) because GOV.UK content is updated regularly with edits and new pages published. The initial version of GOV.UK Chat was set up to test how we could improve user interaction with our website. We chose ‘business’ as our focus area because it’s complex for a user, involving many different departments’ policies and content. \u003c/p\u003e\u003ch3 id=\"project-development\"\u003eProject development\u003c/h3\u003e\u003cp\u003eThe project was managed through a series of phased experiments, each lasting a couple of weeks and focusing on iterative development and evaluation. This approach facilitated controlled experimentation and rapid data gathering for system refinement.\u003c/p\u003e\u003ch3 id=\"measuring-success\"\u003eMeasuring success\u003c/h3\u003e\u003cp\u003eThe efficacy of GOV.UK Chat was evaluated using a multifaceted approach to ensure the reliability and accuracy of its responses. Content designers from GOV.UK and across government assessed the answers generated by the system. This involved exploring how GOV.UK Chat processed user queries, understood the intent behind questions, and retrieved relevant GOV.UK pages to inform its responses. \u003c/p\u003e\u003cp\u003eExperts then evaluated the appropriateness of the questions posed, the system’s interpretation of them, and the factual accuracy and completeness of answers provided.\u003c/p\u003e\u003ch3 id=\"value-delivered\"\u003eValue delivered\u003c/h3\u003e\u003cp\u003eAvailable through specific ‘magic links’, GOV.UK Chat was tested by hundreds of users in a controlled environment. Feedback suggested a preference for GOV.UK Chat over traditional search and navigation methods. This highlighted the convenience of direct question-answering systems, particularly for users with more complex questions. \u003c/p\u003e\u003cp\u003eHowever, the experiment also raised concerns about the reliability of the information provided, as the system occasionally produced ‘hallucinated’ responses. \u003c/p\u003e\u003cp\u003eSatisfaction surveys were sent out to users after use. Notably, nearly 70% of users found the chatbot’s responses useful and about 65% were satisfied with their experience. The primary value of this experiment is in what we learned from observing user interactions, understanding the types of queries posed, common failure modes, and assessing the feasibility of integrating such a system into GOV.UK more broadly.\u003c/p\u003e\u003ch3 id=\"technologies\"\u003eTechnologies \u003c/h3\u003e\u003cp\u003eThe development of GOV.UK Chat was underpinned by a selection of modern cloud and AI technologies, primarily hosted on Google Cloud. This choice was not exclusive, in that similar setups could be effectively implemented on alternative, functionally equivalent cloud services. \u003c/p\u003e\u003cp\u003eThe answer-generation part of the system was powered by direct application programming interface (API) calls to OpenAI, leveraging its advanced generative LLMs for dynamic answer generation. Additionally, we used a \u003ca rel=\"external\" href=\"https://qdrant.tech/\"\u003eQdrant vector store\u003c/a\u003e to facilitate efficient data retrieval and management to support the RAG approach.\u003c/p\u003e\u003ch3 id=\"challenges-and-solutions\"\u003eChallenges and solutions\u003c/h3\u003e\u003cp\u003eIt’s important to acknowledge the rapid evolution of the technology in the generative AI domain. When the GOV.UK Chat system was initially conceived and developed (July 2023), the options for technologies like this were relatively limited. \u003c/p\u003e\u003cp\u003eIn our first experiment we used a simplistic approach to retrieval, whereby we returned whole pages. This approach occasionally resulted in errors because a niche search might return lots of very long pages that would exceed the LLM’s token limit, resulting in an error.\u003c/p\u003e\u003cp\u003eIn response, we started exploring improvement strategies, specifically through nuanced chunking, alternative embedding models, vectorisation strategies, re-ranking and improved few-shot examples in the prompt. With an answer accuracy of 80%, the experiment suggested that accuracy gains would be required prior to the product being deployed for users on the site.\u003c/p\u003e\u003cp\u003eIf we’re to scale GOV.UK Chat into a full live pilot, a major challenge will be quality assurance. The techniques we used in the first stage of the work are highly manual and not practical to scale. We believe a solution will be to build a knowledge base of quality-assessed questions so that a semi-automated quality assurance mechanism can be developed.\u003c/p\u003e\u003ch3 id=\"example-ethical-legal-and-security-considerations\"\u003eExample ethical, legal and security considerations\u003c/h3\u003e\u003cp\u003eSafeguards were implemented to protect user privacy and prevent personal data submission. We complied with UK data protection legislation and conducted a thorough data protection impact assessment, removing any personal data from GOV.UK pages accessible to the LLM. Additionally, collaboration with cyber security experts from CDDO, Number 10DS and i.AI through ‘red teaming’ helped identify and mitigate system vulnerabilities, reinforcing our commitment to responsible and secure technology deployment. For more details, refer to the \u003ca rel=\"external\" href=\"https://insidegovuk.blog.gov.uk/2024/01/18/the-findings-of-our-first-generative-ai-experiment-gov-uk-chat/\"\u003eGOV.UK Chat blog\u003c/a\u003e.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "GOV.UK Chat: doing user research for AI products ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#govuk-chat-doing-user-research-for-ai-products",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#govuk-chat-doing-user-research-for-ai-products",
        "text": "\u003cp\u003eThis case study focuses on the user research we conducted in the Government Digital Service (GDS) to help the team developing the GOV.UK Chat tool explore user reactions and evaluate the system with experts. As an experimental team, we also wanted to learn about appropriate tools and approaches to develop GOV.UK Chat.\u003c/p\u003e\u003cp\u003eInitially, we focused on testing the accuracy of the tool using surveys, presenting typical user questions and answers for subject matter experts (SMEs) to evaluate. We supported this assessment with a round of online testing with internal users from the content design community to gain feedback on the initial design of the chatbot interface. This approach allowed us to develop the system safely before showing it to business users. Data was collected using a survey, where SMEs evaluated a series of questions and answers without being told which answers were from the large language model (LLM) underpinning GOV.UK Chat and which were human. Accuracy ratings were collected via Likert scales supported by qualitative comments. \u003c/p\u003e\u003cp\u003eWe needed to determine if the SMEs agreed about their assigned ratings, as this would tell us about the reliability of the data. We calculated the level of agreement and found it to be high, indicating that SME ratings were consistent. We were then able to compare the ratings for the LLM with the human model answers to see if they were significantly different. We found no significant differences – the accuracy of the LLM was rated on par with the human model answers.\u003c/p\u003e\u003cp\u003eA subsequent round of online testing provided further insights on accuracy from live queries, and helped us examine the tool’s conversational memory. During remote testing, internal users from our content design community interacted with GOV.UK Chat to find information to support a fictitious user who was planning to start a business. After the interaction, participants completed a short survey about their experience. We analysed the test session videos and triangulated this data with their survey responses.\u003c/p\u003e\u003ch3 id=\"project-development-1\"\u003eProject development\u003c/h3\u003e\u003cp\u003eWe worked with users already in a business, and those about to start one. Our testing was conducted in 2 phases. First, we conducted a qualitative study that combined an interview with a usability test of the GOV.UK Chat prototype. This allowed us to explore users’ attitudes and experiences with AI tools, gain feedback on our tool and further assess accuracy.\u003c/p\u003e\u003cp\u003eIn the second phase of testing, we invited 1,000 users to test GOV.UK Chat via a research banner on GOV.UK. Users interacted with the tool, providing message-level feedback on answer usefulness, and completed a short survey about their experience. During both rounds of testing with business users, we repeatedly assessed accuracy by gathering SME ratings of the LLM’s answers.\u003c/p\u003e\u003cp\u003eThe scaled-up testing was an important step. Having a large data set for analysis added weight and confidence to how we were assessing accuracy. It also provided an opportunity to examine the types of questions that users might typically ask, and, importantly, how those questions were phrased. This would help us understand to what extent users required support in articulating their questions. Finally, scaled-up testing allowed us to combine our understanding of accuracy with users’ feedback on the experience of using the tool.\u003c/p\u003e\u003ch3 id=\"value-delivered-1\"\u003eValue delivered\u003c/h3\u003e\u003cp\u003eOverall, 69% of users thought the tool was useful or very useful. We also achieved an accuracy threshold of 80%.\u003c/p\u003e\u003ch3 id=\"challenges-and-solutions-1\"\u003eChallenges and solutions\u003c/h3\u003e\u003cp\u003eOur research approach was inspired by a desire to safely and responsibly test the LLM before showing it to users. However, this was a labour-intensive approach. Each investigation required us to quantify accuracy alongside exploring the user experience, sometimes uncovering broader insights about the propositional value of the tool.\u003c/p\u003e\u003cp\u003eThe scaled testing was particularly useful in combining different data sources, such as survey ratings, question types, participant feedback about whether the generated answer was useful, accuracy of answers (evaluated by SMEs), and response times. To not dissuade users from completing the survey, we kept it short and asked a small number of questions.\u003c/p\u003e\u003cp\u003eIn future live-scaled testing, we plan to bolster this data with live intercept interviews, speaking to business users who are using GOV.UK and GOV.UK Chat to solve their emerging information needs. This will help us to explore their experience and attitudes to the tool in more detail. \u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "CCS commercial agreement recommendation system ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#ccs-commercial-agreement-recommendation-system",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#ccs-commercial-agreement-recommendation-system",
        "text": "\u003cp\u003eThe Crown Commercial Service (CCS) built a commercial agreement recommendation system using spend and customer market segmentation data. Our goal was to generate relevant agreement recommendations for customers to help them discover new agreements by considering their past procurement activity and their market sector peers’ activity. The system was inspired by the technologies used by companies such as Netflix to deliver intelligent and personalised recommendations for users based on their historical data.\u003c/p\u003e\u003ch3 id=\"our-tool-and-the-problem-it-solves-1\"\u003eOur tool and the problem it solves\u003c/h3\u003e\u003cp\u003eWhen a customer uses one of our hundreds of agreement lots to procure products and services, the supplier provides data about that transaction. Over time, we collate these submissions into a historical customer-product interaction data set.\u003c/p\u003e\u003cp\u003eA machine learning (ML) recommendation system trained on data like this can provide relevant and detailed recommendations, operate without customer or staff input, keep pace with evolving behaviour, and take into account other useful information.\u003c/p\u003e\u003cp\u003eThis recommendation system focuses on discovery, generating recommendations in the form of agreements that are new to each customer, which makes them aware of relevant agreements that they might not have known about or found otherwise.\u003c/p\u003e\u003ch3 id=\"project-development-2\"\u003eProject development\u003c/h3\u003e\u003cp\u003eIn the absence of an online digital platform ready for recommendation serving, we used this configuration to enable a small team to rapidly develop a pilot solution using historical data that could be served to customers through email marketing or during conversations. Once this platform was ready, new challenges appeared, including how to ensure continuity of recommendation serving and allow for a richer feedback loop by measuring interactions with recommendations in the digital service.\u003c/p\u003e\u003ch3 id=\"measuring-success-1\"\u003eMeasuring success\u003c/h3\u003e\u003cp\u003eA number of customers were given relevant recommendations and an equivalent number in a control group were not. Acting on these recommendations translated to an increase in the average number of unique agreements used by those customers compared to the control group. This is because we made customers aware of new agreements beyond those which they currently use.\u003c/p\u003e\u003ch3 id=\"value-delivered-2\"\u003eValue delivered\u003c/h3\u003e\u003cp\u003eWe now have the potential to provide commercial agreement recommendations to many more customers. This was previously limited to customers with dedicated account managers who have specialist customer knowledge. The ultimate goal is to integrate this system into an online digital platform where customers can access their recommendations at their convenience.\u003c/p\u003e\u003ch3 id=\"technologies-1\"\u003eTechnologies\u003c/h3\u003e\u003cp\u003eThe algorithm we used to build our tool is based on \u003ca rel=\"external\" href=\"https://www.tensorflow.org/recommenders\"\u003eTensorFlow Recommenders\u003c/a\u003e and consists of a ‘two-tower’ neural network (NN). This is a popular architecture for modern recommendation systems. When training, each NN learns the embeddings for the transaction context and candidate products so that the combination of these embeddings correlates with the observed affinity of that context and candidate in the training data. This then allows the embedding of any number of potential candidates, which are ranked by mathematical distance from a given context to get recommendations.\u003c/p\u003e\u003ch3 id=\"challenges-and-solutions-2\"\u003eChallenges and solutions\u003c/h3\u003e\u003cp\u003eOffline historical data of customer-product interactions allows the reconstruction of the buying behaviour of a customer through time and the comparison of this to generated recommendations. These simple offline ‘hit’ metrics can be flawed in a discovery context.\u003c/p\u003e\u003cp\u003eIn our interaction data set, customers generally use the same products with some variability. A lack of discovery ‘hits’ with what a customer interacted with next is therefore not necessarily a bad thing, as we are specifically trying to predict what a customer has not interacted with yet. This causes challenges when assessing the performance of a discovery-based recommendation system using offline data only.\u003c/p\u003e\u003ch3 id=\"example-ethical-legal-and-security-considerations-1\"\u003eExample ethical, legal and security considerations\u003c/h3\u003e\u003cp\u003eAs our customer organisations are legal entities and not people, no personal data is used when making recommendations. Additionally, we use objective past behaviour as a guide to produce recommendations for consideration only by customer organisations.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Digital Sensitivity Review at FCDO Services ",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#digital-sensitivity-review-at-fcdo-services",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#digital-sensitivity-review-at-fcdo-services",
        "text": "\u003cp\u003eFCDO Services has developed an AI-powered Digital Sensitivity Review toolset. This enables departments to select, review and transfer digital records consistently and securely for permanent preservation at The National Archives (TNA), meeting the requirements of the \u003ca rel=\"external\" href=\"https://www.nationalarchives.gov.uk/information-management/legislation/public-records-act/\"\u003ePublic Records Act\u003c/a\u003e.\u003c/p\u003e\u003ch3 id=\"our-toolset-and-the-problem-it-solves\"\u003eOur toolset and the problem it solves\u003c/h3\u003e\u003cp\u003eThe Digital Sensitivity Review toolset uses AI to significantly increase the efficiency, effectiveness and risk reduction for the review and transfer of digital records. Digital records present several risks, as hidden data within records often contain sensitivities. For example, it’s common to insert cropped images into documents, but users may not realise that the original uncropped image is still present and easily recoverable.\u003c/p\u003e\u003cp\u003eAs there’s a limited supply of trained sensitivity reviewers, our toolset helps departments to detect issues like this and better cope with the increasing volume and complexity of digital records. Techniques that reduce digital volume, such as ephemeral identification, duplicate identification and clustering, are essential to this process. So we designed specific solutions for each of the stages involved in the transfer of digital records.\u003c/p\u003e\u003ch3 id=\"project-development-3\"\u003eProject development\u003c/h3\u003e\u003cp\u003eTo develop this project, we established Team Cicero as a collaborative working arrangement in accordance with \u003ca rel=\"external\" href=\"https://www.iso.org/standard/72798.html\"\u003eISO 44001\u003c/a\u003e. This enabled working with specialist partners in industry and Sheffield, Glasgow and Loughborough Universities, who are leading cutting-edge research into topics such as lifelong machine learning (ML) techniques.\u003c/p\u003e\u003cp\u003eWe defined the target outcome based on the volume and complexity of the unstructured data, commonly called the ‘digital heap’, and established a set of challenges. Integration of the technologies and the users into one system was a consideration from the outset, along with commercial licensing mechanisms. We decided to use off-the-shelf software wherever possible and ensure that the system was modular in design, making it easy to upgrade. \u003c/p\u003e\u003ch3 id=\"measuring-success-2\"\u003eMeasuring success\u003c/h3\u003e\u003cp\u003eOur 2 key metrics were the number of sensitivity reviewers needed to achieve the target risk appetite for a given volume and complexity without changing records selection policy, and the risk of release of sensitive content. \u003c/p\u003e\u003cp\u003eWithout this technology, we would require over 100 additional operations team members, including sensitivity reviewers, to meet the demands of digital review and transfer for one government department. Moreover, to meet the volume of digital records requiring review, we would require a system that worked at least one order of magnitude faster than the equivalent paper review system.\u003c/p\u003e\u003ch3 id=\"value-delivered-3\"\u003eValue delivered\u003c/h3\u003e\u003cp\u003eWe have been able to process records with 10% of the sensitivity review effort and reduce the risk of release of sensitive content to well below what was achievable without the use of this technology.\u003c/p\u003e\u003ch3 id=\"challenges-and-solutions-3\"\u003eChallenges and solutions\u003c/h3\u003e\u003cp\u003eML techniques require considerable volumes of examples to train the model, but the limited amount of material actually redacted so far is small. Of the records selected for review, typically less than 1% of the documents reviewed have any redaction applied. This becomes a statistical constraint on the use of ML, because sourcing the maximum possible volume of sensitive material is fundamental to maximising the potential of the system. \u003c/p\u003e\u003cp\u003eThis problem is exacerbated by the fact that different technologies are more suited to different \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2000/36/contents\"\u003eFreedom of Information Act\u003c/a\u003e exemption categories. For example, international relations sensitivities are fundamentally different to those involving personal information. Different departments applying differing policies reduces the availability of statistically significant volumes. Policy examples include the definitions of duplicate, ephemeral and sensitive content.\u003c/p\u003e\u003ch3 id=\"example-ethical-legal-and-security-considerations-2\"\u003eExample ethical, legal and security considerations\u003c/h3\u003e\u003cp\u003eOur system is aligned with security considerations required by different departments and we considered several legal and ethical issues. For example, the technologies are currently used only to assist human decisions and our reviewers are fully trained to understand the ethical and legal aspects of their work. The technology augments the reviewer’s ability to process a greater volume of material than would otherwise be possible, but it does not remove or replace the responsibilities of the reviewer. \u003c/p\u003e\u003cp\u003eWe have also identified a potential challenge around the ability of departments to keep sensitive content post-transfer, and to provide statistically meaningful volumes for ML, with differing opinions on the legality of doing so. We believe this needs to be discussed further if ML is to reach its full potential in sensitivity reviews.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "NHS user research finder",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#nhs-user-research-finder",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#nhs-user-research-finder",
        "text": "\u003cp\u003eThis project focuses on creating a tool to help colleagues find user research completed by other teams across the organisation.\u003c/p\u003e\u003ch3 id=\"our-tool-and-the-problem-it-solves-2\"\u003eOur tool and the problem it solves\u003c/h3\u003e\u003cp\u003eOur tool allows users to both upload and search for user research. Users can upload user research into our database, which is then summarised using a commercial large language model (LLM) through an application programming interface (API). The user can review and edit the summary generated by AI before confirming the submission. After a review from the user-centred design ops team, the report summary is then published on the tool. Users can query this database using natural language which is interpreted by the same API.\u003c/p\u003e\u003cp\u003eThe tool allows users to enter text search queries such as:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e“What do we know about 111 online?”\u003c/li\u003e\n  \u003cli\u003e“What do we know about records access?”\u003c/li\u003e\n  \u003cli\u003e“What do GP practices need from patient facing apps?”\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eUsers are then presented with research summaries and relevant contacts from the publishing team. \u003c/p\u003e\u003ch3 id=\"project-development-4\"\u003eProject development\u003c/h3\u003e\u003cp\u003eWe developed this tool over a 13-week period with a commercial partner specialising in AI. It’s currently in private beta testing. We worked using typical agile methodologies and conducted user research and testing throughout.\u003c/p\u003e\u003ch3 id=\"measuring-success-3\"\u003eMeasuring success\u003c/h3\u003e\u003cp\u003eWe’re measuring the success of the tool through a range of key performance indicators (KPIs) including usage, uploads, searches, search queries, satisfaction and completion rate. We’re also conducting user research to measure ease of use and effectiveness at responding to the original user need. \u003c/p\u003e\u003ch3 id=\"value-delivered-4\"\u003eValue delivered\u003c/h3\u003e\u003cp\u003eOur tool makes user research more easily findable and visible. If we can refine the summarising capabilities of the tool within the parameters of our information governance, this will save considerable time for user researchers and minimise the risk that relevant user research is neglected or work is duplicated.\u003c/p\u003e\u003ch3 id=\"challenges-and-solutions-4\"\u003eChallenges and solutions\u003c/h3\u003e\u003cp\u003eThe tool makes use of an API to summarise information using an LLM. Limitations and shortcomings include the fact that the AI currently does not always recognise all the sections within a given research output, which then requires the user to fill in the gaps. \u003c/p\u003e\u003cp\u003eAnother unexpected challenge is the way the AI summarises reports. When asked to process the same report several times in succession, the tool will often return varied results. The same is true for the search. Furthermore, when asked for very long, specific or complex search queries, the LLM might return only vaguely related results (instead of nothing).\u003c/p\u003e\u003cp\u003eFor example, the query: “Has anyone done any research on how young people (so around the ages of 11 to 16 or 18), feel about accessing healthcare, understand how it works (like how to see a GP or book an appointment, for example), their parents’ involvement in managing their healthcare tasks and when there might be a ‘handing over of the baton’ to the young person to managing it?” returns results about a 111 online discovery with parents of children under 5 years old, as well as general research around adult proxy patients’ user needs.\u003c/p\u003e\u003cp\u003eWe are inherently reliant on a third-party API for the tool to function. In early prototyping we attempted to run an in-house LLM. However, the quality of the results was not high enough given the time we had available to develop the prototype. This could be explored further.\u003c/p\u003e\u003ch3 id=\"example-ethical-legal-and-security-considerations-3\"\u003eExample ethical, legal and security considerations\u003c/h3\u003e\u003cp\u003eThe development of a tool to aggregate and query user research using a commercial LLM for summarisation and search functionalities requires careful consideration of ethics and security. From an ethical perspective, we considered potential accuracy and bias issues of AI-generated summaries. These can impact the integrity of user research. \u003c/p\u003e\u003cp\u003ePrivacy and legal concerns are also significant because user research often contains sensitive information. Ensuring the LLM does not compromise participant confidentiality or contravene data protection laws is paramount. The use of a commercial LLM involves checking the security of the systems deployed as well as navigating copyright and intellectual property rights. In particular, the project’s reliance on a third-party API for critical functions introduces dependencies and risks related to data security, service continuity, and control over data processing. This necessitates rigorous vendor assessment and contract management to safeguard organisational and user interests. \u003c/p\u003e\u003cp\u003eFurthermore, the variation in AI-generated outputs and the challenge in handling complex queries underline the need for continuous monitoring, user feedback integration, and iterative improvement to effectively address ethical, privacy and legal concerns.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "NHS.UK reviews: an automated reviews moderator",
      "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#nhsuk-reviews-an-automated-reviews-moderator",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government/artificial-intelligence-playbook-for-the-uk-government-html#nhsuk-reviews-an-automated-reviews-moderator",
        "text": "\u003cp\u003eAs operators of the NHS.UK website, we need to moderate hundreds of thousands of reviews of NHS services to make sure they meet our guidelines about personal information, abuse, discrimination and other policies. Data scientists from NHS England have automated the review moderation process using natural language processing (NLP) techniques to build bespoke models to implement our policies. \u003c/p\u003e\u003ch3 id=\"our-tool-and-the-problem-it-solves-3\"\u003eOur tool and the problem it solves\u003c/h3\u003e\u003cp\u003eWe developed an AI tool to automatically identify reviews that contain suicidal and self-harm ideation content; serious issues which should be formal complaints rather than a review; profanity; email addresses; descriptions of a person; names; fully capitalised text; URLs; and text that doesn’t describe an experience. Our AI solution is scalable, trackable and quick. It has a low running cost and improves user experience, as reviews are moderated more quickly and consistently.\u003c/p\u003e\u003ch3 id=\"project-development-5\"\u003eProject development\u003c/h3\u003e\u003cp\u003eTo deliver this project we deployed a multidisciplinary team. Initially, we developed the tool on a flask app using named entity recognition, part of speech tagging, lookups and simple regex rules. This phase helped to establish a robust architecture and to integrate with the NHS.UK platform where the tool was to be used. Following this, we developed more complicated machine learning (ML) models that required good quality training data, and time to train, test and evaluate. \u003c/p\u003e\u003ch3 id=\"measuring-success-4\"\u003eMeasuring success\u003c/h3\u003e\u003cp\u003eWe measured the success of our AI solution through reductions in solution cost, moderation time, and a reduction in the proportion of unpublishable reviews. Our ML models were also tested through:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003econfusion matrices: to describe the outcomes of the models against the test data set. We used balanced test data sets and data sets which represented real-world data volumes to give an idea of real-world performance\u003c/li\u003e\n  \u003cli\u003eclerical review: false positives and false negatives were subjected to a more detailed review. This allowed us to understand edge cases where the models were underperforming, and identify opportunities to fine-tune\u003c/li\u003e\n  \u003cli\u003enon-functional testing: metrics included latency, throughput and spin-up time for compute. These tests allowed us to understand whether we would see degradation of the service over a range of likely and extreme scenarios, and allowed us to assure the product owner that the solution was fit for use\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"value-delivered-5\"\u003eValue delivered\u003c/h3\u003e\u003cp\u003eThe use of AI technologies removed the need for a third-party moderation service, reducing cost, increasing accuracy, and enabling scalability of moderation through automation. It also decreased the time from review submission to publication, improving patient feedback. The immediate feedback provided by the AI system has resulted in an increased percentage of published reviews, a decrease of rejected reviews, and the ability for the NHS team to easily add moderation rules in future based on requirements and reviewing all comments accordingly. \u003c/p\u003e\u003ch3 id=\"technologies-2\"\u003eTechnologies\u003c/h3\u003e\u003cp\u003eThe auto moderation software was written in Python and hosted on our virtual private cloud service. It consists of a flask app which handles API calls and payloads to hosted ML inference models. The API call passes the content to the model and the model returns a JSON payload which informs the NHS.UK system how to handle the content. The tool uses a combination of regex and NLP methods, and pretrained embedding models like BERT, bag of word and MNET.\u003c/p\u003e\u003ch3 id=\"challenges-and-solutions-5\"\u003eChallenges and solutions\u003c/h3\u003e\u003cp\u003eThe use of these technologies in the NHS is not widespread and we had to discover how to sign off a solution which contained AI. The compartmentalised nature of the solution helped, as we were able to assign risk to components and get them signed off in isolation before signing off the solution as a whole.\u003c/p\u003e\u003cp\u003eThe probabilistic nature of the AI models needed clear explanation and rigorous testing to ensure we were not introducing additional risks to the service. Additionally, one model faced the challenge of insufficient training data as it targeted a less common reason for review rejection – safeguarding concerns. We addressed this by augmenting our data set through the generation of synthetic data using NLP tools, which was then validated by expert moderators to ensure its quality and relevance.\u003c/p\u003e\u003ch3 id=\"example-ethical-legal-and-security-considerations-4\"\u003eExample ethical, legal and security considerations\u003c/h3\u003e\u003cp\u003eOur product involves dealing with user feedback that potentially contains personal or sensitive information. This required us to pay particular attention to ethical legal and security issues throughout the project life cycle. Our data processing complies with the Data Protection Act 2018. \u003c/p\u003e\u003cp\u003eWe have made provisions for people to request a human review if they are not content with the automated response. This ensures that human operators are involved in the process and can monitor the outcomes of the automated review of comments.\u003c/p\u003e\u003cp\u003eWe paid particular attention to ethical concerns. For example, we developed an effective suicidal and self-harm content detector. We undertook a clinical safety exercise to ensure clinical risks were elaborated and appropriately mitigated. In addition, a bias analysis was conducted on the 3 models trained by the team for the auto moderation tool: safeguarding, complaints and ‘not an experience’. The analysis aimed to identify any potential bias in these models by examining the sentiment of misclassified comments, the length of the texts, and the number of spelling mistakes standardised by text length.\u003c/p\u003e\u003cp\u003eOur solutions assurance team assessed the solution with a particular focus on training data coverage and model behaviour, examining the extent to which literacy levels in reviews affected the model outcomes. Our technical review group – a panel of experts with a wide range of expertise – reviewed the solution and made recommendations regarding improved security and solution architecture.\u003c/p\u003e"
      }
    }
  ]
}
</script><script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "item": {
        "name": "Home",
        "@id": "https://www.gov.uk/"
      }
    },
    {
      "@type": "ListItem",
      "position": 2,
      "item": {
        "name": "Government",
        "@id": "https://www.gov.uk/government/all"
      }
    },
    {
      "@type": "ListItem",
      "position": 3,
      "item": {
        "name": "AI Playbook for the UK Government",
        "@id": "https://www.gov.uk/government/publications/ai-playbook-for-the-uk-government"
      }
    }
  ]
}
</script>


</body></html>