<!DOCTYPE html><html class="govuk-template govuk-template--rebranded" lang="en"><head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta property="og:description" content="">
<meta property="og:title" content="A pro-innovation approach to AI regulation">
<meta property="og:url" content="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">
<meta property="og:type" content="article">
<meta property="og:site_name" content="GOV.UK">
<meta name="twitter:card" content="summary">
<meta name="govuk:organisations" content="&lt;D1381&gt;&lt;OT1278&gt;">
<meta name="govuk:ga4-publishing-government" content="2022 to 2024 Sunak Conservative government">
<meta name="govuk:ga4-political-status" content="historic">
<meta name="govuk:primary-publishing-organisation" content="Department for Science, Innovation and Technology">
<meta name="govuk:public-updated-at" content="2023-08-03T10:29:45+01:00">
<meta name="govuk:updated-at" content="2025-11-07T13:11:15+00:00">
<meta name="govuk:first-published-at" content="2023-03-29T11:12:11+01:00">
<meta name="govuk:ga4-base-path" content="/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">
<meta name="govuk:content-id" content="10c0680d-2385-4553-bb02-149bd022b026">
<meta name="govuk:schema-name" content="html_publication">
<meta name="govuk:rendering-app" content="government-frontend">
<meta name="govuk:publishing-app" content="whitehall">
<meta name="govuk:format" content="html_publication">
    <meta charset="utf-8">
    <title lang="en">
      A pro-innovation approach to AI regulation - GOV.UK
  </title>

    <script src="/assets/static/govuk_publishing_components/vendor/lux/lux-measurer-d66192b1b665d415e1874e49a48138b7ae339548aa6cdb8687c7f9933e72b969.js" async="async"></script>
    <script src="/assets/static/govuk_publishing_components/rum-custom-data-b9e1806d1da2fa8ef1855d01aa71ba5eb0afc010dd2c4de3672cc7c7a39b0c8c.js" type="module"></script>
    <script src="/assets/static/govuk_publishing_components/rum-loader-a65b10e18ceeba3bd8a2eac507c7f2c513cdc82f35097df903fdea87f1dc2e33.js" async="async" data-lux-reporter-script="/assets/static/govuk_publishing_components/vendor/lux/lux-reporter-205c90cd95af651d7b312dbedc8ed8aa04bbc5f64067777f9b83385a74f7307b.js"></script>

    <meta name="govuk:components_gem_version" content="61.4.1">
    <script src="/assets/static/govuk_publishing_components/load-analytics-2fedf6967543ecf2e1442db2ee751262d0b0227274dc3fbb7f8df6db459b77d8.js" type="module"></script>

    

    <link rel="stylesheet" href="/assets/static/application-43dd279605d9d19856cf6ed3d4dc3e71f5e9fb96a39dea5d6b297b571a71e17b.css" media="all">
    <link rel="icon" sizes="48x48" href="/assets/static/favicon-24f9fbe064118d58937932e73edafd1d50cb60f7bd84f52308382a309bc2d655.ico">
    <link rel="icon" sizes="any" href="/assets/static/favicon-d962d21b5bb443f546c097ea21b567cde639adef7370da45be5e349ba8d62d33.svg" type="image/svg+xml">
    <link rel="mask-icon" href="/assets/static/govuk-icon-mask-cdf4265165f8d7f9eec54aa2c1dfbb3d8b6d297c5d7919f0313e0836a5804bb6.svg" color="#1d70b8">
    <link rel="apple-touch-icon" href="/assets/static/govuk-icon-180-d45a306f0549414cfa5085f16e4a3816a77558b139e5bc226f23162d1d0decb0.png">

    <meta name="theme-color" content="#1d70b8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <meta property="og:image" content="https://www.gov.uk/assets/static/govuk-opengraph-image-4196a4d6333cf92aaf720047f56cfd91b3532d7635fc21ebcf0d5897df6b5f77.png">

    
  <link rel="stylesheet" href="/assets/government-frontend/application-ae1d4c7a38e1c244473a4e6b36da6c8d42e9ba30a6ecd8fc52b6b17052ba96eb.css" media="all">
<link rel="canonical" href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">
<link rel="stylesheet" href="/assets/government-frontend/views/_html-publication-54fffb27670e6a49b040058204ce99bfcfea7a0f078c0f4de1471ac6e1ee9256.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_organisation-logo-f689408b02a3303831ca6310b4fed69a988a13c6bcafb818a6bfef15eaabf652.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_inverse-header-650db705c01a51a37a7fb837bbeb8fbbae7b9ebc49b995f7e518e4010da3f478.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_notice-389cafb96b8c6a5c5d01f53bc1ec3b020c0abb8cbf8698aba64396dddd9b946f.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_contents-list-9cf09e785868fb478a0587bee10ce15724d1e44dfadd71b6528ba9d14cba38f5.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_print-link-315207d796d062285407e0b8ebbbb34d11ebf65c90ff77ebc345dd4cfb9380fd.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_govspeak-html-publication-3d45c330f2a6a08a72fb9a8f7da43e942723fa1aaab83bd281f87a7d19452e74.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_govspeak-c7479da415737489dac701594597e4c84359e5cc14b4cb5173f18c8a28fc1df3.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_attachment-link-32eab833e57657c72835f1c663012a730d34cca34931b9b99dabc803c382baf5.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_attachment-1c0388cdaad71801fcc8bccf49370ff9cc56f47fc31834ec595b84277017badd.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_details-593d45a9fd2a883887784a3e7dcafbac11d9c1e1001296ce48e957b1ee567b23.css">
<link rel="stylesheet" href="/assets/government-frontend/govuk_publishing_components/components/_back-to-top-link-b2ac00053291bacbaad380cd9ed082e0289a6614b275a258ac7a989726053544.css">
<meta name="govuk:rendering-application" content="government-frontend">
</head>
  <body class="gem-c-layout-for-public govuk-template__body js-enabled govuk-frontend-supported">
    <script nonce="">
//<![CDATA[
      document.body.className += ' js-enabled' + ('noModule' in HTMLScriptElement.prototype ? ' govuk-frontend-supported' : '');

//]]>
</script>    
<div id="global-cookie-message" data-module="cookie-banner" data-nosnippet="" aria-label="Cookies on GOV.UK" class="gem-c-cookie-banner govuk-clearfix govuk-cookie-banner js-banner-wrapper" role="region" data-cookie-banner-module-started="true">
  <div class="govuk-cookie-banner__message govuk-width-container">
    <div class="govuk-grid-row">
      <div class="govuk-grid-column-two-thirds">
        <h2 class="govuk-cookie-banner__heading govuk-heading-m">Cookies on GOV.UK</h2>
        <div tabindex="-1" class="govuk-cookie-banner__content gem-c-cookie-banner__confirmation">
          <div class="gem-c-cookie-banner__content">
<p class="govuk-body">We use some essential cookies to make this website work.</p>
<p class="govuk-body">We’d like to set additional cookies to understand how you use GOV.UK, remember your settings and improve government services.</p>
<p class="govuk-body">We also use cookies set by other sites to help us deliver content from their services.</p>
</div>
          <p class="gem-c-cookie-banner__confirmation-message--accepted govuk-body" hidden="" data-ga4-cookie-banner="" data-module="ga4-link-tracker" data-ga4-track-links-only="" data-ga4-set-indexes="" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;cookie banner&quot;,&quot;section&quot;:&quot;You have accepted additional cookies&quot;}" data-ga4-link-tracker-module-started="true">You have accepted additional cookies. <span class="gem-c-cookie-banner__confirmation-message">You can <a class="govuk-link" href="/help/cookies">change your cookie settings</a> at any time.</span></p>
          <p class="gem-c-cookie-banner__confirmation-message--rejected govuk-body" hidden="">You have rejected additional cookies. <span class="gem-c-cookie-banner__confirmation-message">You can <a class="govuk-link" href="/help/cookies">change your cookie settings</a> at any time.</span></p>
        </div>
      </div>
    </div>
    <div class="js-confirmation-buttons govuk-button-group">
        

  <button class="gem-c-button govuk-button" type="submit" data-accept-cookies="true" data-cookie-types="all">Accept additional cookies</button>


        

  <button class="gem-c-button govuk-button" type="submit" data-reject-cookies="true">Reject additional cookies</button>


        <a class="govuk-link" href="/help/cookies">View cookies</a>
    </div>
    <div hidden="" class="js-hide-button govuk-button-group">
      <button class="gem-c-cookie-banner__hide-button govuk-button" data-hide-cookie-banner="true" data-module="ga4-event-tracker" data-ga4-event="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;type&quot;:&quot;cookie banner&quot;,&quot;action&quot;:&quot;closed&quot;,&quot;section&quot;:&quot;You have accepted additional cookies&quot;}" data-ga4-event-tracker-module-started="true">
          Hide cookie message
        </button>
    </div>
  </div>
</div>
    <a data-module="govuk-skip-link" class="gem-c-skip-link govuk-skip-link govuk-!-display-none-print" href="#content" data-govuk-skip-link-init="" data-govuk-skip-link-module-started="true">Skip to main content</a>

          <header data-module="ga4-event-tracker ga4-link-tracker" data-ga4-expandable="" class="gem-c-layout-super-navigation-header" data-ga4-event-tracker-module-started="true" data-ga4-link-tracker-module-started="true">
<div class="gem-c-layout-super-navigation-header__container govuk-width-container">
  <nav aria-labelledby="super-navigation-menu-heading" class="gem-c-layout-super-navigation-header__content js-module-initialised" data-module="super-navigation-mega-menu" data-super-navigation-mega-menu-module-started="true">
    <div class="gem-c-layout-super-navigation-header__header-logo">
      <a class="govuk-header__link govuk-header__link--homepage" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;external&quot;:&quot;false&quot;,&quot;text&quot;:&quot;GOV.UK&quot;,&quot;section&quot;:&quot;Logo&quot;,&quot;index_link&quot;:1,&quot;index_section&quot;:0,&quot;index_section_count&quot;:2,&quot;index_total&quot;:1}" id="logo" aria-label="Go to the GOV.UK homepage" href="https://www.gov.uk">
        <svg xmlns="http://www.w3.org/2000/svg" focusable="false" role="img" viewBox="0 0 324 60" height="30" width="162" fill="currentcolor" class="govuk-header__logotype" aria-label="GOV.UK">
  <title>GOV.UK</title>
  <g>
    <circle cx="20" cy="17.6" r="3.7"></circle>
    <circle cx="10.2" cy="23.5" r="3.7"></circle>
    <circle cx="3.7" cy="33.2" r="3.7"></circle>
    <circle cx="31.7" cy="30.6" r="3.7"></circle>
    <circle cx="43.3" cy="17.6" r="3.7"></circle>
    <circle cx="53.2" cy="23.5" r="3.7"></circle>
    <circle cx="59.7" cy="33.2" r="3.7"></circle>
    <circle cx="31.7" cy="30.6" r="3.7"></circle>
    <path d="M33.1,9.8c.2-.1.3-.3.5-.5l4.6,2.4v-6.8l-4.6,1.5c-.1-.2-.3-.3-.5-.5l1.9-5.9h-6.7l1.9,5.9c-.2.1-.3.3-.5.5l-4.6-1.5v6.8l4.6-2.4c.1.2.3.3.5.5l-2.6,8c-.9,2.8,1.2,5.7,4.1,5.7h0c3,0,5.1-2.9,4.1-5.7l-2.6-8ZM37,37.9s-3.4,3.8-4.1,6.1c2.2,0,4.2-.5,6.4-2.8l-.7,8.5c-2-2.8-4.4-4.1-5.7-3.8.1,3.1.5,6.7,5.8,7.2,3.7.3,6.7-1.5,7-3.8.4-2.6-2-4.3-3.7-1.6-1.4-4.5,2.4-6.1,4.9-3.2-1.9-4.5-1.8-7.7,2.4-10.9,3,4,2.6,7.3-1.2,11.1,2.4-1.3,6.2,0,4,4.6-1.2-2.8-3.7-2.2-4.2.2-.3,1.7.7,3.7,3,4.2,1.9.3,4.7-.9,7-5.9-1.3,0-2.4.7-3.9,1.7l2.4-8c.6,2.3,1.4,3.7,2.2,4.5.6-1.6.5-2.8,0-5.3l5,1.8c-2.6,3.6-5.2,8.7-7.3,17.5-7.4-1.1-15.7-1.7-24.5-1.7h0c-8.8,0-17.1.6-24.5,1.7-2.1-8.9-4.7-13.9-7.3-17.5l5-1.8c-.5,2.5-.6,3.7,0,5.3.8-.8,1.6-2.3,2.2-4.5l2.4,8c-1.5-1-2.6-1.7-3.9-1.7,2.3,5,5.2,6.2,7,5.9,2.3-.4,3.3-2.4,3-4.2-.5-2.4-3-3.1-4.2-.2-2.2-4.6,1.6-6,4-4.6-3.7-3.7-4.2-7.1-1.2-11.1,4.2,3.2,4.3,6.4,2.4,10.9,2.5-2.8,6.3-1.3,4.9,3.2-1.8-2.7-4.1-1-3.7,1.6.3,2.3,3.3,4.1,7,3.8,5.4-.5,5.7-4.2,5.8-7.2-1.3-.2-3.7,1-5.7,3.8l-.7-8.5c2.2,2.3,4.2,2.7,6.4,2.8-.7-2.3-4.1-6.1-4.1-6.1h10.6,0Z"></path>
  </g>
  <circle class="govuk-logo-dot" cx="226" cy="36" r="7.3"></circle>
  <path d="M93.94 41.25c.4 1.81 1.2 3.21 2.21 4.62 1 1.4 2.21 2.41 3.61 3.21s3.21 1.2 5.22 1.2 3.61-.4 4.82-1c1.4-.6 2.41-1.4 3.21-2.41.8-1 1.4-2.01 1.61-3.01s.4-2.01.4-3.01v.14h-10.86v-7.02h20.07v24.08h-8.03v-5.56c-.6.8-1.38 1.61-2.19 2.41-.8.8-1.81 1.2-2.81 1.81-1 .4-2.21.8-3.41 1.2s-2.41.4-3.81.4a18.56 18.56 0 0 1-14.65-6.63c-1.6-2.01-3.01-4.41-3.81-7.02s-1.4-5.62-1.4-8.83.4-6.02 1.4-8.83a20.45 20.45 0 0 1 19.46-13.65c3.21 0 4.01.2 5.82.8 1.81.4 3.61 1.2 5.02 2.01 1.61.8 2.81 2.01 4.01 3.21s2.21 2.61 2.81 4.21l-7.63 4.41c-.4-1-1-1.81-1.61-2.61-.6-.8-1.4-1.4-2.21-2.01-.8-.6-1.81-1-2.81-1.4-1-.4-2.21-.4-3.61-.4-2.01 0-3.81.4-5.22 1.2-1.4.8-2.61 1.81-3.61 3.21s-1.61 2.81-2.21 4.62c-.4 1.81-.6 3.71-.6 5.42s.8 5.22.8 5.22Zm57.8-27.9c3.21 0 6.22.6 8.63 1.81 2.41 1.2 4.82 2.81 6.62 4.82S170.2 24.39 171 27s1.4 5.62 1.4 8.83-.4 6.02-1.4 8.83-2.41 5.02-4.01 7.02-4.01 3.61-6.62 4.82-5.42 1.81-8.63 1.81-6.22-.6-8.63-1.81-4.82-2.81-6.42-4.82-3.21-4.41-4.01-7.02-1.4-5.62-1.4-8.83.4-6.02 1.4-8.83 2.41-5.02 4.01-7.02 4.01-3.61 6.42-4.82 5.42-1.81 8.63-1.81Zm0 36.73c1.81 0 3.61-.4 5.02-1s2.61-1.81 3.61-3.01 1.81-2.81 2.21-4.41c.4-1.81.8-3.61.8-5.62 0-2.21-.2-4.21-.8-6.02s-1.2-3.21-2.21-4.62c-1-1.2-2.21-2.21-3.61-3.01s-3.21-1-5.02-1-3.61.4-5.02 1c-1.4.8-2.61 1.81-3.61 3.01s-1.81 2.81-2.21 4.62c-.4 1.81-.8 3.61-.8 5.62 0 2.41.2 4.21.8 6.02.4 1.81 1.2 3.21 2.21 4.41s2.21 2.21 3.61 3.01c1.4.8 3.21 1 5.02 1Zm36.32 7.96-12.24-44.15h9.83l8.43 32.77h.4l8.23-32.77h9.83L200.3 58.04h-12.24Zm74.14-7.96c2.18 0 3.51-.6 3.51-.6 1.2-.6 2.01-1 2.81-1.81s1.4-1.81 1.81-2.81a13 13 0 0 0 .8-4.01V13.9h8.63v28.15c0 2.41-.4 4.62-1.4 6.62-.8 2.01-2.21 3.61-3.61 5.02s-3.41 2.41-5.62 3.21-4.62 1.2-7.02 1.2-5.02-.4-7.02-1.2c-2.21-.8-4.01-1.81-5.62-3.21s-2.81-3.01-3.61-5.02-1.4-4.21-1.4-6.62V13.9h8.63v26.95c0 1.61.2 3.01.8 4.01.4 1.2 1.2 2.21 2.01 2.81.8.8 1.81 1.4 2.81 1.81 0 0 1.34.6 3.51.6Zm34.22-36.18v18.92l15.65-18.92h10.82l-15.03 17.32 16.03 26.83h-10.21l-11.44-20.21-5.62 6.22v13.99h-8.83V13.9"></path>
</svg>

</a>
</div>    <h2 id="super-navigation-menu-heading" class="govuk-visually-hidden">
      Navigation menu
    </h2>


    <div class="gem-c-layout-super-navigation-header__navigation-item">
      <a class="gem-c-layout-super-navigation-header__navigation-item-link" href="/browse" hidden="hidden"><span class="gem-c-layout-super-navigation-header__navigation-item-link-inner">          Menu
</span></a>
      <button aria-controls="super-navigation-menu" aria-expanded="false" aria-label="Show navigation menu" class="gem-c-layout-super-navigation-header__navigation-top-toggle-button" data-text-for-hide="Hide navigation menu" data-text-for-show="Show navigation menu" data-toggle-desktop-group="top" data-toggle-mobile-group="top" data-ga4-event="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;text&quot;:&quot;Menu&quot;,&quot;index_section&quot;:1,&quot;index_section_count&quot;:2,&quot;section&quot;:&quot;Menu&quot;}" id="super-navigation-menu-toggle" type="button">
        <span class="gem-c-layout-super-navigation-header__navigation-top-toggle-button-inner">Menu</span>
</button>    </div>
    <div id="super-navigation-menu" hidden="hidden" class="gem-c-layout-super-navigation-header__navigation-dropdown-menu">
      <div class="govuk-grid-row gem-c-layout-super-navigation-header__navigation-items">


          <div class="govuk-grid-column-two-thirds-from-desktop gem-c-layout-super-navigation-header__column--services-and-information">
            <h3 class="govuk-heading-m gem-c-layout-super-navigation-header__column-header">
              Services and information
            </h3>
            <ul class="gem-c-layout-super-navigation-header__navigation-second-items gem-c-layout-super-navigation-header__navigation-second-items--services-and-information">
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:1,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/benefits">Benefits</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:2,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/births-deaths-marriages">Births, death, marriages and care</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:3,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/business">Business and self-employed</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:4,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/childcare-parenting">Childcare and parenting</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:5,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/citizenship">Citizenship and living in the UK</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:6,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/justice">Crime, justice and the law</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:7,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/disabilities">Disabled people</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:8,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/driving">Driving and transport</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:9,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/education">Education and learning</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:10,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/employing-people">Employing people</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:11,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/environment-countryside">Environment and countryside</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:12,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/housing-local-services">Housing and local services</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:13,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/tax">Money and tax</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:14,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/abroad">Passports, travel and living abroad</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:15,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/visas-immigration">Visas and immigration</a>
                    
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:1,&quot;index_link&quot;:16,&quot;index_section_count&quot;:3,&quot;index_total&quot;:16,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/working">Working, jobs and pensions</a>
                    
                  </li>
            </ul>
          </div>

          <div class="govuk-grid-column-one-third-from-desktop gem-c-layout-super-navigation-header__column--government-activity">
            <h3 class="govuk-heading-m gem-c-layout-super-navigation-header__column-header">
              Government activity
            </h3>
            <ul class="gem-c-layout-super-navigation-header__navigation-second-items gem-c-layout-super-navigation-header__navigation-second-items--government-activity">
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:1,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/organisations">Departments</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Departments, agencies and public bodies</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:2,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/news-and-communications">News</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">News stories, speeches, letters and notices</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:3,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/guidance-and-regulation">Guidance and regulation</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Detailed guidance, regulations and rules</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:4,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/research-and-statistics">Research and statistics</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Reports, analysis and official statistics</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:5,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/policy-papers-and-consultations">Policy papers and consultations</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Consultations and strategy</p>
                  </li>
                  <li class="gem-c-layout-super-navigation-header__dropdown-list-item">
                    <a class="govuk-link gem-c-layout-super-navigation-header__navigation-second-item-link gem-c-layout-super-navigation-header__navigation-second-item-link--with-description" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;index_section&quot;:2,&quot;index_link&quot;:6,&quot;index_section_count&quot;:3,&quot;index_total&quot;:6,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/transparency-and-freedom-of-information-releases">Transparency</a>
                    <p class="gem-c-layout-super-navigation-header__navigation-second-item-description">Data, Freedom of Information releases and corporate reports</p>
                  </li>
            </ul>
          </div>
      </div>
</div>    <div class="gem-c-layout-super-navigation-header__search-item">
      <button id="super-search-menu-toggle" class="gem-c-layout-super-navigation-header__search-toggle-button" aria-controls="super-search-menu" aria-expanded="false" aria-label="Show search menu" data-text-for-hide="Hide search menu" data-text-for-show="Show search menu" data-toggle-mobile-group="top" data-toggle-desktop-group="top" data-ga4-event="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;type&quot;:&quot;header menu bar&quot;,&quot;text&quot;:&quot;Search&quot;,&quot;index_section&quot;:2,&quot;index_section_count&quot;:2,&quot;section&quot;:&quot;Search&quot;}" type="button">
        <span class="govuk-visually-hidden">
          Search GOV.UK
        </span>
        
<svg class="gem-c-layout-super-navigation-header__search-toggle-button-link-icon" width="27" height="27" viewBox="0 0 27 27" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
  <circle cx="12.0161" cy="11.0161" r="8.51613" stroke="currentColor" stroke-width="3"></circle>
  <line x1="17.8668" y1="17.3587" x2="26.4475" y2="25.9393" stroke="currentColor" stroke-width="3"></line>
</svg>

        <span aria-hidden="true" class="gem-c-layout-super-navigation-header__navigation-top-toggle-close-icon">
          ×
        </span>
</button>
      <a class="gem-c-layout-super-navigation-header__search-item-link" href="/search" hidden="hidden">
        <span class="govuk-visually-hidden">
          Search GOV.UK
        </span>
        
<svg class="gem-c-layout-super-navigation-header__search-item-link-icon" width="27" height="27" viewBox="0 0 27 27" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
  <circle cx="12.0161" cy="11.0161" r="8.51613" stroke="currentColor" stroke-width="3"></circle>
  <line x1="17.8668" y1="17.3587" x2="26.4475" y2="25.9393" stroke="currentColor" stroke-width="3"></line>
</svg>

</a>    </div>
    <div id="super-search-menu" hidden="hidden" class="gem-c-layout-super-navigation-header__navigation-dropdown-menu">
      <div class="gem-c-layout-super-navigation-header__search-container gem-c-layout-super-navigation-header__search-items">
        <h3 class="govuk-visually-hidden">
          Search
        </h3>
        <div class="govuk-grid-row">
          <div class="govuk-grid-column-full">
            <form class="gem-c-layout-super-navigation-header__search-form" id="search" data-module="ga4-search-tracker" data-ga4-search-type="header menu bar" data-ga4-search-url="/search/all" data-ga4-search-section="Search GOV.UK" data-ga4-search-index-section="3" data-ga4-search-index-section-count="3" action="/search/all" method="get" role="search" aria-label="Site-wide" data-ga4-search-tracker-module-started="true">
              <div class="gem-c-search-with-autocomplete gem-c-search-with-autocomplete--large govuk-!-margin-bottom-0" data-module="gem-search-with-autocomplete" data-source-url="https://www.gov.uk/api/search/autocomplete.json" data-source-key="suggestions" data-gem-search-with-autocomplete-module-started="true">
  <div data-module="gem-toggle-input-class-on-focus" class="gem-c-search govuk-!-display-none-print gem-c-search--large gem-c-search--on-white gem-c-search--separate-label govuk-!-margin-bottom-0" data-gem-toggle-input-class-on-focus-module-started="true">
    <label for="search-main-fbf97652" class="govuk-label govuk-label--m gem-c-layout-super-navigation-header__search-label--large-navbar">Search GOV.UK</label>
  <div class="gem-c-search__item-wrapper">
    <div class="js-search-input-wrapper">
      
    <div class="gem-c-search-with-autocomplete__wrapper"><div class="gem-c-search-with-autocomplete__status" style="border: 0px; clip: rect(0px, 0px, 0px, 0px); height: 1px; margin-bottom: -1px; margin-right: -1px; overflow: hidden; padding: 0px; position: absolute; white-space: nowrap; width: 1px;"><div id="search-main-fbf97652__status--A" role="status" aria-atomic="true" aria-live="polite"></div><div id="search-main-fbf97652__status--B" role="status" aria-atomic="true" aria-live="polite"></div></div><input aria-describedby="search-main-fbf97652__assistiveHint" aria-expanded="false" aria-controls="search-main-fbf97652__listbox" aria-autocomplete="list" autocomplete="off" class="gem-c-search-with-autocomplete__input gem-c-search-with-autocomplete__input--default gem-c-search__item gem-c-search__input js-class-toggle" id="search-main-fbf97652" name="keywords" placeholder="" type="search" role="combobox"><ul aria-labelledby="search-main-fbf97652" id="search-main-fbf97652__listbox" role="listbox" class="gem-c-search-with-autocomplete__menu gem-c-search-with-autocomplete__menu--inline gem-c-search-with-autocomplete__menu--hidden"></ul><span id="search-main-fbf97652__assistiveHint" style="display: none;">When search suggestions are available use up and down arrows to review and enter to select. Touch device users, explore by touch or with swipe gestures.</span></div></div>
    <div class="gem-c-search__item gem-c-search__submit-wrapper">
      <button class="gem-c-search__submit" type="submit" enterkeyhint="search">
        Search
        
<svg class="gem-c-search__icon" width="27" height="27" viewBox="0 0 27 27" fill="none" xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false">
  <circle cx="12.0161" cy="11.0161" r="8.51613" stroke="currentColor" stroke-width="3"></circle>
  <line x1="17.8668" y1="17.3587" x2="26.4475" y2="25.9393" stroke="currentColor" stroke-width="3"></line>
</svg>

</button>    </div>
  </div>
</div>
</div>
</form>          </div>
        </div>
      </div>
</div>  </nav>
</div>
</header>

    


      <div id="wrapper" class="direction-ltr govuk-width-container">
        
<div class="gem-c-contextual-breadcrumbs">
    <div class="govuk-!-display-none-print">
      


<nav data-module="ga4-link-tracker" aria-label="Breadcrumb" class="gem-c-breadcrumbs govuk-breadcrumbs govuk-breadcrumbs--collapse-on-mobile" data-ga4-link-tracker-module-started="true">
  <ol class="govuk-breadcrumbs__list">
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_total&quot;:&quot;5&quot;}" class="govuk-breadcrumbs__link" href="/">Home</a>
        </li>
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_total&quot;:&quot;5&quot;}" class="govuk-breadcrumbs__link" href="/business-and-industry">Business and industry</a>
        </li>
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_total&quot;:&quot;5&quot;}" class="govuk-breadcrumbs__link" href="/business-and-industry/science-and-innovation">Science and innovation</a>
        </li>
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_total&quot;:&quot;5&quot;}" class="govuk-breadcrumbs__link" href="/business-and-industry/artificial-intelligence">Artificial intelligence</a>
        </li>
        <li class="govuk-breadcrumbs__list-item">
          <a data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;breadcrumb&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;5&quot;}" class="govuk-breadcrumbs__link" href="/government/publications/ai-regulation-a-pro-innovation-approach">AI regulation: a pro-innovation approach</a>
        </li>
  </ol>
</nav>
    </div>
</div>

    

    

    <main role="main" id="content" class="html-publication govuk-main-wrapper govuk-main-wrapper--auto-spacing" lang="en">
      <span id="Top"></span>
          

  <div class="publication-external">
    <ul class="organisation-logos">
        <li class="organisation-logos__logo">
          
<div class="gem-c-organisation-logo brand--department-for-science-innovation-technology">
    <a class="gem-c-organisation-logo__container gem-c-organisation-logo__link gem-c-organisation-logo__crest gem-c-organisation-logo__crest--single-identity brand__border-color" href="/government/organisations/department-for-science-innovation-and-technology">
      <span class="gem-c-organisation-logo__name">Department for<br>Science, Innovation<br>&amp; Technology</span>
</a>
</div>
        </li>
        <li class="organisation-logos__logo">
          
<div class="gem-c-organisation-logo brand--cabinet-office">
    <a class="gem-c-organisation-logo__container gem-c-organisation-logo__link gem-c-organisation-logo__crest gem-c-organisation-logo__crest--single-identity brand__border-color" href="/government/organisations/office-for-artificial-intelligence">
      <span class="gem-c-organisation-logo__name">Office for Artificial Intelligence</span>
</a>
</div>
        </li>
    </ul>
  </div>

  <header class="gem-c-inverse-header gem-c-inverse-header--html-publication-header govuk-!-padding-top-6 govuk-!-padding-bottom-6">
    
  
<div class="gem-c-heading  gem-c-heading--inverse govuk-!-margin-bottom-0">
      <span class="govuk-caption-xl gem-c-heading__context">
    Policy paper
  </span>


  <h1 class="gem-c-heading__text govuk-heading-xl">
    A pro-innovation approach to AI regulation
</h1>
</div>

    <p class="gem-c-inverse-header__subtext">Updated 3 August 2023</p>
</header>
    <section aria-label="Notice" class="govuk-notification-banner gem-c-notice govuk-!-margin-bottom-8" role="region" title="This was published under the &lt;span lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;2022 to 2024 Sunak Conservative government&lt;/span&gt;">
        <div class="govuk-notification-banner__content">
      
      <span class="gem-c-notice__title govuk-notification-banner__heading">This was published under the <span lang="en" dir="ltr">2022 to 2024 Sunak Conservative government</span></span>
      

      
</div></section>


<div id="contents">
  <div class="govuk-grid-row gem-print-columns-none">
      <div class="govuk-grid-column-one-quarter-from-desktop contents-list-container">
          <nav data-module="ga4-link-tracker" aria-label="Contents" class="gem-c-contents-list govuk-!-margin-bottom-4" data-ga4-link-tracker-module-started="true">
    <h2 class="gem-c-contents-list__title">
      Contents
</h2>
    <ol class="gem-c-contents-list__list">
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:1}" href="#correction-slip">Correction slip</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:2}" href="#ministerial-foreword">Ministerial foreword</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:3}" href="#executive-summary">Executive summary</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:4}" href="#part-1-introduction">Part 1: Introduction</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:5}" href="#part-2-the-current-regulatory-environment">Part 2: The current regulatory environment</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:6}" href="#part-3-an-innovative-and-iterative-approach">Part 3: An innovative and iterative approach</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:7}" href="#part-4-tools-for-trustworthy-ai-to-support-implementation">Part 4: Tools for trustworthy AI to support implementation</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:8}" href="#part-5-territorial-application">Part 5: Territorial application</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:9}" href="#partsix">Part 6: Global interoperability and international engagement</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:10}" href="#part-7-conclusion-and-next-steps">Part 7: Conclusion and next steps</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:11}" href="#annex-a-implementation-of-the-principles-by-regulators">Annex A: Implementation of the principles by regulators</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:12}" href="#annex-b-stakeholder-engagement">Annex B: Stakeholder engagement</a>
        </li>
        <li class="gem-c-contents-list__list-item gem-c-contents-list__list-item--numbered">
          <span class="gem-c-contents-list__list-item-dash" aria-hidden="true"></span>
          <a class="gem-c-contents-list__link govuk-link gem-print-force-link-styles govuk-link--no-underline" data-ga4-link="{&quot;event_name&quot;:&quot;select_content&quot;,&quot;section&quot;:&quot;Contents&quot;,&quot;type&quot;:&quot;contents list&quot;,&quot;index_total&quot;:13,&quot;index_link&quot;:13}" href="#annexc">Annex C: How to respond to this consultation</a>
        </li>
    </ol>
</nav>

        <div class="gem-c-print-link govuk-!-display-none-print govuk-!-margin-bottom-6">
    <button class="govuk-link gem-c-print-link__button" data-module="print-link" data-print-link-module-started="true">Print this page</button>
</div>
      </div>

    <div class="print-wrapper">
      <div class="meta-data meta-data--display-print">
        <p>
  <img class="meta-data-licence" src="/assets/government-frontend/open-government-licence-min-93b6a51b518ff99714a1aa2a7d2162735c155ec3cb073c75fb88b2a332fa83d3.png">
</p>
<p>
  © Crown copyright 2023
</p>
<p>
  This publication is licensed under the terms of the Open Government Licence v3.0 except where otherwise stated. To view this licence, visit <a href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3">nationalarchives.gov.uk/doc/open-government-licence/version/3</a> or write to the Information Policy Team, The National Archives, Kew, London TW9 4DU, or email: <a href="mailto:psi@nationalarchives.gov.uk">psi@nationalarchives.gov.uk</a>.
</p>
<p>
  Where we have identified any third party copyright information you will need to obtain permission from the copyright holders concerned.
</p>
<p>
  This publication is available at https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper
</p>


      </div>
    </div>

    <div class="govuk-grid-column-three-quarters-from-desktop contents-container">
      <div class="gem-c-govspeak-html-publication">
  <div data-module="govspeak" class="gem-c-govspeak govuk-govspeak gem-c-govspeak--direction-ltr govuk-!-margin-bottom-0" data-govspeak-module-started="true">
    
    
        <div class="govspeak">
<p>Presented to Parliament by the Secretary of State for Science, Innovation and Technology by Command of His Majesty on 29 March 2023.</p>

<p>Command Paper Number: 815</p>

<p>© Crown copyright 2023</p>

<p>ISBN: 978-1-5286-4009-1</p>

<hr>

<h2 id="correction-slip">Correction slip</h2>

<p>Correction:</p>

<p>Text currently reads <strong><a href="#annexc">in Annex C</a></strong>:</p>

<p>1. Do you agree that requiring organisations to make it clear when they are using <abbr title="Artificial Intelligence">AI</abbr> would adequately ensure transparency?</p>

<p>2. What other transparency measures would be appropriate, if any?</p>

<p>3. Do you agree that current routes to contestability or redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms are adequate?</p>

<p>4. How could routes to contestability or redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms be improved, if at all?</p>

<p>[…]</p>

<p>L3.&nbsp;If you are a business that develops, uses, or sells <abbr title="Artificial Intelligence">AI</abbr>, how do you currently manage <abbr title="Artificial Intelligence">AI</abbr> risk including through the wider supply chain? How could government support effective <abbr title="Artificial Intelligence">AI</abbr>-related risk management?</p>

<p>[…]</p>

<p>S1. Which of the sandbox models described in <a href="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section334">section 3.3.4</a> would be most likely to support innovation?</p>

<p>Text should read:</p>

<p>1: Do you agree that requiring organisations to make it clear when they are using <abbr title="Artificial Intelligence">AI</abbr> would improve transparency?</p>

<p>2: Are there other measures we could require of organisations to improve transparency for <abbr title="Artificial Intelligence">AI</abbr>?</p>

<p>3: Do you agree that current routes to contest or get redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms are adequate?</p>

<p>4: How could current routes to contest or seek redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms be improved, if at all?</p>

<p>[…]</p>

<p>L3: If you work for a business that develops, uses, or sells <abbr title="Artificial Intelligence">AI</abbr>, how do you currently manage <abbr title="Artificial Intelligence">AI</abbr> risk including through the wider supply chain? How could government support effective <abbr title="Artificial Intelligence">AI</abbr>-related risk management?</p>

<p>[…]</p>

<p>S1: To what extent would the sandbox models described in section 3.3.4 support innovation?</p>

<p>Date of correction: 4 July 2023</p>

<h2 id="ministerial-foreword">Ministerial foreword</h2>

<figure class="image embedded"><div class="img"><img src="https://assets.publishing.service.gov.uk/media/6420d13632a8e0000cfa94cc/s960_Minister_Michelle_Donelan_UK_Parliament_960px_640px.jpg" alt=""></div>
<figcaption><p>The Rt Hon Michelle Donelan MP, Secretary of State for Science, Innovation and Technology</p></figcaption></figure>

<p>I believe that a common-sense, outcomes-oriented approach is the best way to get right to the heart of delivering on the priorities of people across the UK. Better public services, high quality jobs and opportunities to learn the skills that will power our future – these are the priorities that will drive our goal to become a science and technology superpower by 2030.</p>

<p>Artificial Intelligence (<abbr title="Artificial Intelligence">AI</abbr>) will play a central part in delivering and enabling these goals, and this white paper will ensure we are putting the UK on course to be the best place in the world to build, test and use <abbr title="Artificial Intelligence">AI</abbr> technology. But we are not starting from zero. Having invested over £2.5 billion in <abbr title="Artificial Intelligence">AI</abbr> since 2014, this paper builds on our recent announcements of £110 million for our <a rel="external" href="https://www.ukri.org/news/250m-to-secure-the-uks-world-leading-position-in-technologies-of-tomorrow/"><abbr title="Artificial Intelligence">AI</abbr> Tech Missions Fund</a>, £900 million to establish a new <abbr title="Artificial Intelligence">AI</abbr> Research Resource and to develop an exascale supercomputer capable of running large <abbr title="Artificial Intelligence">AI</abbr> models – backed up by our new £8 million <abbr title="Artificial Intelligence">AI</abbr> Global Talent Network and £117 million of existing funding to create hundreds of new <abbr title="Doctorate of Philosophy">PhDs</abbr> for <abbr title="Artificial Intelligence">AI</abbr> researchers.</p>

<p>Most of us are only now beginning to understand the transformative potential of <abbr title="Artificial Intelligence">AI</abbr> as the technology rapidly improves. But in many ways, <abbr title="Artificial Intelligence">AI</abbr> is already delivering fantastic social and economic benefits for real people – from improving <abbr title="National Health Service">NHS</abbr> medical care to making transport safer. Recent advances in things like generative <abbr title="Artificial Intelligence">AI</abbr> give us a glimpse into the enormous opportunities that await us in the near future if we are prepared to lead the world in the <abbr title="Artificial Intelligence">AI</abbr> sector with our values of transparency, accountability and innovation.</p>

<p>My vision for an <abbr title="Artificial Intelligence">AI</abbr>-enabled country is one where our <abbr title="National Health Service">NHS</abbr> heroes are able to save lives using <abbr title="Artificial Intelligence">AI</abbr> technologies that were unimaginable just a few decades ago. I want our police, transport networks and climate scientists and many more to be empowered by <abbr title="Artificial Intelligence">AI</abbr> technologies that will make the UK the smartest, healthiest, safest and happiest place to live and work. That is why <abbr title="Artificial Intelligence">AI</abbr> is one of this government’s 5 technologies of tomorrow – bringing stronger growth, better jobs, and bold new discoveries. It is a vision that has been shaped by stakeholders and experts in <abbr title="Artificial Intelligence">AI</abbr>, whose expertise and ideas I am determined to see reflected in our department.</p>

<p>The UK has been at the forefront of this progress, placing third in the world for <abbr title="Artificial Intelligence">AI</abbr> research and development. We are home to a third of Europe’s total <abbr title="Artificial Intelligence">AI</abbr> companies and twice as many as any other European country. Our world-leading status is down to our thriving research base and the pipeline of expertise graduating through our universities, the ingenuity of our innovators and the government’s long-term commitment to invest in <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>To ensure we become an <abbr title="Artificial Intelligence">AI</abbr> superpower, though, it is crucial that we do all we can to create the right environment to harness the benefits of <abbr title="Artificial Intelligence">AI</abbr> and remain at the forefront of technological developments. That includes getting regulation right so that innovators can thrive and the risks posed by <abbr title="Artificial Intelligence">AI</abbr> can be addressed.</p>

<p>These risks could include anything from physical harm, an undermining of national security, as well as risks to mental health. The development and deployment of <abbr title="Artificial Intelligence">AI</abbr> can also present ethical challenges which do not always have clear answers. Unless we act, household consumers, public services and businesses will not trust the technology and will be nervous about adopting it. Unless we build public trust, we will miss out on many of the benefits on offer.</p>

<p>Indeed, the pace of change itself can be unsettling. Some fear a future in which <abbr title="Artificial Intelligence">AI</abbr> replaces or displaces jobs, for example. Our white paper and our vision for a future <abbr title="Artificial Intelligence">AI</abbr>-enabled country is one in which our ways of working are complemented by <abbr title="Artificial Intelligence">AI</abbr> rather than disrupted by it. In the modern world, too much of our professional lives are taken up by monotonous tasks – inputting data, filling out paperwork, scanning through documents for one piece of information and so on. <abbr title="Artificial Intelligence">AI</abbr> in the workplace has the potential to free us up from these tasks, allowing us to spend more time doing the things we trained for – teachers with more time to teach, clinicians with more time to spend with patients, police officers with more time on the beat rather than behind a desk – the list goes on.</p>

<p>Indeed, since <abbr title="Artificial Intelligence">AI</abbr> is already in our day-to-day lives, there are numerous examples that can help to illustrate the real, tangible benefits that <abbr title="Artificial Intelligence">AI</abbr> can bring once any risks are mitigated. Streaming services already use advanced <abbr title="Artificial Intelligence">AI</abbr> to recommend TV shows and films to us. Our <abbr title="satellite navigation">satnav</abbr> uses <abbr title="Artificial Intelligence">AI</abbr> to plot the fastest routes for our journeys, or helps us avoid traffic by intelligently predicting where congestion will be on our journey. And of course, almost all of us carry a smartphone in our pockets that uses advanced <abbr title="Artificial Intelligence">AI</abbr> in all sorts of ways. These common devices all carried risks at one time or another, but today they benefit us enormously.</p>

<p>That is why our white paper details how we intend to support innovation while providing a framework to ensure risks are identified and addressed. However, a heavy-handed and rigid approach can stifle innovation and slow <abbr title="Artificial Intelligence">AI</abbr> adoption. That is why we set out a proportionate and pro-innovation regulatory framework. Rather than target specific technologies, it focuses on the context in which <abbr title="Artificial Intelligence">AI</abbr> is deployed. This enables us to take a balanced approach to weighing up the benefits versus the potential risks.</p>

<p>We recognise that particular <abbr title="Artificial Intelligence">AI</abbr> technologies, foundation models for example, can be applied in many different ways and this means the risks can vary hugely. For example, using a chatbot to produce a summary of a long article presents very different risks to using the same technology to provide medical advice. We understand the need to monitor these developments in partnership with innovators while also avoiding placing unnecessary regulatory burdens on those deploying <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>To ensure our regulatory framework is effective, we will leverage the expertise of our world class regulators. They understand the risks in their sectors and are best placed to take a proportionate approach to regulating <abbr title="Artificial Intelligence">AI</abbr>. This will mean supporting innovation and working closely with business, but also stepping in to address risks when necessary. By underpinning the framework with a set of principles, we will drive consistency across regulators while also providing them with the flexibility needed.</p>

<p>For innovators working at the cutting edge and developing novel technologies, navigating regulatory regimes can be challenging. That’s why we are confirming our commitment to taking forward a key recommendation made by Sir Patrick Vallance to establish a regulatory sandbox for <abbr title="Artificial Intelligence">AI</abbr>. This will bring together regulators to support innovators directly and help them get their products to market. The sandbox will also enable us to understand how regulation interacts with new technologies and refine this interaction where necessary.</p>

<p>Having exited the European Union we are free to establish a regulatory approach that enables us to establish the UK as an <abbr title="Artificial Intelligence">AI</abbr> superpower. It is an approach that will actively support innovation while addressing risks and public concerns. The UK is home to thriving start-ups, which our framework will support to scale-up and compete internationally. Our pro-innovation approach will also act as a strong incentive when it comes to <abbr title="Artificial Intelligence">AI</abbr> businesses based overseas establishing a presence in the UK. The white paper sets out our commitment to engaging internationally to support interoperability across different regulatory regimes. Not only will this ease the burden on business but it will also allow us to embed our values as global approaches to governing <abbr title="Artificial Intelligence">AI</abbr> develop.</p>

<p>Our approach relies on collaboration between government, regulators and business. Initially, we do not intend to introduce new legislation. By rushing to legislate too early, we would risk placing undue burdens on businesses. But alongside empowering regulators to take a lead, we are also setting expectations. Our new monitoring functions will provide a real time assessment of how the regulatory framework is performing so that we can be confident that it is proportionate. The pace of technological development also means that we need to understand new and emerging risks, engaging with experts to ensure we take action where necessary. A critical component of this activity will be engaging with the public to understand their expectations, raising awareness of the potential of <abbr title="Artificial Intelligence">AI</abbr> and demonstrating that we are responding to concerns.</p>

<p>The framework set out in this white paper is deliberately designed to be flexible. As the technology evolves, our regulatory approach may also need to adjust. Our principles-based approach, with central functions to monitor and drive collaboration, will enable us to adapt as needed while providing industry with the clarity needed to innovate. We will continue to develop our approach, building on our commitment to making the UK the best place in the world to be a business developing and using <abbr title="Artificial Intelligence">AI</abbr>. Responses to the consultation will inform how we develop the regulatory framework – I encourage all of those with an interest to respond.</p>

<h2 id="executive-summary">Executive summary</h2>

<h3 id="artificial-intelligence--the-opportunity-and-the-challenge">Artificial intelligence – the opportunity and the challenge</h3>

<p>1. Artificial intelligence (<abbr title="Artificial Intelligence">AI</abbr>) is already delivering wide societal benefits, from medical advances<sup id="fnref:1"><a href="#fn:1" class="footnote" rel="footnote" role="doc-noteref">[footnote 1]</a></sup>  to mitigating climate change.<sup id="fnref:2"><a href="#fn:2" class="footnote" rel="footnote" role="doc-noteref">[footnote 2]</a></sup> For example, an <abbr title="Artificial Intelligence">AI</abbr> technology developed by DeepMind, a UK-based business, can now predict the structure of almost every protein known to science.<sup id="fnref:3"><a href="#fn:3" class="footnote" rel="footnote" role="doc-noteref">[footnote 3]</a></sup> This breakthrough will accelerate scientific research and the development of life-saving medicines – it has already helped scientists to make huge progress in combating malaria, antibiotic resistance, and plastic waste.</p>

<p>2. The UK Science and Technology Framework<sup id="fnref:4"><a href="#fn:4" class="footnote" rel="footnote" role="doc-noteref">[footnote 4]</a></sup> sets out government’s strategic vision and identifies <abbr title="Artificial Intelligence">AI</abbr> as one of 5 critical technologies. The framework notes the role of regulation in creating the environment for <abbr title="Artificial Intelligence">AI</abbr> to flourish. We know that we have yet to see <abbr title="Artificial Intelligence">AI</abbr> technologies reach their full potential. Under the right conditions, <abbr title="Artificial Intelligence">AI</abbr> will transform all areas of life<sup id="fnref:5"><a href="#fn:5" class="footnote" rel="footnote" role="doc-noteref">[footnote 5]</a></sup> and stimulate the UK economy by unleashing innovation and driving productivity,<sup id="fnref:6"><a href="#fn:6" class="footnote" rel="footnote" role="doc-noteref">[footnote 6]</a></sup> creating new jobs and improving the workplace.</p>

<p>3. Across the world, countries and regions are beginning to draft the rules for <abbr title="Artificial Intelligence">AI</abbr>. The UK needs to act quickly to continue to lead the international conversation on <abbr title="Artificial Intelligence">AI</abbr> governance and demonstrate the value of our pragmatic, proportionate regulatory approach. The need to act was highlighted by Sir Patrick Vallance in his recent Regulation for Innovation review. The report identifies the short time frame for government intervention to provide a clear, pro-innovation regulatory environment in order to make the UK one of the top places in the world to build foundational <abbr title="Artificial Intelligence">AI</abbr> companies.<sup id="fnref:7"><a href="#fn:7" class="footnote" rel="footnote" role="doc-noteref">[footnote 7]</a></sup></p>

<p>4. While we should capitalise on the benefits of these technologies, we should also not overlook the new risks that may arise from their use, nor the unease that the complexity of <abbr title="Artificial Intelligence">AI</abbr> technologies can produce in the wider public. We already know that some uses of <abbr title="Artificial Intelligence">AI</abbr> could damage our physical<sup id="fnref:8"><a href="#fn:8" class="footnote" rel="footnote" role="doc-noteref">[footnote 8]</a></sup> and mental health, <sup id="fnref:9"><a href="#fn:9" class="footnote" rel="footnote" role="doc-noteref">[footnote 9]</a></sup> infringe on the privacy of individuals<sup id="fnref:10"><a href="#fn:10" class="footnote" rel="footnote" role="doc-noteref">[footnote 10]</a></sup> and undermine human rights.<sup id="fnref:11"><a href="#fn:11" class="footnote" rel="footnote" role="doc-noteref">[footnote 11]</a></sup></p>

<p>5. Public trust in <abbr title="Artificial Intelligence">AI</abbr> will be undermined unless these risks, and wider concerns about the potential for bias and discrimination, are addressed. By building trust, we can accelerate the adoption of <abbr title="Artificial Intelligence">AI</abbr> across the UK to maximise the economic and social benefits that the technology can deliver, while attracting investment and stimulating the creation of high-skilled <abbr title="Artificial Intelligence">AI</abbr> jobs.<sup id="fnref:12"><a href="#fn:12" class="footnote" rel="footnote" role="doc-noteref">[footnote 12]</a></sup> In order to maintain the UK’s position as a global <abbr title="Artificial Intelligence">AI</abbr> leader, we need to ensure that the public continues to see how the benefits of <abbr title="Artificial Intelligence">AI</abbr> can outweigh the risks.<sup id="fnref:13"><a href="#fn:13" class="footnote" rel="footnote" role="doc-noteref">[footnote 13]</a></sup></p>

<p>6. Responding to risk and building public trust are important drivers for regulation. But clear and consistent regulation can also support business investment and build confidence in innovation. Throughout our extensive engagement, industry repeatedly emphasised that consumer trust is key to the success of innovation economies. We therefore need a clear, proportionate approach to regulation that enables the responsible application of <abbr title="Artificial Intelligence">AI</abbr> to flourish. Instead of creating cumbersome rules applying to all <abbr title="Artificial Intelligence">AI</abbr> technologies, our framework ensures that regulatory measures are proportionate to context and outcomes, by focusing on the use of <abbr title="Artificial Intelligence">AI</abbr> rather than the technology itself.</p>

<p>7. People and organisations develop and use <abbr title="Artificial Intelligence">AI</abbr> in the UK within the rules set by our existing laws, informed by standards, guidance and other tools. But <abbr title="Artificial Intelligence">AI</abbr> is a general purpose technology and its uses can cut across regulatory remits. As a result, <abbr title="Artificial Intelligence">AI</abbr> technologies are currently regulated through a complex patchwork of legal requirements. We are concerned by feedback from across industry that the absence of cross-cutting <abbr title="Artificial Intelligence">AI</abbr> regulation creates uncertainty and inconsistency which can undermine business and consumer confidence in <abbr title="Artificial Intelligence">AI</abbr>, and stifle innovation. By providing a clear and unified approach to regulation, our framework will build public confidence, making it clear that <abbr title="Artificial Intelligence">AI</abbr> technologies are subject to cross-cutting, principles-based regulation.</p>

<h3 id="our-pro-innovation-framework">Our pro-innovation framework</h3>

<p>8. The government will put in place a new framework to bring clarity and coherence to the <abbr title="Artificial Intelligence">AI</abbr> regulatory landscape. This regime is designed to make responsible innovation easier. It will strengthen the UK’s position as a global leader in <abbr title="Artificial Intelligence">AI</abbr>, harness <abbr title="Artificial Intelligence">AI</abbr>’s ability to drive growth and prosperity,<sup id="fnref:14"><a href="#fn:14" class="footnote" rel="footnote" role="doc-noteref">[footnote 14]</a></sup> and increase public trust in its use and application.</p>

<p>9. We are taking a deliberately agile and iterative approach, recognising the speed at which these technologies are evolving. Our framework is designed to build the evidence base so that we can learn from experience and continuously adapt to develop the best possible regulatory regime. Industry has praised our pragmatic and proportionate approach.</p>

<p>10. Our framework is underpinned by 5 principles to guide and inform the responsible development and use of <abbr title="Artificial Intelligence">AI</abbr> in all sectors of the economy:</p>

<ul>
  <li>Safety, security and robustness</li>
  <li>Appropriate transparency and explainability</li>
  <li>Fairness</li>
  <li>Accountability and governance</li>
  <li>Contestability and redress</li>
</ul>

<p>11. We will not put these principles on a statutory footing initially. New rigid and onerous legislative requirements on businesses could hold back <abbr title="Artificial Intelligence">AI</abbr> innovation and reduce our ability to respond quickly and in a proportionate way to future technological advances. Instead, the principles will be issued on a non-statutory basis and implemented by existing regulators. This approach makes use of regulators’ domain-specific expertise to tailor the implementation of the principles to the specific context in which <abbr title="Artificial Intelligence">AI</abbr> is used. During the initial period of implementation, we will continue to collaborate with regulators to identify any barriers to the proportionate application of the principles, and evaluate whether the non-statutory framework is having the desired effect.</p>

<p>12. Following this initial period of implementation, and when parliamentary time allows, we anticipate introducing a statutory duty on regulators requiring them to have due regard to the principles. Some feedback from regulators, industry and academia suggested we should implement further measures to support the enforcement of the framework. A duty requiring regulators to have regard to the principles should allow regulators the flexibility to exercise judgement when applying the principles in particular contexts, while also strengthening their mandate to implement them. In line with our proposal to work collaboratively with regulators and take an adaptable approach, we will not move to introduce such a statutory duty if our monitoring of the framework shows that implementation is effective without the need to legislate.</p>

<p>13. In the 2022 <a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement"><abbr title="Artificial Intelligence">AI</abbr> regulation policy paper</a>,<sup id="fnref:15"><a href="#fn:15" class="footnote" rel="footnote" role="doc-noteref">[footnote 15]</a></sup> we proposed a small coordination layer within the regulatory architecture. Industry and civil society were supportive of our intention to ensure coherence across the <abbr title="Artificial Intelligence">AI</abbr> regulatory framework. However, feedback often argued strongly for greater central coordination to support regulators on issues requiring cross-cutting collaboration and ensure that the overall regulatory framework functions as intended.</p>

<p>14. We have identified a number of central support functions required to make sure that the overall framework offers a proportionate but effective response to risk while promoting innovation across the regulatory landscape:</p>

<ul>
  <li>Monitoring and evaluation of the overall regulatory framework’s effectiveness and the implementation of the principles, including the extent to which implementation supports innovation. This will allow us to remain responsive and adapt the framework if necessary, including where it needs to be adapted to remain effective in the context of developments in <abbr title="Artificial Intelligence">AI</abbr>’s capabilities and the state of the art.<br>
</li>
  <li>Assessing and monitoring risks across the economy arising from <abbr title="Artificial Intelligence">AI</abbr>.<br>
</li>
  <li>Conducting horizon scanning and gap analysis, including by convening industry, to inform a coherent response to emerging <abbr title="Artificial Intelligence">AI</abbr> technology trends.<br>
</li>
  <li>Supporting testbeds and sandbox initiatives to help <abbr title="Artificial Intelligence">AI</abbr> innovators get new technologies to market.<br>
</li>
  <li>Providing education and awareness to give clarity to businesses and empower citizens to make their voices heard as part of the ongoing iteration of the framework.<br>
</li>
  <li>Promoting interoperability with international regulatory frameworks.<br>
</li>
</ul>

<p>15. The central support functions will initially be provided from within government but will leverage existing activities and expertise from across the broader economy. The activities described above will neither replace nor duplicate the work undertaken by regulators and will not involve the creation of a new <abbr title="Artificial Intelligence">AI</abbr> regulator.</p>

<p>16. Our proportionate approach recognises that regulation is not always the most effective way to support responsible innovation. The proposed framework is aligned with, and supplemented by, a variety of tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr>, such as assurance techniques, voluntary guidance and technical standards. Government will promote the use of such tools. We are collaborating with partners like the <a rel="external" href="https://aistandardshub.org/">UK <abbr title="Artificial Intelligence">AI</abbr> Standards Hub</a> to ensure that our overall governance framework encourages responsible <abbr title="Artificial Intelligence">AI</abbr> innovation (see <a href="#partfour">part 4</a> for details).</p>

<p>17. In keeping with the global nature of these technologies, we will also continue to work with international partners to deliver interoperable measures that incentivise the responsible design, development and application of <abbr title="Artificial Intelligence">AI</abbr>. During our call for views, industry, academia and civil society stressed that international alignment should support UK businesses to capitalise on global markets and protect UK citizens from cross-border harms.</p>

<p>18. The UK is frequently ranked third in the world across a range of measures, including level of investment, innovation and implementation of <abbr title="Artificial Intelligence">AI</abbr>.<sup id="fnref:16"><a href="#fn:16" class="footnote" rel="footnote" role="doc-noteref">[footnote 16]</a></sup> To make the UK the most attractive place in the world for <abbr title="Artificial Intelligence">AI</abbr> innovation and support UK companies wishing to export and attract international investment, we must ensure international compatibility between approaches. Countries around the world, as well as multilateral forums, are exploring approaches to regulating <abbr title="Artificial Intelligence">AI</abbr>. Thanks to our reputation for pragmatic regulation, the UK is rightly seen by international partners as a leader in this global conversation.</p>

<h2 id="part-1-introduction">Part 1: Introduction</h2>

<h3 id="the-power-and-potential-of-artificial-intelligence">1.1 The power and potential of artificial intelligence</h3>

<p>19. <abbr title="Artificial Intelligence">AI</abbr> is already delivering major advances and efficiencies in many areas. <abbr title="Artificial Intelligence">AI</abbr> quietly automates aspects of our everyday activities, from systems that monitor traffic to make our commutes smoother,<sup id="fnref:17"><a href="#fn:17" class="footnote" rel="footnote" role="doc-noteref">[footnote 17]</a></sup> to those that detect fraud in our bank accounts.<sup id="fnref:18"><a href="#fn:18" class="footnote" rel="footnote" role="doc-noteref">[footnote 18]</a></sup> <abbr title="Artificial Intelligence">AI</abbr> has revolutionised large-scale safety-critical practices in industry, like controlling the process of nuclear fusion.<sup id="fnref:19"><a href="#fn:19" class="footnote" rel="footnote" role="doc-noteref">[footnote 19]</a></sup> And it has also been used to accelerate scientific advancements, such as the discovery of new medicine<sup id="fnref:20"><a href="#fn:20" class="footnote" rel="footnote" role="doc-noteref">[footnote 20]</a></sup> or the technologies we need to tackle climate change.<sup id="fnref:21"><a href="#fn:21" class="footnote" rel="footnote" role="doc-noteref">[footnote 21]</a></sup></p>

<p>20. But this is just the beginning. <abbr title="Artificial Intelligence">AI</abbr> can be used in a huge variety of settings and has the extraordinary potential to transform our society and economy.<sup id="fnref:22"><a href="#fn:22" class="footnote" rel="footnote" role="doc-noteref">[footnote 22]</a></sup> It could have as much impact as electricity or the internet, and has been identified as one of the 5 critical technologies in the UK Science and Technology Framework.<sup id="fnref:23"><a href="#fn:23" class="footnote" rel="footnote" role="doc-noteref">[footnote 23]</a></sup> As <abbr title="Artificial Intelligence">AI</abbr> becomes more powerful, and as innovators explore new ways to use it, we will see more applications of <abbr title="Artificial Intelligence">AI</abbr> emerge. As a result, <abbr title="Artificial Intelligence">AI</abbr> has a huge potential to drive growth<sup id="fnref:24"><a href="#fn:24" class="footnote" rel="footnote" role="doc-noteref">[footnote 24]</a></sup> and create jobs.<sup id="fnref:25"><a href="#fn:25" class="footnote" rel="footnote" role="doc-noteref">[footnote 25]</a></sup> It will support people to carry out their existing jobs, by helping to improve workforce efficiency and workplace safety.<sup id="fnref:26"><a href="#fn:26" class="footnote" rel="footnote" role="doc-noteref">[footnote 26]</a></sup> To remain world leaders in <abbr title="Artificial Intelligence">AI</abbr>, attract global talent and create high-skilled jobs in the UK, we must create a regulatory environment where such innovation can thrive.</p>

<p>21. Technological advances like large language models (<abbr title="large language models">LLMs</abbr>) are an indication of the transformative developments yet to come.<sup id="fnref:27"><a href="#fn:27" class="footnote" rel="footnote" role="doc-noteref">[footnote 27]</a></sup> <abbr title="large language models">LLMs</abbr> provide substantial opportunities to transform the economy and society. For example, <abbr title="large language models">LLMs</abbr> can automate the process of writing code and fixing programming bugs. The technology can support genetic medicine by identifying links between genetic sequences and medical conditions. It can support people to review and summarise key points from lengthy documents. In the last 4 years, <abbr title="large language models">LLMs</abbr> have been developed beyond expectations and they are becoming applicable to an increasingly wide range of tasks.<sup id="fnref:28"><a href="#fn:28" class="footnote" rel="footnote" role="doc-noteref">[footnote 28]</a></sup> We expand on the development of <abbr title="large language model">LLM</abbr> and other foundation models in <a href="#section333">section 3.3.3</a> below.</p>

<div class="call-to-action">
  <h4 id="box11">Box 1.1: Examples of <abbr title="Artificial Intelligence">AI</abbr> opportunities</h4>

  <p><br>
<strong><abbr title="Artificial Intelligence">AI</abbr> helps piece together the first complete image of a black hole</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> can enable scientific discovery. A computer vision model was used to piece together the first ever image of a black hole 55 million light years away, combining images from 8 telescopes around the world.<sup id="fnref:29"><a href="#fn:29" class="footnote" rel="footnote" role="doc-noteref">[footnote 29]</a></sup></p>

  <p><strong><abbr title="Artificial Intelligence">AI</abbr> solves decades old protein-folding puzzle</strong></p>

  <p>An <abbr title="Artificial Intelligence">AI</abbr> company based in the UK trained neural networks to predict the structures of proteins, solving a problem that had long stumped scientists. The predictions are advancing the field of structural biology: scientists have already used them to prevent antibiotic resistance,<sup id="fnref:30"><a href="#fn:30" class="footnote" rel="footnote" role="doc-noteref">[footnote 30]</a></sup> advance disease research,<sup id="fnref:31"><a href="#fn:31" class="footnote" rel="footnote" role="doc-noteref">[footnote 31]</a></sup> and accelerate the fight against plastic pollution.<sup id="fnref:32"><a href="#fn:32" class="footnote" rel="footnote" role="doc-noteref">[footnote 32]</a></sup> As we find more uses for <abbr title="Artificial Intelligence">AI</abbr>, it will rewrite scientific fields and change the way we learn about our world.</p>

  <p><strong>Deep learning <abbr title="Artificial Intelligence">AI</abbr> could improve breast cancer screening</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> could transform how diseases are detected, prevented, and treated. Doctors are testing if deep learning can be applied to breast cancer screening. Currently, every mammogram is double-checked by radiologists but this is labour-intensive and causes diagnosis delays. A UK medical technology company is working with the <abbr title="National Health Service">NHS</abbr> to test <abbr title="Artificial Intelligence">AI</abbr> for the second screening, meaning greater numbers of patients could be screened faster and clinicians could spend more time with patients and provide faster access to treatment.<sup id="fnref:33"><a href="#fn:33" class="footnote" rel="footnote" role="doc-noteref">[footnote 33]</a></sup></p>

  <p><strong>Farming efficiency increased by <abbr title="Artificial Intelligence">AI</abbr> robots</strong></p>

  <p>Applying robotics and <abbr title="Artificial Intelligence">AI</abbr> to field management can make farming more efficient, sustainable and productive. Lightweight, autonomous mapping and monitoring robots operating across the UK can spend hours on the field in all conditions and significantly reduce soil compaction. These systems can digitise the field, providing farmers with data to improve weed and pest management. If these systems become widely used, they could contribute to agricultural and horticultural productivity, reduce the pressure of labour shortages and better preserve the environment.<sup id="fnref:34"><a href="#fn:34" class="footnote" rel="footnote" role="doc-noteref">[footnote 34]</a></sup></p>

  <p><strong><abbr title="Artificial Intelligence">AI</abbr> helps accelerate the discovery of new medicines</strong></p>

  <p>Significant time and resources are currently needed to develop new and effective medicines. <abbr title="Artificial Intelligence">AI</abbr> can accelerate the discovery of new medicines by quickly identifying potential biologically active compounds from millions of candidates within a short period.<sup id="fnref:35"><a href="#fn:35" class="footnote" rel="footnote" role="doc-noteref">[footnote 35]</a></sup> Scientists may also have succeeded in using generative <abbr title="Artificial Intelligence">AI</abbr> to design antibodies that bind to a human protein linked to cancer.<sup id="fnref:36"><a href="#fn:36" class="footnote" rel="footnote" role="doc-noteref">[footnote 36]</a></sup></p>

  <p><strong><abbr title="Artificial Intelligence">AI</abbr> is used in the fight against the most serious and harmful crimes</strong></p>

  <p>The Child Images Abuse Database<sup id="fnref:37"><a href="#fn:37" class="footnote" rel="footnote" role="doc-noteref">[footnote 37]</a></sup> uses the powerful data processing capabilities of <abbr title="Artificial Intelligence">AI</abbr> to identify victims and perpetrators of child sexual abuse. The quick and effective identification of victims and perpetrators in digital abuse images allows for real world action to remove victims from harm and ensure their abusers are held to account. The use of <abbr title="Artificial Intelligence">AI</abbr> increases the scale and speed of analysis while protecting staff welfare by reducing their exposure to distressing content.</p>

  <p><strong><abbr title="Artificial Intelligence">AI</abbr> increases cyber security capabilities</strong></p>

  <p><a rel="external" href="https://darktrace.com/news/darktrace-artificial-intelligence-autonomously-stops-consequences-of-fast-moving-cyber-attack-at-major-italian-electronics-distributor-6">Companies providing cyber security services are increasingly using <abbr title="Artificial Intelligence">AI</abbr></a> to analyse large amounts of data about malware and respond to vulnerabilities in network security at faster-than-human speeds.<sup id="fnref:38"><a href="#fn:38" class="footnote" rel="footnote" role="doc-noteref">[footnote 38]</a></sup> As the complexity of the cyber threat landscape evolves, the pattern-recognition and recursive learning capabilities of <abbr title="Artificial Intelligence">AI</abbr> are likely to play an increasingly significant role in proactive cyber defence against malicious actors.</p>
</div>

<h3 id="managing-ai-risks">1.2 Managing <abbr title="Artificial Intelligence">AI</abbr> risks</h3>

<p>22. The concept of <abbr title="Artificial Intelligence">AI</abbr> is not new, but recent advances in data generation and processing have changed the field and the technology it produces. For example, while recent developments in the capabilities of generative <abbr title="Artificial Intelligence">AI</abbr> models have created exciting opportunities, they have also sparked new debates about potential <abbr title="Artificial Intelligence">AI</abbr> risks.<sup id="fnref:39"><a href="#fn:39" class="footnote" rel="footnote" role="doc-noteref">[footnote 39]</a></sup> As <abbr title="Artificial Intelligence">AI</abbr> research and development continues at pace and scale, we expect to see even greater impact and public awareness of <abbr title="Artificial Intelligence">AI</abbr> risks.<sup id="fnref:40"><a href="#fn:40" class="footnote" rel="footnote" role="doc-noteref">[footnote 40]</a></sup></p>

<p>23. We know that not all <abbr title="Artificial Intelligence">AI</abbr> risks arise from the deliberate action of bad actors. Some <abbr title="Artificial Intelligence">AI</abbr> risks can emerge as an unintended consequence or from a lack of appropriate controls to ensure responsible <abbr title="Artificial Intelligence">AI</abbr> use.<sup id="fnref:41"><a href="#fn:41" class="footnote" rel="footnote" role="doc-noteref">[footnote 41]</a></sup></p>

<p>24. We have made an initial assessment of <abbr title="Artificial Intelligence">AI</abbr>-specific risks and their potential to cause harm, with reference in our analysis to the values that they threaten if left unaddressed. These values include safety, security, fairness, privacy and agency, human rights, societal well-being and prosperity.</p>

<p>25. Our assessment of cross-cutting <abbr title="Artificial Intelligence">AI</abbr> risk identified a range of high-level risks that our framework will seek to prioritise and mitigate with proportionate interventions. For example, safety risks include physical damage to humans and property, as well as damage to mental health.<sup id="fnref:42"><a href="#fn:42" class="footnote" rel="footnote" role="doc-noteref">[footnote 42]</a></sup> <abbr title="Artificial Intelligence">AI</abbr> creates a range of new security risks to individuals, organisations, and critical infrastructure.<sup id="fnref:43"><a href="#fn:43" class="footnote" rel="footnote" role="doc-noteref">[footnote 43]</a></sup> Without government action, <abbr title="Artificial Intelligence">AI</abbr> could cause and amplify discrimination that results in, for example, unfairness in the justice system.<sup id="fnref:44"><a href="#fn:44" class="footnote" rel="footnote" role="doc-noteref">[footnote 44]</a></sup> Similarly, without regulatory oversight, <abbr title="Artificial Intelligence">AI</abbr> technologies could pose risks to our privacy and human dignity, potentially harming our fundamental liberties.<sup id="fnref:45"><a href="#fn:45" class="footnote" rel="footnote" role="doc-noteref">[footnote 45]</a></sup> Our regulatory intervention will ensure that <abbr title="Artificial Intelligence">AI</abbr> does not cause harm at a societal level, threatening democracy<sup id="fnref:46"><a href="#fn:46" class="footnote" rel="footnote" role="doc-noteref">[footnote 46]</a></sup> or UK values.</p>

<div class="call-to-action">
  <h4 id="box-12-illustrative-ai-risks">Box 1.2: Illustrative <abbr title="Artificial Intelligence">AI</abbr> risks</h4>

  <p><br>
The patchwork of legal frameworks that currently regulate some uses of <abbr title="Artificial Intelligence">AI</abbr> may not sufficiently address the risks that <abbr title="Artificial Intelligence">AI</abbr> can pose. The following examples are <strong>hypothetical scenarios</strong> designed to illustrate <abbr title="Artificial Intelligence">AI</abbr>’s potential to create harm.</p>

  <p><strong>Risks to human rights</strong></p>

  <p>Generative <abbr title="Artificial Intelligence">AI</abbr> is used to generate deepfake pornographic video content, potentially damaging the reputation, relationships and dignity of the subject.</p>

  <p><strong>Risks to safety</strong></p>

  <p>An <abbr title="Artificial Intelligence">AI</abbr> assistant based on <abbr title="large language model">LLM</abbr> technology recommends a dangerous activity that it has found on the internet, without understanding or communicating the context of the website where the activity was described. The user undertakes this activity causing physical harm.</p>

  <p><strong>Risks to fairness</strong><sup id="fnref:47"><a href="#fn:47" class="footnote" rel="footnote" role="doc-noteref">[footnote 47]</a></sup></p>

  <p>An <abbr title="Artificial Intelligence">AI</abbr> tool assessing credit-worthiness of loan applicants is trained on incomplete or biased data, leading the company to offer loans to individuals on different terms based on characteristics like race or gender.</p>

  <p><strong>Risks to privacy and agency</strong></p>

  <p>Connected devices in the home may constantly gather data, including conversations, potentially creating a near-complete portrait of an individual’s home life. Privacy risks are compounded the more parties can access this data.</p>

  <p><strong>Risks to societal wellbeing</strong></p>

  <p>Disinformation generated and propagated by <abbr title="Artificial Intelligence">AI</abbr> could undermine access to reliable information and trust in democratic institutions and processes.</p>

  <p><strong>Risks to security</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> tools can be used to automate, accelerate and magnify the impact of highly targeted cyber attacks, increasing the severity of the threat from malicious actors. The emergence of <abbr title="large language models">LLMs</abbr> enables hackers<sup id="fnref:48"><a href="#fn:48" class="footnote" rel="footnote" role="doc-noteref">[footnote 48]</a></sup> with little technical knowledge or skill to generate phishing campaigns with malware delivery capabilities.<sup id="fnref:49"><a href="#fn:49" class="footnote" rel="footnote" role="doc-noteref">[footnote 49]</a></sup></p>
</div>

<h3 id="a-note-on-terminology">1.3 A note on terminology</h3>

<p>Terminology used in this paper:<sup id="fnref:50"><a href="#fn:50" class="footnote" rel="footnote" role="doc-noteref">[footnote 50]</a></sup></p>

<p><strong><abbr title="Artificial Intelligence">AI</abbr></strong> or <strong><abbr title="Artificial Intelligence">AI</abbr> system</strong> or <strong><abbr title="Artificial Intelligence">AI</abbr> technologies</strong>: products and services that are ‘adaptable’ and ‘autonomous’ in the sense outlined in our definition in <a href="#section321">section 3.2.1.</a></p>

<p><strong><abbr title="Artificial Intelligence">AI</abbr> supplier</strong>: any organisation or individual who plays a role in the research, development, training, implementation, deployment, maintenance, provision or sale of <abbr title="Artificial Intelligence">AI</abbr> systems.</p>

<p><strong><abbr title="Artificial Intelligence">AI</abbr> user</strong>: any individual or organisation that uses an <abbr title="Artificial Intelligence">AI</abbr> product.</p>

<p><strong><abbr title="Artificial Intelligence">AI</abbr> life cycle</strong>: all events and processes that relate to an <abbr title="Artificial Intelligence">AI</abbr> system’s lifespan, from inception to decommissioning, including its design, research, training, development, deployment, integration, operation, maintenance, sale, use and governance.</p>

<p><strong><abbr title="Artificial Intelligence">AI</abbr> ecosystem</strong>: the complex network of actors and processes that enable the use and supply of <abbr title="Artificial Intelligence">AI</abbr> throughout the <abbr title="Artificial Intelligence">AI</abbr> life cycle (including supply chains, markets, and governance mechanisms).</p>

<p><strong>Foundation model</strong>: a type of <abbr title="Artificial Intelligence">AI</abbr> model that is trained on a vast quantity of data and is adaptable for use on a wide range of tasks. Foundation models can be used as a base for building more specific <abbr title="Artificial Intelligence">AI</abbr> models. Foundation models are discussed in more detail in section <a href="#section333">3.3.3</a> below.<sup id="fnref:51"><a href="#fn:51" class="footnote" rel="footnote" role="doc-noteref">[footnote 51]</a></sup></p>

<p><strong>Impacted third party</strong>: an individual or company that is impacted by the outcomes of the <abbr title="Artificial Intelligence">AI</abbr> systems that they do not use or supply themselves.</p>

<h2 id="part-2-the-current-regulatory-environment">Part 2: The current regulatory environment</h2>

<h3 id="navigating-the-current-landscape">2.1 Navigating the current landscape</h3>

<p>26. The UK’s <abbr title="Artificial Intelligence">AI</abbr> success is, in part, due to our reputation for high-quality regulators and our strong approach to the rule of law, supported by our technology-neutral legislation and regulations. UK laws, regulators and courts already address some of the emerging risks posed by <abbr title="Artificial Intelligence">AI</abbr> technologies (see <a href="#box21">box 2.1</a> for examples). This strong legal foundation encourages investment in new technologies, enabling <abbr title="Artificial Intelligence">AI</abbr> innovation to thrive,<sup id="fnref:52"><a href="#fn:52" class="footnote" rel="footnote" role="doc-noteref">[footnote 52]</a></sup> and high-quality jobs to flourish.<sup id="fnref:53"><a href="#fn:53" class="footnote" rel="footnote" role="doc-noteref">[footnote 53]</a></sup></p>

<div class="call-to-action">
  <h4 id="box21">Box 2.1: Example of legal coverage of <abbr title="Artificial Intelligence">AI</abbr> in the UK and potential gaps</h4>

  <p><br>
Discriminatory outcomes that result from the use of <abbr title="Artificial Intelligence">AI</abbr> may contravene the protections set out in the Equality Act 2010.<sup id="fnref:54"><a href="#fn:54" class="footnote" rel="footnote" role="doc-noteref">[footnote 54]</a></sup> <abbr title="Artificial Intelligence">AI</abbr> systems are also required by data protection law to process personal data fairly.<sup id="fnref:55"><a href="#fn:55" class="footnote" rel="footnote" role="doc-noteref">[footnote 55]</a></sup> However, <abbr title="Artificial Intelligence">AI</abbr> can increase the risk of unfair bias or discrimination across a range of indicators or characteristics. This could undermine public trust in <abbr title="Artificial Intelligence">AI</abbr>.</p>

  <p>Product safety laws ensure that goods manufactured and placed on the market in the UK are safe. Product-specific legislation (such as for electrical and electronic equipment,<sup id="fnref:56"><a href="#fn:56" class="footnote" rel="footnote" role="doc-noteref">[footnote 56]</a></sup> medical devices,<sup id="fnref:57"><a href="#fn:57" class="footnote" rel="footnote" role="doc-noteref">[footnote 57]</a></sup> and toys<sup id="fnref:58"><a href="#fn:58" class="footnote" rel="footnote" role="doc-noteref">[footnote 58]</a></sup>) may apply to some products that include integrated <abbr title="Artificial Intelligence">AI</abbr>. However, safety risks specific to <abbr title="Artificial Intelligence">AI</abbr> technologies should be monitored closely. As the capability and adoption of <abbr title="Artificial Intelligence">AI</abbr> increases, it may pose new and substantial risks that are unaddressed by existing rules.</p>

  <p>Consumer rights law<sup id="fnref:59"><a href="#fn:59" class="footnote" rel="footnote" role="doc-noteref">[footnote 59]</a></sup> may protect consumers where they have entered into a sales contract for <abbr title="Artificial Intelligence">AI</abbr>-based products and services. Certain contract terms (for example, that goods are of satisfactory quality, fit for a particular purpose, and as described) are relevant to consumer contracts. Similarly, businesses are prohibited from including certain terms in consumer contracts. Tort law provides a complementary regime that may provide redress where a civil wrong has caused harm. It is not yet clear whether consumer rights law will provide the right level of protection in the context of products that include integrated <abbr title="Artificial Intelligence">AI</abbr> or services based on <abbr title="Artificial Intelligence">AI</abbr>, or how tort law may apply to fill any gap in consumer rights law protection.</p>
</div>

<p>27. While <abbr title="Artificial Intelligence">AI</abbr> is currently regulated through existing legal frameworks like financial services regulation,<sup id="fnref:60"><a href="#fn:60" class="footnote" rel="footnote" role="doc-noteref">[footnote 60]</a></sup> some <abbr title="Artificial Intelligence">AI</abbr> risks arise across, or in the gaps between, existing regulatory remits. Industry told us that conflicting or uncoordinated requirements from regulators create unnecessary burdens and that regulatory gaps may leave risks unmitigated, harming public trust and slowing <abbr title="Artificial Intelligence">AI</abbr> adoption.</p>

<p>28. Industry has warned us that regulatory incoherence could stifle innovation and competition by causing a disproportionate amount of smaller businesses to leave the market. If regulators are not proportionate and aligned in their regulation of <abbr title="Artificial Intelligence">AI</abbr>, businesses may have to spend excessive time and money complying with complex rules instead of creating new technologies. Small businesses and start-ups often do not have the resources to do both.<sup id="fnref:61"><a href="#fn:61" class="footnote" rel="footnote" role="doc-noteref">[footnote 61]</a></sup> With the vast majority of digital technology businesses employing under 50 people,<sup id="fnref:62"><a href="#fn:62" class="footnote" rel="footnote" role="doc-noteref">[footnote 62]</a></sup> it is important to ensure that regulatory burdens do not fall disproportionately on smaller companies, which play an essential role in the <abbr title="Artificial Intelligence">AI</abbr> innovation ecosystem and act as engines for economic growth and job creation.<sup id="fnref:63"><a href="#fn:63" class="footnote" rel="footnote" role="doc-noteref">[footnote 63]</a></sup></p>

<p>29. Regulatory coordination will support businesses to invest confidently in <abbr title="Artificial Intelligence">AI</abbr> innovation and build public trust by ensuring real risks are effectively addressed. While some regulators already work together to ensure regulatory coherence for <abbr title="Artificial Intelligence">AI</abbr> through formal networks like the <abbr title="Artificial Intelligence">AI</abbr> and digital regulations service in the health sector<sup id="fnref:64"><a href="#fn:64" class="footnote" rel="footnote" role="doc-noteref">[footnote 64]</a></sup> and the Digital Regulation Cooperation Forum (<abbr title="Digital Regulation Cooperation Forum">DRCF</abbr>), other regulators have limited capacity and access to <abbr title="Artificial Intelligence">AI</abbr> expertise. This creates the risk of inconsistent enforcement across regulators. There is also a risk that some regulators could begin to dominate and interpret the scope of their remit or role more broadly than may have been intended in order to fill perceived gaps in a way that increases incoherence and uncertainty. Industry asked us to support further system-wide coordination to clarify who is responsible for addressing cross-cutting <abbr title="Artificial Intelligence">AI</abbr> risks and avoid duplicate requirements across multiple regulators.</p>

<div class="call-to-action">
  <h4 id="case-study-21-addressing-ai-fairness-under-the-existing-legal-and-regulatory-framework">Case study 2.1: Addressing <abbr title="Artificial Intelligence">AI</abbr> fairness under the existing legal and regulatory framework</h4>

  <p>A fictional company, ‘<abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited’, is designing a new <abbr title="Artificial Intelligence">AI</abbr>-driven algorithm to set prices for insurance premiums that accurately reflect a client’s risk. Setting fair prices and building consumer trust is a key component of <abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited’s brand so ensuring it complies with the relevant legislation and guidance is a priority.</p>

  <p>Fairness in <abbr title="Artificial Intelligence">AI</abbr> systems is covered by a variety of regulatory requirements and best practice. <abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited’s use of <abbr title="Artificial Intelligence">AI</abbr> to set prices for insurance premiums could be subject to a range of legal frameworks, including data protection, equality, and general consumer protection laws. It could also be subject to sectoral rules like the Financial Services and Markets Act 2000.<sup id="fnref:65"><a href="#fn:65" class="footnote" rel="footnote" role="doc-noteref">[footnote 65]</a></sup></p>

  <p>It can be challenging for a company like <abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited to identify which rules are relevant and confidently apply them to <abbr title="Artificial Intelligence">AI</abbr> use cases. There is currently a lack of support for businesses like <abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited to navigate the regulatory landscape, with no cross-cutting principles and limited system-wide coordination.</p>
</div>

<p>30. Government intervention is needed to improve the regulatory landscape. We intend to leverage and build on existing regimes, maximising the benefits of what we already have, while intervening in a proportionate way to address regulatory uncertainty and gaps. This will deliver a pro-innovation regulatory framework that is designed to be adaptable and future-proof, supported by tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> including assurance techniques and technical standards. This approach will provide more clarity and encourage collaboration between government, regulators and industry to unlock innovation.</p>

<div class="call-to-action">
  <h4 id="case-study-22-adapting-regulatory-approaches-to-ai--ai-as-a-medical-device">Case study 2.2: Adapting regulatory approaches to <abbr title="Artificial Intelligence">AI</abbr> – <abbr title="Artificial Intelligence">AI</abbr> as a medical device</h4>

  <p>Some UK regulators have led the way and proactively adapted their approaches to <abbr title="Artificial Intelligence">AI</abbr>-enabled technologies.</p>

  <p>In 2022, the <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr> (Medicines and Healthcare products Regulatory Agency) published a roadmap clarifying in guidance the requirements for <abbr title="Artificial Intelligence">AI</abbr> and software used in medical devices.<sup id="fnref:66"><a href="#fn:66" class="footnote" rel="footnote" role="doc-noteref">[footnote 66]</a></sup> regulator is also updating the regulatory framework for medical devices to protect patients and secure the UK’s global reputation for responsible innovation in medical device software.</p>

  <p>As part of this work, the <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr> will develop guidance on the transparency and interpretability of <abbr title="Artificial Intelligence">AI</abbr> as a medical device.<sup id="fnref:67"><a href="#fn:67" class="footnote" rel="footnote" role="doc-noteref">[footnote 67]</a></sup> The <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr> will consider the specific challenges posed by <abbr title="Artificial Intelligence">AI</abbr> in this context, drawing on the applicable <abbr title="Artificial Intelligence">AI</abbr> regulation cross-sectoral principles and ethical principles for <abbr title="Artificial Intelligence">AI</abbr> in health and social care to issue practical guidance on how to meet legal product safety requirements. The <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr> will work with other regulators such as the Information Commissioner’s Office (<abbr title="Information Commissioner’s Office">ICO</abbr>) and the National Data Guardian to consider patients’ data protection and trust in medical devices.</p>

  <p>This work will provide manufacturers with clear requirements and guidance to attract responsible innovation to the UK.</p>
</div>

<h2 id="part-3-an-innovative-and-iterative-approach">Part 3: An innovative and iterative approach</h2>

<h3 id="aims-of-the-regulatory-framework">3.1 Aims of the regulatory framework</h3>

<p>31. Regulation can increase innovation by giving businesses the incentive to solve important problems while addressing the risk of harm to citizens. For example, product safety legislation has increased innovation towards safer products and services.<sup id="fnref:68"><a href="#fn:68" class="footnote" rel="footnote" role="doc-noteref">[footnote 68]</a></sup> In the case of <abbr title="Artificial Intelligence">AI</abbr>, a context-based, proportionate approach to regulation will help strengthen public trust and increase <abbr title="Artificial Intelligence">AI</abbr> adoption.<sup id="fnref:69"><a href="#fn:69" class="footnote" rel="footnote" role="doc-noteref">[footnote 69]</a></sup></p>

<p>32. The National <abbr title="Artificial Intelligence">AI</abbr> Strategy set out our aim to regulate <abbr title="Artificial Intelligence">AI</abbr> effectively and support innovation.<sup id="fnref:70"><a href="#fn:70" class="footnote" rel="footnote" role="doc-noteref">[footnote 70]</a></sup> In line with the principles set out in the Plan for Digital Regulation,<sup id="fnref:71"><a href="#fn:71" class="footnote" rel="footnote" role="doc-noteref">[footnote 71]</a></sup> our approach to <abbr title="Artificial Intelligence">AI</abbr> regulation will be proportionate; balancing real risks against the opportunities and benefits that <abbr title="Artificial Intelligence">AI</abbr> can generate. We will maintain an effective balance as we implement the framework by focusing on the context and outcomes of <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>33. Our policy paper proposed a pro-innovation framework designed to give consumers the confidence to use <abbr title="Artificial Intelligence">AI</abbr> products and services, and provide businesses the clarity they need to invest in <abbr title="Artificial Intelligence">AI</abbr> and innovate responsibly.<sup id="fnref:72"><a href="#fn:72" class="footnote" rel="footnote" role="doc-noteref">[footnote 72]</a></sup> This approach was broadly welcomed – particularly by industry. Based on feedback, we have distilled our aims into 3 objectives that our framework is designed to achieve:</p>

<ul>
  <li>
<strong>Drive growth and prosperity</strong> by making responsible innovation easier and reducing regulatory uncertainty. This will encourage investment in <abbr title="Artificial Intelligence">AI</abbr> and support its adoption throughout the economy, creating jobs and helping us to do them more efficiently.<br><br>To achieve this objective we must act quickly to remove existing barriers to innovation and prevent the emergence of new ones. This will allow <abbr title="Artificial Intelligence">AI</abbr> companies to capitalise on early development successes and achieve long term market advantage.<sup id="fnref:73"><a href="#fn:73" class="footnote" rel="footnote" role="doc-noteref">[footnote 73]</a></sup> By acting now, we can give UK innovators a headstart in the global race to convert the potential of <abbr title="Artificial Intelligence">AI</abbr> into long term advantages for the UK, maximising the economic and social value of these technologies and strengthening our current position as a world leader in <abbr title="Artificial Intelligence">AI</abbr>.<sup id="fnref:74"><a href="#fn:74" class="footnote" rel="footnote" role="doc-noteref">[footnote 74]</a></sup>
<br><br>
</li>
  <li>
<strong>Increase public trust in <abbr title="Artificial Intelligence">AI</abbr></strong> by addressing risks and protecting our fundamental values.<br><br>Trust is a critical driver for <abbr title="Artificial Intelligence">AI</abbr> adoption.<sup id="fnref:75"><a href="#fn:75" class="footnote" rel="footnote" role="doc-noteref">[footnote 75]</a></sup> If people do not trust <abbr title="Artificial Intelligence">AI</abbr>, they will be reluctant to use it. Such reluctance can reduce demand for <abbr title="Artificial Intelligence">AI</abbr> products and hinder innovation. Therefore we must demonstrate that our regulatory framework (described in section <a href="#32">3.2</a>) effectively addresses <abbr title="Artificial Intelligence">AI</abbr> risks.
<br><br>
</li>
  <li>
<strong>Strengthen the UK’s position as a global leader in <abbr title="Artificial Intelligence">AI</abbr>.</strong> The development of <abbr title="Artificial Intelligence">AI</abbr> technologies can address some of the most pressing global challenges, from climate change to future pandemics. There is also growing international recognition that <abbr title="Artificial Intelligence">AI</abbr> requires new regulatory responses to guide responsible innovation.<br><br>The UK can play a central role in the global conversation by shaping international governance and regulation to maximise opportunities and build trust in the technology, while mitigating potential cross-border risks and protecting our democratic values. There is also an important leadership role for the UK in the development of the global <abbr title="Artificial Intelligence">AI</abbr> assurance industry,<sup id="fnref:76"><a href="#fn:76" class="footnote" rel="footnote" role="doc-noteref">[footnote 76]</a></sup> including in auditing and safety.<br><br>We will ensure that the UK remains attractive to innovators and investors by promoting interoperability with other regulatory approaches and minimising cross-border frictions. We will work closely with global partners through multilateral and bilateral engagements to learn from, influence and adapt as international and domestic approaches to <abbr title="Artificial Intelligence">AI</abbr> regulation continue to emerge (see <a href="#partsix">part 6</a>).</li>
</ul>

<p>34. The proposed regulatory framework does not seek to address all of the wider societal and global challenges that may relate to the development or use of <abbr title="Artificial Intelligence">AI</abbr>. This includes issues relating to access to data, compute capability, and sustainability, as well as the balancing of the rights of content producers and <abbr title="Artificial Intelligence">AI</abbr> developers. These are important issues to consider – especially in the context of the UK’s ability to maintain its place as a global leader in <abbr title="Artificial Intelligence">AI</abbr> – but they are outside of the scope of our proposals for a new overarching framework for <abbr title="Artificial Intelligence">AI</abbr> regulation.</p>

<p>35. Government is taking wider action to ensure the UK retains its status as a global leader in <abbr title="Artificial Intelligence">AI</abbr>, for example by taking forward Sir Patrick Vallance’s recommendation relating to intellectual property law and generative <abbr title="Artificial Intelligence">AI</abbr>.<sup id="fnref:77"><a href="#fn:77" class="footnote" rel="footnote" role="doc-noteref">[footnote 77]</a></sup> This will ensure we keep the right balance between protecting rights holders and our thriving creative industries, while supporting <abbr title="Artificial Intelligence">AI</abbr> developers to access the data they need.</p>

<h3 id="the-proposed-regulatory-framework">3.2 The proposed regulatory framework</h3>

<p id="paragraph37">36. Our innovative approach to <abbr title="Artificial Intelligence">AI</abbr> regulation uses a principles-based framework for regulators to interpret and apply to <abbr title="Artificial Intelligence">AI</abbr> within their remits. This collaborative and iterative approach can keep pace with a fast moving technology that requires proportionate action to balance risk and opportunity and to strengthen the UK’s position as a global leader in <abbr title="Artificial Intelligence">AI</abbr>. Our agile approach aligns with Sir Patrick Vallance’s Regulation for Innovation report,<sup id="fnref:78"><a href="#fn:78" class="footnote" rel="footnote" role="doc-noteref">[footnote 78]</a></sup> which highlights that flexible regulatory approaches can better strike the balance between providing clarity, building trust and enabling experimentation. Our framework will provide more clarity to innovators by encouraging collaboration between government, regulators, industry and civil society.</p>
<p>37. We have identified the essential characteristics of our regulatory regime. Our framework will be <strong>pro-innovation</strong>, <strong>proportionate</strong>, <strong>trustworthy</strong>, <strong>adaptable</strong>, <strong>clear</strong> and <strong>collaborative</strong>.<sup id="fnref:79"><a href="#fn:79" class="footnote" rel="footnote" role="doc-noteref">[footnote 79]</a></sup></p>

<ul>
  <li>
<strong>Pro-innovation</strong>: enabling rather than stifling responsible innovation.<br>
</li>
  <li>
<strong>Proportionate</strong>: avoiding unnecessary or disproportionate burdens for businesses and regulators.<br>
</li>
  <li>
<strong>Trustworthy</strong>: addressing real risks and fostering public trust in <abbr title="Artificial Intelligence">AI</abbr> in order to promote and encourage its uptake.<br>
</li>
  <li>
<strong>Adaptable</strong>: enabling us to adapt quickly and effectively to keep pace with emergent opportunities and risks as <abbr title="Artificial Intelligence">AI</abbr> technologies evolve.<br>
</li>
  <li>
<strong>Clear</strong>: making it easy for actors in the <abbr title="Artificial Intelligence">AI</abbr> life cycle, including businesses using <abbr title="Artificial Intelligence">AI</abbr>, to know what the rules are, who they apply to, who enforces them, and how to comply with them.<br>
</li>
  <li>
<strong>Collaborative</strong>: encouraging government, regulators, and industry to work together to facilitate <abbr title="Artificial Intelligence">AI</abbr> innovation, build trust and ensure that the voice of the public is heard and considered.<br>
</li>
</ul>

<p>38. The framework, built around the 4 key elements below, is designed to empower our existing regulators and promote coherence across the regulatory landscape. The 4 key elements are:</p>

<ul>
  <li>Defining <abbr title="Artificial Intelligence">AI</abbr> based on its unique characteristics to support regulator coordination (<a href="#section321">section 3.2.1</a>).<br>
</li>
  <li>Adopting a context-specific approach (<a href="#section322">section 3.2.2</a>).<br>
</li>
  <li>Providing a set of cross-sectoral principles to guide regulator responses to <abbr title="Artificial Intelligence">AI</abbr> risks and opportunities(<a href="#section323">section 3.2.3</a>).<br>
    <ul>
      <li>The principles clarify government’s expectations for responsible <abbr title="Artificial Intelligence">AI</abbr> and describe good governance at all stages of the <abbr title="Artificial Intelligence">AI</abbr> life cycle.<br>
</li>
      <li>The application of the principles will initially be at the discretion of the regulators, allowing prioritisation according to the needs of their sectors.<br>
</li>
      <li>Following this initial non-statutory period of implementation, and when parliamentary time allows, we anticipate introducing a statutory duty requiring regulators to have due regard to the principles.</li>
    </ul>
  </li>
  <li>Delivering new central functions to support regulators to deliver the <abbr title="Artificial Intelligence">AI</abbr> regulatory framework, maximising the benefits of an iterative approach and ensuring that the framework is coherent (<a href="#section324">section 3.2.4</a>).<br>
</li>
</ul>

<h3 id="section321">3.2.1 Defining Artificial Intelligence</h3>

<p>39. To regulate <abbr title="Artificial Intelligence">AI</abbr> effectively, and to support the clarity of our proposed framework, we need a common understanding of what is meant by ‘artificial intelligence’. There is no general definition of <abbr title="Artificial Intelligence">AI</abbr> that enjoys widespread consensus.<sup id="fnref:80"><a href="#fn:80" class="footnote" rel="footnote" role="doc-noteref">[footnote 80]</a></sup> That is why we have defined <abbr title="Artificial Intelligence">AI</abbr> by reference to the 2 characteristics that generate the need for a bespoke regulatory response.</p>

<ul>
  <li>The ‘adaptivity’ of <abbr title="Artificial Intelligence">AI</abbr> can make it difficult to explain the intent or logic of the system’s outcomes:<br>
    <ul>
      <li>
<abbr title="Artificial Intelligence">AI</abbr> systems are ‘trained’ – once or continually – and operate by inferring patterns and connections in data which are often not easily discernible to humans.<br>
</li>
      <li>Through such training, <abbr title="Artificial Intelligence">AI</abbr> systems often develop the ability to perform new forms of inference not directly envisioned by their human programmers.<br>
</li>
    </ul>
  </li>
  <li>The ‘autonomy’ of <abbr title="Artificial Intelligence">AI</abbr> can make it difficult to assign responsibility for outcomes:<br>
    <ul>
      <li>Some <abbr title="Artificial Intelligence">AI</abbr> systems can make decisions without the express intent or ongoing control of a human.</li>
    </ul>
  </li>
</ul>

<p>40. The combination of adaptivity and autonomy can make it difficult to explain, predict, or control the outputs of an <abbr title="Artificial Intelligence">AI</abbr> system, or the underlying logic by which they are generated. It can also be challenging to allocate responsibility for the system’s operation and outputs. For regulatory purposes, this has potentially serious implications, particularly when decisions are made relating to significant matters, like an individual’s health, or where there is an expectation that a decision should be justifiable in easily understood terms, like a legal ruling.</p>

<p>41. By defining <abbr title="Artificial Intelligence">AI</abbr> with reference to these functional capabilities and designing our approach to address the challenges created by these characteristics, we future-proof our framework against unanticipated new technologies that are autonomous and adaptive. Because we are not creating blanket new rules for specific technologies or applications of <abbr title="Artificial Intelligence">AI</abbr>, like facial recognition or <abbr title="large language models">LLMs</abbr>, we do not need to use rigid legal definitions. Our use of these defining characteristics was widely supported in responses to our policy paper,<sup id="fnref:81"><a href="#fn:81" class="footnote" rel="footnote" role="doc-noteref">[footnote 81]</a></sup> as rigid definitions can quickly become outdated and restrictive with the rapid evolution of <abbr title="Artificial Intelligence">AI</abbr>.<sup id="fnref:82"><a href="#fn:82" class="footnote" rel="footnote" role="doc-noteref">[footnote 82]</a></sup> We will, however, retain the ability to adapt our approach to defining <abbr title="Artificial Intelligence">AI</abbr> if necessary, alongside the ongoing monitoring and iteration of the wider regulatory framework.</p>

<p>42. Below, we provide some illustrative examples of <abbr title="Artificial Intelligence">AI</abbr> systems to demonstrate their autonomous and adaptive characteristics. While many aspects of the technologies described in these case studies will be covered by existing law, they illustrate how <abbr title="Artificial Intelligence">AI</abbr>-specific characteristics introduce novel risks and regulatory implications.</p>

<h4 id="figure-1-illustration-of-our-strategy-for-regulating-ai">Figure 1: Illustration of our strategy for regulating <abbr title="Artificial Intelligence">AI</abbr>
</h4>

<figure class="image embedded"><div class="img"><img src="https://assets.publishing.service.gov.uk/media/6423094d3d885d000fdadd30/Diagram01__1_.svg" alt=""></div>
<figcaption><p>Illustration of our strategy for regulating AI</p></figcaption></figure>

<div class="call-to-action">
  <h4 id="case-study-31-natural-language-processing-in-customer-service-chatbots">Case study 3.1: Natural language processing in customer service chatbots</h4>

  <p><strong>Adaptivity:</strong> Provides responses to real-time customer messages, having been trained on huge datasets to identify statistical patterns in ordinary human speech, potentially increasing personalisation over time as the system learns from each new experience.</p>

  <p><strong>Autonomy:</strong> Generates a human-like output based on the customer’s text input, to answer queries, help customers find products and services, or send targeted updates. Operates with little need for human oversight or intervention.</p>

  <p><strong>Illustrative <abbr title="Artificial Intelligence">AI</abbr>-related regulatory implication:</strong> Unintentional inclusion of inaccurate or misleading information in training data, producing harmful instructions or convincingly spreading misinformation.</p>
</div>

<div class="call-to-action">
  <h4 id="case-study-32-automated-healthcare-triage-systems">Case study 3.2: Automated healthcare triage systems</h4>

  <p><strong>Adaptivity:</strong> Predicts patient conditions based on the pathology, treatment and risk factors associated with health conditions from the analysis of medical datasets, patient records and real-time health data.</p>

  <p><strong>Autonomy:</strong> Generates information about the likely causes of a patient’s symptoms and recommends potential interventions and treatments, either to a medical professional or straight to a patient.</p>

  <p><strong>Illustrative <abbr title="Artificial Intelligence">AI</abbr>-related regulatory implication:</strong> Unclear liability for an <abbr title="Artificial Intelligence">AI</abbr> triage system that provides incorrect medical advice, leading to negative health outcomes for a patient and affecting the patient’s ability to obtain redress.</p>
</div>

<div class="call-to-action">
  <h4 id="case-study-33-text-to-image-generators">Case study 3.3: Text-to-image generators</h4>

  <p><strong>Adaptivity:</strong> Uses large amounts of online content to learn how to create rich, highly specific images on the basis of a short text prompt.</p>

  <p><strong>Autonomy:</strong> Based on text input, these systems generate images that mimic the qualities of human-created art, with no ongoing oversight from the user.</p>

  <p><strong>Illustrative <abbr title="Artificial Intelligence">AI</abbr>-related regulatory implication:</strong> Reproduction of biases or stereotyping in training data, leading to offensive language or content.</p>
</div>

<p>43. Industry, regulators, and civil society responded positively to our proposed definition, recognising that it supports our context-based and flexible approach to <abbr title="Artificial Intelligence">AI</abbr> regulation. We will monitor how regulators interpret and apply adaptivity and autonomy when formulating domain-specific definitions of <abbr title="Artificial Intelligence">AI</abbr>. Government will support coordination between regulators when we see potential for better alignment between their interpretations and use of our defining characteristics.</p>

<p>44. Active and collaborative horizon scanning will ensure that we can identify developments and emerging trends, and adapt our framework accordingly. We will convene industry, academia and other key stakeholders to inform economy-wide horizon scanning activity. This work will build on the activity of individual regulators.</p>

<h3 id="section322">3.2.2 Regulating the use – not the technology</h3>

<p>45. Our framework is context-specific.<sup id="fnref:83"><a href="#fn:83" class="footnote" rel="footnote" role="doc-noteref">[footnote 83]</a></sup> We will not assign rules or risk levels to entire sectors or technologies. Instead, we will regulate based on the outcomes <abbr title="Artificial Intelligence">AI</abbr> is likely to generate in particular applications. For example, it would not be proportionate or effective to classify all applications of <abbr title="Artificial Intelligence">AI</abbr> in critical infrastructure as high risk. Some uses of <abbr title="Artificial Intelligence">AI</abbr> in critical infrastructure, like the identification of superficial scratches on machinery, can be relatively low risk. Similarly, an <abbr title="Artificial Intelligence">AI</abbr>-powered chatbot used to triage customer service requests for an online clothing retailer should not be regulated in the same way as a similar application used as part of a medical diagnostic process.</p>

<p>46. A context-specific approach allows regulators to weigh the risks of using <abbr title="Artificial Intelligence">AI</abbr> against the costs of missing opportunities to do so.<sup id="fnref:84"><a href="#fn:84" class="footnote" rel="footnote" role="doc-noteref">[footnote 84]</a></sup> Regulators told us that <abbr title="Artificial Intelligence">AI</abbr> risk assessments should include the failure to exploit <abbr title="Artificial Intelligence">AI</abbr> capabilities. For example, there can be a significant opportunity cost related to not having access to <abbr title="Artificial Intelligence">AI</abbr> in safety-critical operations, from heavy industry,<sup id="fnref:85"><a href="#fn:85" class="footnote" rel="footnote" role="doc-noteref">[footnote 85]</a></sup> to personal healthcare (see <a href="#box11">box 1.1</a>). Sensitivity to context will allow the framework to respond to the level of risk in a proportionate manner and avoid stifling innovation or missing opportunities to capitalise on the social benefits made available by <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>47. To best achieve this context-specificity we will empower existing UK regulators to apply the cross-cutting principles. Regulators are best placed to conduct detailed risk analysis and enforcement activities within their areas of expertise. Creating a new <abbr title="Artificial Intelligence">AI</abbr>-specific, cross-sector regulator would introduce complexity and confusion, undermining and likely conflicting with the work of our existing expert regulators.</p>

<h3 id="section323">3.2.3 A principles-based approach</h3>

<p>48. Existing regulators will be expected to implement the framework underpinned by 5 values-focused cross-sectoral principles:</p>

<ul>
  <li>Safety, security and robustness<br>
</li>
  <li>Appropriate transparency and explainability<br>
</li>
  <li>Fairness<br>
</li>
  <li>Accountability and governance<br>
</li>
  <li>Contestability and redress<br>
</li>
</ul>

<p>These build on, and reflect our commitment to, the Organisation for Economic Co-operation and Development (<abbr title="Organisation for Economic Co-operation and Development">OECD</abbr>) values-based <abbr title="Artificial Intelligence">AI</abbr> principles, which promote the ethical use of <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>49. The principles set out the key elements of responsible <abbr title="Artificial Intelligence">AI</abbr> design, development and use, and will help guide businesses. Regulators will lead the implementation of the framework, for example by issuing guidance on best practice for adherence to these principles.</p>

<p>50. Regulators will be expected to apply the principles proportionately to address the risks posed by <abbr title="Artificial Intelligence">AI</abbr> within their remits, in accordance with existing laws and regulations. In this way, the principles will complement existing regulation, increase clarity, and reduce friction for businesses operating across regulatory remits.</p>

<p>51. A principles-based approach allows the framework to be agile and proportionate. It is in line with the Plan for Digital Regulation,<sup id="fnref:86"><a href="#fn:86" class="footnote" rel="footnote" role="doc-noteref">[footnote 86]</a></sup> the findings from the independent Taskforce on Innovation, Growth and Regulatory Reform,<sup id="fnref:87"><a href="#fn:87" class="footnote" rel="footnote" role="doc-noteref">[footnote 87]</a></sup> the Regulatory Horizons Council’s Closing the Gap report on implementing innovation-friendly regulation,<sup id="fnref:88"><a href="#fn:88" class="footnote" rel="footnote" role="doc-noteref">[footnote 88]</a></sup> and Sir Patrick Vallance’s Regulation for Innovation report.<sup id="fnref:89"><a href="#fn:89" class="footnote" rel="footnote" role="doc-noteref">[footnote 89]</a></sup></p>

<p>52. Since publishing the <abbr title="Artificial Intelligence">AI</abbr> regulation policy paper,<sup id="fnref:90"><a href="#fn:90" class="footnote" rel="footnote" role="doc-noteref">[footnote 90]</a></sup> we have updated and strengthened the principles. We have:</p>

<ul>
  <li>Reflected stakeholder feedback by expanding on concepts such as robustness and governance. We have also considered the results of public engagement research that highlighted an expectation for principles such as transparency, fairness and accountability to be included within an <abbr title="Artificial Intelligence">AI</abbr> governance framework.<sup id="fnref:91"><a href="#fn:91" class="footnote" rel="footnote" role="doc-noteref">[footnote 91]</a></sup><br>
</li>
  <li>Merged the safety principle with security and robustness, given the significant overlap between these concepts.<br>
</li>
  <li>Better reflected concepts of accountability and responsibility.<br>
</li>
  <li>Refined each principle’s definition and rationale.<br>
</li>
</ul>

<div class="call-to-action">
  <h4 id="principle--safety-security-robustness">Principle:  Safety, security, robustness</h4>
  <p><br></p>

  <p><strong>Definition and explanation</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> systems should function in a robust, secure and safe way throughout the <abbr title="Artificial Intelligence">AI</abbr> life cycle, and risks should be continually identified, assessed and managed.</p>

  <p>Regulators may need to introduce measures for regulated entities to ensure that <abbr title="Artificial Intelligence">AI</abbr> systems are technically secure and function reliably as intended throughout their entire life cycle.</p>

  <p><strong>Rationale for the principle</strong></p>

  <p>The breadth of possible uses for <abbr title="Artificial Intelligence">AI</abbr> and its capacity to autonomously develop new capabilities and functions mean that <abbr title="Artificial Intelligence">AI</abbr> can have a significant impact on safety and security. Safety-related risks are more apparent in certain domains, such as health or critical infrastructure, but they can materialise in many areas. Safety will be a core consideration for some regulators and more marginal for others. However, it will be important for all regulators to assess the likelihood that <abbr title="Artificial Intelligence">AI</abbr> could pose a risk to safety in their sector or domain, and take a proportionate approach to managing it.</p>

  <p>Additionally, <abbr title="Artificial Intelligence">AI</abbr> systems should be technically secure and should reliably function as intended and described. System developers should be aware of the specific security threats that could apply at different stages of the <abbr title="Artificial Intelligence">AI</abbr> life cycle and embed resilience to these threats into their systems. Other actors should remain vigilant of security issues when they interact with an <abbr title="Artificial Intelligence">AI</abbr> system. We anticipate that regulators may wish to consider the National Cyber Security Centre (<abbr title="National Cyber Security Centre">NCSC</abbr>) principles for securing machine learning models when assessing whether <abbr title="Artificial Intelligence">AI</abbr> actors are adequately prioritising security.<sup id="fnref:92"><a href="#fn:92" class="footnote" rel="footnote" role="doc-noteref">[footnote 92]</a></sup></p>

  <p>When applying this principle, regulators will need to consider providing guidance in a way that is coordinated and coherent with the activities of other regulators. Regulators’ implementation of this principle may require the corresponding <abbr title="Artificial Intelligence">AI</abbr> life cycle actors to regularly test or carry out due diligence on the functioning, resilience and security of a system.<sup id="fnref:93"><a href="#fn:93" class="footnote" rel="footnote" role="doc-noteref">[footnote 93]</a></sup> Regulators may also need to consider technical standards addressing safety, robustness and security to benchmark the safe and robust performance of <abbr title="Artificial Intelligence">AI</abbr> systems and to provide <abbr title="Artificial Intelligence">AI</abbr> life cycle actors with guidance for implementing this principle in their remit.</p>

  <h4 id="principle--appropriate-transparency-and-explainability">Principle:  Appropriate transparency and explainability</h4>
  <p><br></p>

  <p><strong>Definition and explanation</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> systems should be appropriately transparent and explainable.</p>

  <p>Transparency refers to the communication of appropriate information about an <abbr title="Artificial Intelligence">AI</abbr> system to relevant people (for example, information on how, when, and for which purposes an <abbr title="Artificial Intelligence">AI</abbr> system is being used). Explainability refers to the extent to which it is possible for relevant parties to access, interpret and understand the decision-making processes of an <abbr title="Artificial Intelligence">AI</abbr> system.<sup id="fnref:94"><a href="#fn:94" class="footnote" rel="footnote" role="doc-noteref">[footnote 94]</a></sup></p>

  <p>An appropriate level of transparency and explainability will mean that regulators have sufficient information about <abbr title="Artificial Intelligence">AI</abbr> systems and their associated inputs and outputs to give meaningful effect to the other principles (for example, to identify accountability). An appropriate degree of transparency and explainability should be proportionate to the risk(s) presented by an <abbr title="Artificial Intelligence">AI</abbr> system.</p>

  <p>Regulators may need to look for ways to support and encourage relevant life cycle actors to implement appropriate transparency measures, for example through regulatory guidance. Parties directly affected by the use of an <abbr title="Artificial Intelligence">AI</abbr> system should also be able to access sufficient information about <abbr title="Artificial Intelligence">AI</abbr> systems to be able to enforce their rights. In applying the principle to their business processes, relevant life cycle actors may be asked to provide this information in the form and manner required by regulators, including through product labelling. Technical standards could also provide useful guidance on available methods to assess, design, and improve transparency and explainability within <abbr title="Artificial Intelligence">AI</abbr> systems – recognising that consumers, users and regulators will require different information.<sup id="fnref:95"><a href="#fn:95" class="footnote" rel="footnote" role="doc-noteref">[footnote 95]</a></sup></p>

  <p><strong>Rationale for the principle</strong></p>

  <p>Transparency can increase public trust,<sup id="fnref:96"><a href="#fn:96" class="footnote" rel="footnote" role="doc-noteref">[footnote 96]</a></sup> which has been shown to be a significant driver of <abbr title="Artificial Intelligence">AI</abbr> adoption.<sup id="fnref:97"><a href="#fn:97" class="footnote" rel="footnote" role="doc-noteref">[footnote 97]</a></sup></p>

  <p>When <abbr title="Artificial Intelligence">AI</abbr> systems are not sufficiently explainable, <abbr title="Artificial Intelligence">AI</abbr> suppliers and users risk inadvertently breaking laws, infringing rights, causing harm and compromising the security of <abbr title="Artificial Intelligence">AI</abbr> systems.</p>

  <p>At a technical level, the explainability of <abbr title="Artificial Intelligence">AI</abbr> systems remains an important research and development challenge. The logic and decision-making in <abbr title="Artificial Intelligence">AI</abbr> systems cannot always be meaningfully explained in a way that is intelligible to humans, although in many settings this poses no substantial risk. It is also true that in some cases, a decision made by <abbr title="Artificial Intelligence">AI</abbr> may perform no worse on explainability than a comparable decision made by a human.<sup id="fnref:98"><a href="#fn:98" class="footnote" rel="footnote" role="doc-noteref">[footnote 98]</a></sup> Future developments of the technology may pose additional challenges to achieving explainability. <abbr title="Artificial Intelligence">AI</abbr> systems should display levels of explainability that are appropriate to their context, including the level of risk and consideration of what is achievable given the state of the art.</p>

  <h4 id="principle-fairness">Principle: Fairness</h4>
  <p><br></p>

  <p><strong>Definition and explanation</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> systems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes. Actors involved in all stages of the <abbr title="Artificial Intelligence">AI</abbr> life cycle should consider definitions of fairness that are appropriate to a system’s use, outcomes and the application of relevant law.</p>

  <p>Fairness is a concept embedded across many areas of law and regulation, including equality and human rights, data protection, consumer and competition law, public and common law, and rules protecting vulnerable people.</p>

  <p>Regulators may need to develop and publish descriptions and illustrations of fairness that apply to <abbr title="Artificial Intelligence">AI</abbr> systems within their regulatory domain, and develop guidance that takes into account relevant law, regulation, technical standards,<sup id="fnref:99"><a href="#fn:99" class="footnote" rel="footnote" role="doc-noteref">[footnote 99]</a></sup> and assurance techniques.</p>

  <p>Regulators will need to ensure that <abbr title="Artificial Intelligence">AI</abbr> systems in their domain are designed, deployed and used considering such descriptions of fairness. Where concepts of fairness are relevant in a broad range of intersecting regulatory domains, we anticipate that developing joint guidance will be a priority for regulators.</p>

  <p><strong>Rationale for the principle</strong></p>

  <p>In certain circumstances, <abbr title="Artificial Intelligence">AI</abbr> can have a significant impact on people’s lives, including insurance offers, credit scores, and recruitment outcomes. <abbr title="Artificial Intelligence">AI</abbr>-enabled decisions with high impact outcomes should not be arbitrary and should be justifiable.</p>

  <p>In order to ensure a proportionate and context-specific approach regulators should be able to describe and illustrate what fairness means within their sectors and domains, and consult with other regulators where multiple remits are engaged by a specific use case. We expect that regulators’ interpretations of fairness will include consideration of compliance with relevant law and regulation, including:</p>

  <ol>
    <li>
      <p><abbr title="Artificial Intelligence">AI</abbr> systems should not produce discriminatory outcomes, such as those which contravene the Equality Act 2010 or the Human Rights Act 1998. Use of <abbr title="Artificial Intelligence">AI</abbr> by public authorities should comply with the additional duties placed on them by legislation (such as the Public Sector Equality Duty).</p>
    </li>
    <li>
      <p>Processing of personal data involved in the design, training, and use of <abbr title="Artificial Intelligence">AI</abbr> systems should be compliant with requirements under the UK General Data Protection Regulation (<abbr title="UK General Data Protection Regulation">GDPR</abbr>), the Data Protection Act 2018,<sup id="fnref:100"><a href="#fn:100" class="footnote" rel="footnote" role="doc-noteref">[footnote 100]</a></sup> particularly around fair processing and solely automated decision-making.</p>
    </li>
    <li>
      <p>Consumer and competition law, including rules protecting vulnerable consumers and individuals.<sup id="fnref:101"><a href="#fn:101" class="footnote" rel="footnote" role="doc-noteref">[footnote 101]</a></sup></p>
    </li>
    <li>
      <p>Relevant sector-specific fairness requirements, such as the Financial Conduct Authority (<abbr title="Financial Conduct Authority">FCA</abbr>) Handbook.</p>
    </li>
  </ol>

  <h4 id="principle-accountability-and-governance">Principle: Accountability and governance</h4>
  <p><br></p>

  <p><strong>Definition and explanation</strong></p>

  <p>Governance measures should be in place to ensure effective oversight of the supply and use of <abbr title="Artificial Intelligence">AI</abbr> systems, with clear lines of accountability established across the <abbr title="Artificial Intelligence">AI</abbr> life cycle.</p>

  <p><abbr title="Artificial Intelligence">AI</abbr> life cycle actors should take steps to consider, incorporate and adhere to the principles and introduce measures necessary for the effective implementation of the principles at all stages of the <abbr title="Artificial Intelligence">AI</abbr> life cycle.</p>

  <p>Regulators will need to look for ways to ensure that clear expectations for regulatory compliance and good practice are placed on appropriate actors in the <abbr title="Artificial Intelligence">AI</abbr> supply chain, and may need to encourage the use of governance procedures that reliably ensure these expectations are met.</p>

  <p>Regulator guidance on this principle should reflect that ‘accountability’ refers to the expectation that organisations or individuals will adopt appropriate measures to ensure the proper functioning, throughout their life cycle, of the <abbr title="Artificial Intelligence">AI</abbr> systems that they research, design, develop, train, operate, deploy, or otherwise use.</p>

  <p><strong>Rationale for the principle</strong></p>

  <p><abbr title="Artificial Intelligence">AI</abbr> systems can operate with a high level of autonomy, making decisions about how to achieve a certain goal or outcome in a way that has not been explicitly programmed or foreseen.<sup id="fnref:102"><a href="#fn:102" class="footnote" rel="footnote" role="doc-noteref">[footnote 102]</a></sup> Establishing clear, appropriate lines of ownership and accountability is essential for creating business certainty while ensuring regulatory compliance.</p>

  <p>Doing so for actors in the <abbr title="Artificial Intelligence">AI</abbr> life cycle is difficult, given the complexity of <abbr title="Artificial Intelligence">AI</abbr> supply chains, as well as the adaptivity, autonomy and opacity of <abbr title="Artificial Intelligence">AI</abbr> systems. In some cases, technical standards can provide useful guidance on good practices for <abbr title="Artificial Intelligence">AI</abbr> governance.<sup id="fnref:103"><a href="#fn:103" class="footnote" rel="footnote" role="doc-noteref">[footnote 103]</a></sup> Assurance techniques like impact assessments can help to identify potential risks early in the development life cycle, enabling their mitigation through appropriate safeguards and governance mechanisms.</p>

  <p>Regulatory guidance should also reflect the responsibilities such life cycle actors have for demonstrating proper accountability and governance (for example, by providing documentation on key decisions throughout the <abbr title="Artificial Intelligence">AI</abbr> system life cycle, conducting impact assessments or allowing audits where appropriate).</p>

  <h4 id="principle-contestability-and-redress">Principle: Contestability and redress</h4>
  <p><br></p>

  <p><strong>Definition and explanation</strong></p>

  <p>Where appropriate, users, impacted third parties and actors in the <abbr title="Artificial Intelligence">AI</abbr> life cycle should be able to contest an <abbr title="Artificial Intelligence">AI</abbr> decision or outcome that is harmful or creates material risk of harm.</p>

  <p>Regulators will be expected to clarify existing routes to contestability and redress, and implement proportionate measures to ensure that the outcomes of <abbr title="Artificial Intelligence">AI</abbr> use are contestable where appropriate.</p>

  <p>We would also expect regulators to encourage and guide regulated entities to make clear routes (including informal channels) easily available and accessible, so affected parties can contest harmful <abbr title="Artificial Intelligence">AI</abbr> outcomes or decisions as needed.</p>

  <p><strong>Rationale for the principle</strong></p>

  <p>The use of <abbr title="Artificial Intelligence">AI</abbr> technologies can result in different types of harm and can have a material impact on people’s lives. <abbr title="Artificial Intelligence">AI</abbr> systems’ outcomes may introduce risks such as the reproduction of biases or safety concerns.</p>

  <p>People and organisations should be able to contest outcomes where existing rights have been violated or they have been harmed.</p>

  <p>It will be important for regulators to provide clear guidance on this principle so that <abbr title="Artificial Intelligence">AI</abbr> life cycle actors can implement it in practice. This should include clarifying that appropriate transparency and explainability are relevant to good implementation of this contestability and redress principle.</p>

  <p>The UK’s initial non-statutory approach will not create new rights or new routes to redress at this stage.</p>
</div>

<p>53. We anticipate that regulators will need to issue guidance on the principles or update existing guidance to provide clarity to business. Regulators may also publish joint guidance on one or more of the principles, focused on <abbr title="Artificial Intelligence">AI</abbr> use cases that cross multiple regulatory remits. We are keen to work with regulators and industry to understand the best approach to providing guidance. We expect that practical guidance will support actors in the <abbr title="Artificial Intelligence">AI</abbr> life cycle to adhere to the principles and embed them into their technical and operational business processes. Regulators may also use alternative measures and introduce other tools or resources, in addition to issuing guidance, within their existing remits and powers to implement the principles.</p>

<p>54. Government will monitor the overall effectiveness of the principles and the wider impact of the framework.<sup id="fnref:104"><a href="#fn:104" class="footnote" rel="footnote" role="doc-noteref">[footnote 104]</a></sup> This will include working with regulators to understand how the principles are being applied and whether the framework is adequately supporting innovation.</p>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>1. Do you agree that requiring organisations to make it clear when they are using <abbr title="Artificial Intelligence">AI</abbr> would improve transparency?</p>

  <p>2. Are there other measures we could require of organisations to improve <abbr title="Artificial Intelligence">AI</abbr> transparency?</p>

  <p>3. Do you agree that current routes to contest or get redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms are adequate?</p>

  <p>4. How could current routes to contest or seek redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms be improved, if at all?</p>

  <p>5. Do you agree that, when implemented effectively, the revised cross-sectoral principles will cover the risks posed by <abbr title="Artificial Intelligence">AI</abbr> technologies?</p>

  <p>6. What, if anything, is missing from the revised principles?</p>
</div>

<div class="call-to-action">
  <h4 id="case-study-34-explainable-ai-in-practice">Case study 3.4: Explainable <abbr title="Artificial Intelligence">AI</abbr> in practice</h4>

  <p>The level of explainability needed from an <abbr title="Artificial Intelligence">AI</abbr> system is highly specific to its context, including the extent to which an application is safety-critical. The level and type of explainability required will likely vary depending on whether the intended audience of the explanation is a regulator, technical expert, or lay person.</p>

  <p>For example, a technical expert designing self-driving vehicles would need to understand the system’s decision-making capabilities to test, assess and refine them. In the same context, a lay person may need to understand the decision-making process only in order to use the vehicle safely. If the vehicle malfunctioned and caused a harmful outcome,<sup id="fnref:105"><a href="#fn:105" class="footnote" rel="footnote" role="doc-noteref">[footnote 105]</a></sup> a regulator may need information about how the system operates in order to allocate responsibility – similar to the level of explainability currently needed to hold human drivers accountable.</p>

  <p>While <abbr title="Artificial Intelligence">AI</abbr> explainability remains a technical challenge and an area of active research, regulators are already conducting work to address it. In 2021, the <abbr title="Information Commissioner’s Office">ICO</abbr> and the Alan Turing Institute issued co-developed guidance on explaining decisions made with <abbr title="Artificial Intelligence">AI</abbr>,<sup id="fnref:106"><a href="#fn:106" class="footnote" rel="footnote" role="doc-noteref">[footnote 106]</a></sup> giving organisations practical advice to help explain the processes, services and decisions delivered or assisted by <abbr title="Artificial Intelligence">AI</abbr> to the individuals affected by them.</p>

  <p>The audience for an explanation of <abbr title="Artificial Intelligence">AI</abbr>’s outcomes will often be a regulator, who may require a higher standard of explainability depending on the risks represented by an application. The <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr>’s Project Glass Box work is addressing the challenge of setting medical device requirements that take into account adequate consideration of human interpretability and its consequences for the safety and effectiveness for <abbr title="Artificial Intelligence">AI</abbr> used in medical devices.<sup id="fnref:107"><a href="#fn:107" class="footnote" rel="footnote" role="doc-noteref">[footnote 107]</a></sup></p>
</div>

<div class="call-to-action">
  <h4 id="case-study-35-what-the-principles-mean-for-businesses-in-practice">Case study 3.5: What the principles mean for businesses in practice</h4>

  <p>A fictional company, ‘Good <abbr title="Artificial Intelligence">AI</abbr> Recruitment Limited’, provides recruitment services that use a range of <abbr title="Artificial Intelligence">AI</abbr> systems to accelerate the recruitment process, including a service that automatically shortlists candidates based on application forms. While potentially useful, such systems may discriminate against certain groups that have historically not been selected for certain positions.</p>

  <p>After the implementation of the UK’s new <abbr title="Artificial Intelligence">AI</abbr> regulatory framework, the Equality and Human Rights Commission (<abbr title="Equality and Human Rights Commission">EHRC</abbr>) and the Information Commissioner Office (<abbr title="Information Commissioner’s Office">ICO</abbr>) will be supported and encouraged to work with the Employment Agency Standards Inspectorate (<abbr title="Employment Agency Standards Inspectorate">EASI</abbr>) and other regulators and organisations in the employment sector to issue joint guidance. The joint guidance could address the cross-cutting principles relating to fairness, appropriate transparency and explainability, and contestability and redress in the context of the use of <abbr title="Artificial Intelligence">AI</abbr> systems in recruitment or employment. Such joint guidance could, for example, make things clearer and easier for Good <abbr title="Artificial Intelligence">AI</abbr> Recruitment Limited by:</p>

  <ol>
    <li>Clarifying the type of information businesses should provide when implementing such systems</li>
    <li>Identifying appropriate supply chain management processes such as due diligence or <abbr title="Artificial Intelligence">AI</abbr> impact assessments</li>
    <li>Suggesting proportionate measures for bias detection, mitigation and monitoring</li>
    <li>Providing suggestions for the provision of contestability and redress routes.</li>
  </ol>

  <p>Good <abbr title="Artificial Intelligence">AI</abbr> Recruitment Limited would also be able to apply a variety of tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr>, such as technical standards, that would supplement regulatory guidance and other measures promoted by regulators. In their published guidance regulators could, where appropriate, refer businesses to existing technical standards on transparency (such as <a rel="external" href="https://standards.ieee.org/ieee/7001/6929/">IEEE 7001-2021</a>), as well as standards on bias mitigation (such as <a rel="external" href="https://www.iso.org/standard/77607.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 24027:2021</a>).</p>

  <p>By following this guidance Good <abbr title="Artificial Intelligence">AI</abbr> Recruitment Limited would be able to develop and deploy their services responsibly.</p>
</div>

<h3 id="section324">3.2.4 Our preferred model for applying the principles</h3>

<p>55. Initially, the principles will be issued by government on a non-statutory basis and applied by regulators within their remits. We will support regulators to apply the principles using the powers and resources available to them. This initial period of implementation will provide a valuable opportunity to ensure that the principles are effective and that the wider framework is supporting innovation while addressing risks appropriately.</p>

<p>56. While industry has strongly supported non-statutory measures in the first instance, favouring flexibility and fewer burdens, some businesses and regulators have suggested that government should go beyond a non-statutory approach to ensure the principles have the desired impact.<sup id="fnref:108"><a href="#fn:108" class="footnote" rel="footnote" role="doc-noteref">[footnote 108]</a></sup> Some regulators have also expressed concerns that they lack the statutory basis to consider the application of the principles. We are committed to an approach that leverages collaboration with our expert regulators but we agree that we may need to intervene further to ensure that our framework is effective.</p>

<p>57. Following a period of non-statutory implementation, and when parliamentary time allows, we anticipate that we will want to strengthen and clarify regulators’ mandates by introducing a new duty requiring them to have due regard to the principles. Such a duty would give a clear signal that we expect regulators to act and support coherence across the regulatory landscape, ensuring that the framework displays the characteristics that we have identified.<sup id="fnref:109"><a href="#fn:109" class="footnote" rel="footnote" role="doc-noteref">[footnote 109]</a></sup> One of the strengths of this approach is that regulators would still be able to exercise discretion and expert judgement regarding the relevance of each principle to their individual domains.</p>

<p>58. A duty would ensure that regulators retain the ability to exercise judgement when applying the principles in particular contexts – benefiting from some of the flexibility expected through non-statutory implementation. For example, while the duty to have due regard would require regulators to demonstrate that they had taken account of the principles, it may be the case that not every regulator will need to introduce measures to implement every principle. In having due regard to a particular principle, a regulator may exercise their expert judgement and determine that their sector or domain does not require action to be taken. The introduction of the duty will, however, give regulators a clear mandate and incentive to apply the principles where relevant to their sectors or domains.</p>

<p>59. If our monitoring of the effectiveness of the initial, non-statutory framework suggests that a statutory duty is unnecessary, we would not introduce it. Similarly, we will monitor whether particular principles cannot be, or are not being, applied in certain circumstances or by specific regulators because of the interpretation of existing legal requirements or because of technical constraints. Such circumstances may require broader legislative changes. Should we decide there is a need for statutory measures, we will work with regulators to review the interaction of our principles with their existing duties and powers.</p>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>7. Do you agree that introducing a statutory duty on regulators to have due regard to the principles would clarify and strengthen regulators’ mandates to implement our principles while retaining a flexible approach to implementation?</p>

  <p>8. Is there an alternative statutory intervention that would be more effective?</p>
</div>

<h3 id="the-role-of-individual-regulators-in-applying-the-principles">3.2.5 The role of individual regulators in applying the principles</h3>

<p>60. In some sectors, principles for <abbr title="Artificial Intelligence">AI</abbr> governance will already exist and may even go further than the cross-cutting principles we propose. Our framework gives sectors the ability to develop and apply more specific principles to suit their own domains, where government or regulators identify these are needed.</p>

<p>61. The Ministry of Defence published its own <abbr title="Artificial Intelligence">AI</abbr> ethical principles and policy in June 2022, which determines HM Government’s approach regarding <abbr title="Artificial Intelligence">AI</abbr>-enabled military capabilities. We will ensure appropriate coherence and alignment in the application of this policy through a context specific approach and thereby promote UK leadership in the employment of <abbr title="Artificial Intelligence">AI</abbr> for defence purposes. Ahead of introducing any statutory duty to have due regard to our principles, and in advance of introducing other material iterations of the framework, we will consider whether exemptions are needed to allow existing regulators (such as those working in areas like national security) to continue their domain-level approach</p>

<p>62. Not all principles will be equally relevant in all contexts and sometimes two or more principles may come into conflict. For example, it may be difficult to assess the fairness of an algorithm’s outputs without access to sensitive personal data about the subjects of the processing. Regulators will need to use their expertise and judgement to prioritise and apply the principles in such cases, sharing information where possible with government and other regulators about how they are assessing the relevance of each principle. This collaboration between regulators and government will allow the framework to be adapted to ensure it is practical, coherent and supporting innovation.</p>

<p>63. In implementing the new regulatory framework we expect that regulators will:</p>

<ul>
  <li>Assess the cross-cutting principles and apply them to <abbr title="Artificial Intelligence">AI</abbr> use cases that fall within their remit.<br>
</li>
  <li>Issue relevant guidance on how the principles interact with existing legislation to support industry to apply the principles. Such guidance should also explain and illustrate what compliance looks like.<br>
</li>
  <li>Support businesses operating within the remits of multiple regulators by collaborating and producing clear and consistent guidance, including joint guidance where appropriate.<br>
</li>
</ul>

<p>64. Regulators will need to monitor and evaluate their own implementation of the framework and their own effectiveness at regulating <abbr title="Artificial Intelligence">AI</abbr> within their remits. We understand that there may be <abbr title="Artificial Intelligence">AI</abbr>-related risks that do not clearly fall within the remits of the UK’s existing regulators.<sup id="fnref:110"><a href="#fn:110" class="footnote" rel="footnote" role="doc-noteref">[footnote 110]</a></sup> Not every new <abbr title="Artificial Intelligence">AI</abbr>-related risk will require a regulatory response and there is a growing ecosystem of tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> that can support the application of the cross-cutting principles. These are described further in <a href="#partfour">part 4</a>.</p>

<p>65. Where prioritised risks fall within a gap in the legal landscape, regulators will need to collaborate with government to identify potential actions. This may include identifying iterations to the framework such as changes to regulators’ remits, updates to the Regulators’ Code,<sup id="fnref:111"><a href="#fn:111" class="footnote" rel="footnote" role="doc-noteref">[footnote 111]</a></sup> or additional legislative interventions. Our approach benefits from our strong sovereign parliamentary system, which reliably allows for the introduction of targeted and proportionate measures in response to emerging issues, including by adapting existing legislation if necessary.<sup id="fnref:112"><a href="#fn:112" class="footnote" rel="footnote" role="doc-noteref">[footnote 112]</a></sup></p>

<p>66. The Sir Patrick Vallance review has highlighted that rushed attempts to regulate <abbr title="Artificial Intelligence">AI</abbr> too early would risk stifling innovation.<sup id="fnref:113"><a href="#fn:113" class="footnote" rel="footnote" role="doc-noteref">[footnote 113]</a></sup> Our approach aligns with this perspective. We recognise the need to build a stronger evidence base before making decisions on statutory interventions. In doing so, we will ensure that we strike the right balance between retaining flexibility in our iterative approach and providing clarity to businesses. As detailed in <a href="#section331">section 3.3.1</a>, we will deliver a range of central functions, including horizon scanning and risk monitoring, to identify and respond to situations where prioritised risks are not adequately covered by the framework, or where gaps between regulators’ remits are negatively impacting innovation.</p>

<div class="call-to-action">
  <h4 id="case-study-36-responding-to-regulatory-policy-challenges--self-driving-vehicles">Case study 3.6: Responding to regulatory policy challenges – self-driving vehicles</h4>

  <p>Some aspects of a new <abbr title="Artificial Intelligence">AI</abbr> use case may sit outside regulators’ existing remits, meaning they do not have a mandate to address specific harms or support a new product to enter the market.</p>

  <p>The advent of self-driving vehicles highlighted such a regulatory and policy challenge. Where sophisticated <abbr title="Artificial Intelligence">AI</abbr>-enabled software is capable of performing the designated driving task, existing regulatory structures – where responsibility for road safety is achieved by licensing human drivers – are not fit for purpose. This creates uncertainty regarding the development and deployment of self-driving vehicles that cannot be addressed by regulators alone.</p>

  <p>To achieve the government’s ambition to ‘make the UK one of the best places in the world to develop and deploy self-driving vehicles technology’,<sup id="fnref:114"><a href="#fn:114" class="footnote" rel="footnote" role="doc-noteref">[footnote 114]</a></sup> manufacturers need clarity about the regulatory landscape they are operating in and the general public needs to have confidence in the safety, fairness and trustworthiness of these vehicles.</p>

  <p>The government published its Connected &amp; Automated Mobility 2025 report<sup id="fnref:115"><a href="#fn:115" class="footnote" rel="footnote" role="doc-noteref">[footnote 115]</a></sup> to address this challenge, describing how the ecosystem could be adapted to spur innovation and secure the economic and social benefits of this technology.</p>

  <p>The work of the UK’s Centre for Connected and Autonomous Vehicles is an example of government acting to identify regulatory gaps, develop policy and build UK capabilities. A central monitoring and evaluation function, described below, will identify and assess gaps in the regulatory ecosystem that could stifle <abbr title="Artificial Intelligence">AI</abbr> innovation so that government can take action to address them.</p>
</div>

<h3 id="guidance-to-regulators-on-applying-the-principles">3.2.6 Guidance to regulators on applying the principles</h3>

<p>67. The proposed regulatory framework is dependent upon the implementation of the principles by our expert regulators. This regulator-led approach has received broad support from across industry, with stakeholders acknowledging the importance of the sector-specific expertise held by individual regulators. We expect regulators to collaborate proactively to achieve the best outcomes for the economy and society. We will work with regulators to monitor the wider framework and ensure that this collaborative approach to implementation is effective. If improvements are needed, including interventions to drive stronger collaboration across regulators, we will take further action.</p>

<p>68. Our engagement with regulators and industry highlighted the need for central government to support regulators. We will work with regulators to develop guidance that helps them implement the principles in a way that aligns with our expectations for how the framework should operate. Existing legal frameworks already mandate and guide regulators’ actions. For example, nearly all regulators are bound by the Regulators’ Code<sup id="fnref:116"><a href="#fn:116" class="footnote" rel="footnote" role="doc-noteref">[footnote 116]</a></sup> and all regulators – as public bodies – are required to comply with the Human Rights Act.<sup id="fnref:117"><a href="#fn:117" class="footnote" rel="footnote" role="doc-noteref">[footnote 117]</a></sup> Our proposed guidance to regulators will seek to ensure that when applying the principles, regulators are supported and encouraged to:</p>

<ul>
  <li>Adopt a proportionate approach that promotes growth and innovation by focusing on the risks that <abbr title="Artificial Intelligence">AI</abbr> poses in a particular context.<br>
</li>
  <li>Consider proportionate measures to address prioritised risks, taking into account cross-cutting risk assessments undertaken by, or on behalf of, government.<br>
</li>
  <li>Design, implement and enforce appropriate regulatory requirements and, where possible, integrate delivery of the principles into existing monitoring, investigation and enforcement processes.<br>
</li>
  <li>Develop joint guidance, where appropriate, to support industry compliance with the principles and relevant regulatory requirements.<br>
</li>
  <li>Consider how tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> like assurance techniques and technical standards can support regulatory compliance.<br>
</li>
  <li>Engage proactively and collaboratively with government’s monitoring and evaluation of the framework.<br>
</li>
</ul>

<div class="call-to-action">
  <h4 id="case-study-37-what-this-means-for-businesses">Case study 3.7: What this means for businesses</h4>

  <p>A fictional company, ‘<abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited’, has delayed the deployment of a new <abbr title="Artificial Intelligence">AI</abbr> application as – under the current patchwork of relevant regulatory requirements – it has been challenging to identify appropriate compliance actions for <abbr title="Artificial Intelligence">AI</abbr>-driven insurance products.</p>

  <p>Following implementation of the UK’s new pro-innovation framework to regulate <abbr title="Artificial Intelligence">AI</abbr>, we could expect to see joint guidance produced collaboratively by the Information Commissioner’s Office (<abbr title="Information Commissioner’s Office">ICO</abbr>), Equality and Human Rights Commission (<abbr title="Equality and Human Rights Commission">EHRC</abbr>), Financial Conduct Authority (<abbr title="Financial Conduct Authority">FCA</abbr>) and other relevant regulatory authorities. This would provide greater clarity on the regulatory requirements relevant to <abbr title="Artificial Intelligence">AI</abbr> as well as guidance on how to satisfy those requirements in the context of insurance and consumer-facing financial services.</p>

  <p>Under the proposed regulatory framework, <abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited could be supported by new or updated guidance issued by regulators to address the <abbr title="Artificial Intelligence">AI</abbr> regulatory principles. The company may also be able to follow joint regulatory guidance, issued as a result of collaboration between regulators, and use a set of tools provided by regulators, such as template risk assessments and transparency measures, and relevant technical standards (for example, international standards for transparency and bias mitigation). The collaboration between regulators and focus on practical implementation measures will guide the responsible deployment of <abbr title="Artificial Intelligence">AI</abbr> Fairness Insurance Limited’s <abbr title="Artificial Intelligence">AI</abbr> product by making it easier for the company to navigate the regulatory landscape and address specific risks such as discrimination.</p>
</div>

<p>69. Further details about the implementation of the regulatory framework will be provided through an <abbr title="Artificial Intelligence">AI</abbr> regulation roadmap which will be published alongside the government response to the consultation on this white paper.</p>

<h3 id="section331">3.3.1 New central functions to support the framework</h3>

<p>70. Government has a responsibility to make sure the regulatory framework operates proportionately and supports innovation. Feedback to our proposals from businesses has been clear that the current patchwork of regulation, with relatively little in the way of central coordination or oversight, will create a growing barrier to innovation if left unaddressed. Responses from over 130 organisations and individuals to our 2022 policy paper highlighted the need for a greater level of monitoring and coordination to achieve the coherence and improved clarity we need to support innovation. Businesses, particularly small to medium sized enterprises, noted that regulatory coordination could improve business certainty and investment, resulting in more and better jobs in the sector.</p>

<p>71. Government therefore intends to put mechanisms in place to coordinate, monitor and adapt the framework as a whole. Further detail on these functions is set out below. Enhanced monitoring activity will allow us to take a structured approach to gathering feedback from industry on the impact of our regime as it is introduced. These mechanisms will supplement and support the work of regulators, without undermining their independence. Equally, such mechanisms are not intended to duplicate existing activities.</p>

<p>72. Delivering some functions centrally provides government with an overarching view of how the framework is working, where it is effective and where it may need improving. A central suite of functions will also facilitate collaboration by bringing together a wide range of interested parties, including regulators, international partners, industry, civil society organisations such as trade unions and advocacy groups, academia and the general public. Our engagement with these groups has highlighted the need for our proposed central functions. We will continue to convene a wide range of stakeholders to ensure that we hear the full spectrum of viewpoints. This breadth of engagement and collaboration will be integral to government’s ability to monitor and improve the framework. The functions will identify and support opportunities for further coordination between regulators, resulting in greater clarity for businesses and stronger consumer trust.</p>

<p>73. We have identified a set of functions that will drive regulatory coherence and support regulators to implement the pro-innovation approach that we have outlined. These functions have been informed by our discussions with industry, research organisations, and regulators following the publication of the <abbr title="Artificial Intelligence">AI</abbr> policy paper.</p>

<div class="call-to-action">
  <h4 id="box31">Box 3.1: Functions required to support implementation of the framework</h4>

  <p><br>
<strong>Monitoring, assessment and feedback</strong><sup id="fnref:118"><a href="#fn:118" class="footnote" rel="footnote" role="doc-noteref">[footnote 118]</a></sup></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Develop and maintain a central monitoring and evaluation (<abbr title="monitoring and evaluation">M&amp;E</abbr>) framework to assess cross-economy and sector-specific impacts of the new regime.<br>
</li>
      <li>Ensure appropriate data is gathered from relevant sources – for example, from industry, regulators, government and civil society – and considered as part of the overall assessment of the effectiveness of the framework.<br>
</li>
      <li>Support and equip regulators to undertake internal <abbr title="monitoring and evaluation">M&amp;E</abbr> and find ways to support regulators’ contributions to the central <abbr title="monitoring and evaluation">M&amp;E</abbr> function.<br>
</li>
      <li>Monitor the regime’s overall effectiveness including the extent to which it is proportionate and supporting innovation.<br>
</li>
      <li>Provide advice to ministers on issues that may need to be addressed to improve the regime, including where additional intervention may be required to ensure that the framework remains effective as the capability of <abbr title="Artificial Intelligence">AI</abbr> and the state of the art develops.</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>This function is at the heart of our iterative approach. We need to know whether the framework is working – for example, whether it is able to respond to and mitigate prioritised risks and whether the framework is actively supporting innovation – and we need the ability to spot issues quickly so we can adapt the framework in response.</p>

    <p><abbr title="monitoring and evaluation">M&amp;E</abbr> needs to be undertaken centrally to determine whether the regime as a whole is delivering against our objectives. <abbr title="monitoring and evaluation">M&amp;E</abbr> will assess whether our regime is operating in a way that is pro-innovation, clear, proportionate, adaptable, trustworthy and collaborative.</p>

    <p>Our engagement with industry, regulators, and civil society has shown us the importance of establishing a feedback loop to measure the effectiveness of the framework. We will ensure mechanisms are in place to gather evidence and insights to inform policy design.</p>
  </div>

  <p><strong>Support coherent implementation of the principles</strong></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Develop and maintain central regulatory guidance to support regulators in the implementation of the principles.</li>
      <li>Identify barriers that prevent regulators from effectively implementing the principles, such as:
        <ul>
          <li>Scope of regulatory remit.</li>
          <li>Insufficient regulatory powers.</li>
          <li>Insufficient regulatory capabilities.</li>
        </ul>
      </li>
      <li>Identify conflicts or inconsistencies in the way the principles are interpreted across regulatory remits, and assess the impact this is having on innovation. Some variation across regulators’ approaches to implementation is to be expected and encouraged, given the context-based approach that we are taking.</li>
      <li>Work with regulators to resolve discrepancies that are having a significant impact on innovation, and share learning and best practice.</li>
      <li>Monitor and assess the ongoing relevance of the principles themselves.</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>Businesses have noted that, within a context-based regulatory framework, an appropriate degree of central leadership is needed to ensure coherence. To be effective, this function must be performed centrally, as the whole regulatory landscape needs to be considered to:</p>

    <ul>
      <li>Ensure that, as far as necessary to support innovation, regulators interpret and implement the principles in a coherent way.</li>
      <li>Effectively monitor how well the principles are being implemented, as well as their ongoing relevance.</li>
    </ul>

    <p>This function will play a central part in delivering a regulatory regime that is:</p>

    <ul>
      <li>Clear: by making it easier for businesses working across regulatory remits (for example, by supporting the development of joint regulatory guidance where appropriate).</li>
      <li>Proportionate and pro-innovation: as it allows government to find and prevent any application of the principles that has a disproportionate or harmful impact on innovation.</li>
      <li>Adaptable and trustworthy: as it forms part of the feedback loop established by the <abbr title="monitoring and evaluation">M&amp;E</abbr> function to understand how well the regime operates and whether it should be changed.</li>
      <li>Collaborative: by encouraging cross-government cooperation aligned to the principles.</li>
    </ul>
  </div>

  <p><strong>Cross-sectoral risk assessment</strong><sup id="fnref:119"><a href="#fn:119" class="footnote" rel="footnote" role="doc-noteref">[footnote 119]</a></sup></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Develop and maintain a cross-economy, society-wide <abbr title="Artificial Intelligence">AI</abbr> risk register to support regulators’ internal risks assessments.</li>
      <li>Monitor, review and re-prioritise known risks.</li>
      <li>Identify and prioritise new and emerging risks (working with the horizon scanning function).</li>
      <li>Work with regulators to clarify responsibilities in relation to new risks or areas of contested responsibility.</li>
      <li>Support join-up between regulators on <abbr title="Artificial Intelligence">AI</abbr>-related risks that cut across remits and facilitate issuing of joint guidance where appropriate.</li>
      <li>Identify where risks are not adequately covered.</li>
      <li>Share risk enforcement best practices.</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>Stakeholders have emphasised that a cross-sectoral assessment of risk is required to ensure that any new risks can be addressed and do not fall in any gaps between regulator remits. To be effective, this function must be performed centrally, as risks need to be considered across the whole economy to:</p>

    <ul>
      <li>Encourage regulators to take a coherent approach to assessing <abbr title="Artificial Intelligence">AI</abbr>-related risks.</li>
      <li>Ensure risks do not fall between regulatory gaps and that appropriate action is taken where cross-sector risks do not have an obvious ‘home’ within a single regulatory remit.</li>
    </ul>

    <p>A centrally delivered risk function will ensure that the framework’s approach to risk is informed by a cross-sector, holistic viewpoint. A cross-cutting approach to risk allows a proportionate but effective response.</p>

    <p>This function will play a central part in delivering a regulatory regime that is:</p>

    <ul>
      <li>Clear: by making it easier for businesses working across sectors.</li>
      <li>Proportionate: by ensuring an appropriate response to cross-sector risks.</li>
      <li>Trustworthy: by making sure priority <abbr title="Artificial Intelligence">AI</abbr>-related risks are being addressed.</li>
      <li>Collaborative: by allowing important actors – such as frontier researchers, civil society, international partners and regulators – to be convened to engage in focused, prioritised discussions on <abbr title="Artificial Intelligence">AI</abbr>-related risks.</li>
    </ul>
  </div>

  <p><strong>Support for innovators (including testbeds and sandboxes as detailed in <a href="#section334">section 3.3.4</a>)</strong></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Remove barriers to innovation by assisting <abbr title="Artificial Intelligence">AI</abbr> innovators to navigate regulatory complexity and get their product to market while minimising legal and compliance risk (drawing on the expertise of all relevant regulators).</li>
      <li>Identify cross-cutting regulatory issues that are having real-world impacts and stifling innovation, and identify opportunities for improvement to our regulatory framework.</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>We want to make it easy for innovators to navigate the regulatory landscape. Businesses have noted that tools such as regulatory sandboxes can help innovators to navigate the regulatory landscape. Central commissioning or delivery of the sandbox or testbed will also enable information and insights generated from this work to directly inform our implementation of the overall regulatory framework.</p>

    <p>This function will play a central part in delivering a regulatory regime that is:</p>

    <ul>
      <li>Clear: by making it easier for businesses working across sectors.</li>
      <li>Adaptable and trustworthy: as it forms an important part of the feedback loop to understand how well the regime is functioning and how it should iterate.</li>
    </ul>

    <p>To support innovators, we will take forward Sir Patrick Vallance’s recommendation for a multi-regulator <abbr title="Artificial Intelligence">AI</abbr> sandbox to be established<sup id="fnref:120"><a href="#fn:120" class="footnote" rel="footnote" role="doc-noteref">[footnote 120]</a></sup> (see <a href="#section334">section 3.3.4</a> for more detail).</p>
  </div>

  <p><strong>Education and awareness</strong></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Provide guidance to businesses seeking to navigate the <abbr title="Artificial Intelligence">AI</abbr> regulatory landscape.</li>
      <li>Raise awareness and provide guidance to consumers and the general public to ensure that these groups are empowered and encouraged to engage with the ongoing monitoring and iteration of the framework.</li>
      <li>Encourage regulators to promote awareness campaigns to educate consumers and users on <abbr title="Artificial Intelligence">AI</abbr> regulation and risks.<sup id="fnref:121"><a href="#fn:121" class="footnote" rel="footnote" role="doc-noteref">[footnote 121]</a></sup>
</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>To be effective, this function must be performed centrally, as the whole regulatory landscape needs to be considered to provide useful guidance to businesses and consumers on navigating it. This will ensure that businesses and consumers are able to contribute to the monitoring and evaluation of the framework and its ongoing iteration.</p>

    <p>This function will help deliver a regulatory regime that is:</p>

    <ul>
      <li>Clear: by helping businesses working across sectors to navigate the regulatory landscape.</li>
      <li>Trustworthy: by increasing awareness of the framework and its requirements among consumers and businesses.</li>
      <li>Collaborative: by educating and raising awareness to empower businesses and consumers to participate in the ongoing evaluation and iteration of the framework.</li>
      <li>Pro-innovation: by enhancing trust, which is shown to increase <abbr title="Artificial Intelligence">AI</abbr> adoption.</li>
    </ul>
  </div>

  <p><strong>Horizon scanning</strong></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Monitor emerging trends and opportunities in <abbr title="Artificial Intelligence">AI</abbr> development to ensure that the framework can respond to them effectively.</li>
      <li>Proactively convene industry, frontier researchers, academia and other key stakeholders to establish how the <abbr title="Artificial Intelligence">AI</abbr> regulatory framework could support the UK’s <abbr title="Artificial Intelligence">AI</abbr> ecosystem to maximise the benefits of emerging opportunities whilst continuing to take a proportionate approach to <abbr title="Artificial Intelligence">AI</abbr> risk.</li>
      <li>Support the risk assessment function to identify and prioritise new and emerging <abbr title="Artificial Intelligence">AI</abbr> risks, working collaboratively with industry, academia, global partners, and regulators.</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>This function will support horizon-scanning activities in individual regulators but a central function is also necessary. As stakeholders have highlighted, an economy-wide view is required to anticipate opportunities that emerge across the landscape, particularly those that cut across regulatory remits or fall in the gaps between them.</p>

    <p>This function will help deliver a regulatory regime that is:</p>

    <ul>
      <li>Adaptable: by identifying emerging trends to enable intelligent, coordinated adaptation of the regulatory framework.</li>
      <li>Collaborative: by convening partners including frontier researchers, industry, civil society, international partners and regulators, to identify emerging trends.</li>
      <li>Trustworthy: by ensuring that our regulatory framework is able to adapt in the face of emerging trends</li>
    </ul>
  </div>

  <p><strong>Ensure interoperability with international regulatory frameworks</strong></p>
  <div class="example">
    <p><strong>Activities</strong></p>

    <ul>
      <li>Monitor alignment between UK principles and international approaches to regulation, assurance and/or risk management, and technical standards.</li>
      <li>Support cross-border coordination and collaboration by identifying opportunities for regulatory interoperability.</li>
    </ul>

    <p><strong>Rationale</strong></p>

    <p>To be effective, this function must be performed centrally. The whole regulatory landscape needs to be considered to understand how well the UK framework aligns with international jurisdictions. The impact of international alignment on innovation and adoption of <abbr title="Artificial Intelligence">AI</abbr> in the UK is a key concern for businesses. The central oversight and monitoring of the global alignment of the framework will support UK engagement with like-minded international partners on <abbr title="Artificial Intelligence">AI</abbr> regulation, building our influence in <abbr title="Artificial Intelligence">AI</abbr>.</p>

    <p>This function will play a central part in delivering a regulatory regime that is:</p>

    <ul>
      <li>Pro-innovation: by ensuring that UK innovators can trade internationally and UK companies can attract overseas investment.</li>
      <li>Collaborative: by fostering close cooperation with international partners.</li>
      <li>Proportionate: by making sure the framework is sufficiently aligned with international approaches to maximise market access and business opportunities without imposing unnecessary burdens that could stifle innovation or otherwise negatively impact on international trade and/or investment in <abbr title="Artificial Intelligence">AI</abbr> in the UK.</li>
      <li>Adaptable: as it forms an important part of the feedback loop to understand how well the regime is functioning and how it should iterate.</li>
    </ul>
  </div>
</div>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>9. Do you agree that the functions outlined in <a href="#box31">Box 3.1</a> would benefit our <abbr title="Artificial Intelligence">AI</abbr> regulation framework if delivered centrally?</p>

  <p>10. What, if anything, is missing from the central functions?</p>

  <p>11. Do you know of any existing organisations who should deliver one or more of our proposed central functions?</p>

  <p>12. Are there additional activities that would help businesses confidently innovate and use <abbr title="Artificial Intelligence">AI</abbr> technologies?</p>

  <p>12.1. If so, should these activities be delivered by government, regulators or a different organisation?</p>

  <p>13. Are there additional activities that would help individuals and consumers confidently use <abbr title="Artificial Intelligence">AI</abbr> technologies?</p>

  <p>13.1. If so, should these activities be delivered by government, regulators or a different organisation?</p>

  <p>14. How can we avoid overlapping, duplicative or contradictory guidance on <abbr title="Artificial Intelligence">AI</abbr> issued by different regulators?</p>
</div>

<div class="call-to-action">
  <h4 id="box-32-supporting-coherence-in-risk-assessment">Box 3.2: Supporting coherence in risk assessment</h4>

  <p><br>
<strong>Why?</strong></p>

  <p>Many <abbr title="Artificial Intelligence">AI</abbr> risks do not fall neatly into the remit of one individual regulator and they could go unaddressed if not monitored at a cross-sector level. A central, cross-economy risk function will also enable government to monitor future risks in a rigorous, coherent and balanced way. This will include ‘high impact but low probability’ risks such as existential risks posed by artificial general intelligence or <abbr title="Artificial Intelligence">AI</abbr> biosecurity risks.</p>

  <p>A pro-innovation approach to regulation involves tolerating a certain degree of risk rather than intervening in all cases. Government needs the ability to assess and prioritise <abbr title="Artificial Intelligence">AI</abbr> risks, ensuring that any intervention is proportionate and consistent with levels of risk mitigation activity elsewhere across the economy or <abbr title="Artificial Intelligence">AI</abbr> life cycle.</p>

  <p>Establishing a central risk function will bring coherence to the way regulators and industry think about <abbr title="Artificial Intelligence">AI</abbr> risk. It will also foster collaboration between government, regulators, industry and civil society to provide clarity for businesses managing <abbr title="Artificial Intelligence">AI</abbr> risk across sectors.</p>

  <p><strong>What?</strong></p>

  <p>The central risk function will identify, assess, prioritise and monitor cross-cutting <abbr title="Artificial Intelligence">AI</abbr> risks that may require government intervention.</p>

  <p><strong>How?</strong></p>

  <p>The central risk function will bring together cutting-edge knowledge from industry, regulators, academia and civil society – including skilled computer scientists with a deep technical understanding of <abbr title="Artificial Intelligence">AI</abbr>.</p>

  <p>Given the importance of risk management expertise, we will seek inspiration and learning from sectors where operational risk management is highly developed. This will include looking for examples of how failures and near misses can be recorded and used to inform good practice.</p>

  <p>Regulators will have a key role in designing the central risk framework and ensuring alignment with their existing practices. Where a risk that has been prioritised for intervention falls outside of any existing regulator’s remit, the central risk function will identify measures that could be taken to address the gap (for example, updates to regulatory remits). The central risk function will also support smaller regulators that lack technical <abbr title="Artificial Intelligence">AI</abbr> expertise to better understand <abbr title="Artificial Intelligence">AI</abbr> risks.</p>
</div>

<h4 id="figure-2-central-risks-function-activities">Figure 2: Central risks function activities</h4>

<figure class="image embedded"><div class="img"><img src="https://assets.publishing.service.gov.uk/media/6422fec960a35e000c0cafb0/Diagram03-blackText__1_.svg" alt=""></div>
<figcaption><p>Central risk function activities</p></figcaption></figure>

<div class="call-to-action">
  <h4 id="box3-3">Box 3.3: How a central monitoring and evaluation (<abbr title="monitoring and evaluation">M&amp;E</abbr>) function enables a proportionate, adaptable approach</h4>

  <p><br>
<strong>Why?</strong></p>

  <p>We will need to monitor the implementation of the framework closely to make sure that it is working as designed. We will monitor the regime to ensure it is pro-innovation, proportionate, adaptable, trustworthy, clear and collaborative – our desired characteristics.</p>

  <p><strong>What?</strong></p>

  <p>The central <abbr title="monitoring and evaluation">M&amp;E</abbr> function will gather evidence and feedback from a range of sources and actors in the ecosystem. For example, effective <abbr title="monitoring and evaluation">M&amp;E</abbr> of the whole framework is likely to require input and data from industry, regulators, civil society, academia, international partners and the general public. Insights from regulatory sandboxes and testbeds as well as wider monitoring of the <abbr title="Artificial Intelligence">AI</abbr> ecosystem as a whole will also be valuable.<sup id="fnref:122"><a href="#fn:122" class="footnote" rel="footnote" role="doc-noteref">[footnote 122]</a></sup></p>

  <p>Government’s ability to access reliable, comprehensive data and insights for the purposes of monitoring the <abbr title="Artificial Intelligence">AI</abbr> regulatory framework will be closely related to our work raising awareness and educating businesses and consumers on <abbr title="Artificial Intelligence">AI</abbr>-related issues. It is important for our <abbr title="monitoring and evaluation">M&amp;E</abbr> data to be drawn from a wide range of sources, reflecting the full spectrum of views and including seldom heard voices from the general public. Raising awareness and educating stakeholder groups will help to ensure that the broader conversation is inclusive, informed and rigorous.</p>

  <p>We will develop and monitor metrics that demonstrate whether the framework is working as intended. For example, the central <abbr title="monitoring and evaluation">M&amp;E</abbr> function will look at the effectiveness of the framework in mitigating unacceptable risks and assess whether the implementation of the principles by regulators is disproportionate or negatively affecting innovation.</p>

  <p>Insights from the <abbr title="monitoring and evaluation">M&amp;E</abbr> function will contribute to the adaptability of our framework by enabling government to identify opportunities for improvement so we can benefit fully from the flexibility we have built into our approach. Such iteration could include removing or amending existing regulation as well as updating the <abbr title="Artificial Intelligence">AI</abbr> regulatory framework itself.</p>

  <p><strong>How?</strong></p>

  <p>The range, sources and quality of the data that informs our monitoring and evaluation of the framework will be critical.</p>

  <p>The <abbr title="monitoring and evaluation">M&amp;E</abbr> function will identify the metrics and data sources to help us measure how well the regime is working, both in terms of the framework’s ability to mitigate risk but also to ensure that it is supporting innovation. It will bring together a wide range of views including industry, civil society groups and academia.</p>

  <p>Crucially, we will work with regulators to identify how their work – including data collected from their own regulatory activities – can support our central <abbr title="monitoring and evaluation">M&amp;E</abbr> function in order to ensure the best outcomes for the whole economy.</p>
</div>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>15. Do you agree with our overall approach to monitoring and evaluation?</p>

  <p>16. What is the best way to measure the impact of our framework?</p>

  <p>17. Do you agree that our approach strikes the right balance between supporting <abbr title="Artificial Intelligence">AI</abbr> innovation; addressing known, prioritised risks; and future-proofing the <abbr title="Artificial Intelligence">AI</abbr> regulation framework?</p>
</div>

<p>74. It is important to have the right architecture in place to oversee the delivery of the central functions described above. The <abbr title="Artificial Intelligence">AI</abbr> ecosystem already benefits from a range of organisations with extensive expertise in regulatory issues. Ground-breaking coordination initiatives like the Digital Regulation Cooperation Forum (<abbr title="Digital Regulation Cooperation Forum">DRCF</abbr>) play a valuable role in enhancing regulatory alignment and fostering dialogue on digital issues across regulators. However, the <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr> was not created to support the delivery of all the functions we have identified or the implementation of our proposed regulatory framework for <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>75. Government will initially be responsible for delivering the central functions described above, working in partnership with regulators and other key actors in the <abbr title="Artificial Intelligence">AI</abbr> ecosystem to leverage existing activities where possible. This is aligned with our overall iterative approach and enables system-wide review of the framework. We recognise that there may be value in a more independent delivery of the central functions in the longer term.</p>

<p>76. Where relevant activities are already undertaken by organisations either within or outside of government, the primary role of the central functions will be to leverage these activities and assess their effectiveness. Where this is not the case – for example, where new bespoke capabilities are needed to monitor and evaluate the operation of the framework as a whole – these functions will initially be established in government.</p>

<div class="call-to-action">
  <h4 id="case-study-38-building-on-a-strong-foundation-of-regulatory-coordination">Case study 3.8: Building on a strong foundation of regulatory coordination</h4>

  <p>The growth of digital technologies requires regulators to coordinate and act cohesively. The Digital Regulation Cooperation Forum (<abbr title="Digital Regulation Cooperation Forum">DRCF</abbr>) has published its vision for a joined-up approach to digital regulation. It conducts cross-regulator horizon scanning for future technology and has issued detailed discussion papers on the benefits, harms and auditing of algorithms.</p>

  <p>Regulators are also exploring ways to provide simpler ‘shop fronts’ for those they regulate, with the <abbr title="National Health Service">NHS</abbr> <abbr title="Artificial Intelligence">AI</abbr> and Digital Regulations Service offer already robustly tested with end users and now widely available.<sup id="fnref:123"><a href="#fn:123" class="footnote" rel="footnote" role="doc-noteref">[footnote 123]</a></sup> <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr> regulators have a multi-agency advice service for digital innovators pilot underway, supported by government’s Regulators’ Pioneer Fund,<sup id="fnref:124"><a href="#fn:124" class="footnote" rel="footnote" role="doc-noteref">[footnote 124]</a></sup> which aims to make it easier for firms operating across digital regulatory boundaries to do business.</p>

  <p>Existing regulatory forums may need to be supplemented or adapted to successfully implement the cross-cutting principles. We will work in partnership with existing bodies as well as industry to improve and enhance regulatory coordination.</p>
</div>

<p>77. We are deliberately taking an iterative approach to the delivery of the regulatory framework and we anticipate that the model for providing the central functions will develop over time. We will identify where existing structures may need to be supplemented or adapted. In particular, we are focused on understanding:</p>

<ul>
  <li>Whether existing regulatory forums could be expanded to include the full range of regulators involved in the regulation of <abbr title="Artificial Intelligence">AI</abbr> or whether additional mechanisms are needed.<br>
</li>
  <li>What additional expertise government may need to support the implementation and monitoring of the principles, including the potential role that could be played by established advisory bodies.<br>
</li>
  <li>The most effective way to convene input from across industry and consumers to ensure a broad range of opinions.<br>
</li>
</ul>

<p>78. Government, in fulfilling the regulatory central functions and overseeing the framework, will benefit from engaging external expertise to gather insights and advice from experts in industry, academia and civil society. The <abbr title="Artificial Intelligence">AI</abbr> Council has been an important source of expertise over the last 3 years, advising government on the development of the National <abbr title="Artificial Intelligence">AI</abbr> Strategy as well as our approach to <abbr title="Artificial Intelligence">AI</abbr> governance. As we enter a new phase we will review the role of the <abbr title="Artificial Intelligence">AI</abbr> Council and consider how best to engage expertise to support the implementation of the regulatory framework.</p>

<p>79. As the regulatory framework evolves and we develop a clearer understanding of the system-level functions that are needed, we will review the operational model outlined above. In particular, we will consider if a government unit is the most appropriate mechanism for delivering the central functions in the longer term or if an independent body would be more effective.</p>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>18. Do you agree that regulators are best placed to apply the principles and government is best placed to provide oversight and deliver central functions?</p>
</div>

<h3 id="section332">3.3.2 Government’s role in addressing accountability across the life cycle</h3>
<p>80. The clear allocation of accountability and legal responsibility is important for effective <abbr title="Artificial Intelligence">AI</abbr> governance. Legal responsibility for compliance with the principles should be allocated to the actors in the <abbr title="Artificial Intelligence">AI</abbr> life cycle best able to identify, assess and mitigate <abbr title="Artificial Intelligence">AI</abbr> risks effectively. Incoherent or misplaced allocation of legal responsibility could hinder innovation or adoption of <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>81. However, <abbr title="Artificial Intelligence">AI</abbr> supply chains can be complex and opaque, making effective governance of <abbr title="Artificial Intelligence">AI</abbr> and supply chain risk management difficult. Inappropriate allocation of <abbr title="Artificial Intelligence">AI</abbr> risk, liability, and responsibility for <abbr title="Artificial Intelligence">AI</abbr> governance throughout the <abbr title="Artificial Intelligence">AI</abbr> life cycle and within <abbr title="Artificial Intelligence">AI</abbr> supply chains could impact negatively on innovation. For example, inappropriate allocation of liability to a business using, but not developing, <abbr title="Artificial Intelligence">AI</abbr> could stifle <abbr title="Artificial Intelligence">AI</abbr> adoption. Similarly, allocating too much responsibility to businesses developing foundation models, on the grounds that these models could be used by third parties in a range of contexts, would hamper innovation.</p>

<p>82. We recognise the need to consider which actors should be responsible and liable for complying with the principles, which may not be the same actors who bear the burden under current legal frameworks. For example, data protection law differentiates between data controllers and data processors. Similarly, product safety laws include the concepts of producers and distributors. In the context of those specific legal frameworks, liability for compliance with various existing legal obligations is allocated by law to those identified supply chain actors. It is not yet clear how responsibility and liability for demonstrating compliance with the <abbr title="Artificial Intelligence">AI</abbr> regulatory principles will be or should ideally be, allocated to existing supply chain actors within the <abbr title="Artificial Intelligence">AI</abbr> life cycle.</p>

<p>83. We are not proposing to intervene and make changes to life cycle accountability at this stage. It is too soon to make decisions about liability as it is a complex, rapidly evolving issue which must be handled properly to ensure the success of our wider <abbr title="Artificial Intelligence">AI</abbr> ecosystem. However, to further our understanding of this topic we will engage a range of experts, including technicians and lawyers. It may become apparent that current legal frameworks, when combined with implementation of our <abbr title="Artificial Intelligence">AI</abbr> principles by regulators, will allocate legal responsibility and liability across the supply chain in a way that is not fair or effective. We would consider proportionate interventions to address such issues which could otherwise undermine our pro-innovation approach to <abbr title="Artificial Intelligence">AI</abbr> regulation. Our agile approach benefits our sovereign parliamentary system’s reliable ability to introduce targeted measures – for example by amending existing legislation if necessary – in response to new evidence.<sup id="fnref:125"><a href="#fn:125" class="footnote" rel="footnote" role="doc-noteref">[footnote 125]</a></sup></p>

<p>84. Tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> like assurance techniques and technical standards can support supply chain risk management. These tools can also drive the uptake and adoption of <abbr title="Artificial Intelligence">AI</abbr> by building justified trust in these systems, giving users confidence that key <abbr title="Artificial Intelligence">AI</abbr>-related risks have been identified, addressed and mitigated across the supply chain. For example, by describing measures that manufacturers should take to ensure the safety of <abbr title="Artificial Intelligence">AI</abbr> systems, technical standards can provide reassurance to purchasers and users of <abbr title="Artificial Intelligence">AI</abbr> systems that appropriate safety-focused measures have been adopted, ultimately encouraging adoption of <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p>85. Our evaluation of the framework will assess whether the legal responsibility for <abbr title="Artificial Intelligence">AI</abbr> is effectively and fairly distributed. As we implement the framework, we will continue our extensive engagement to gather evidence from regulators, industry, academia, and civil society on its impact on different actors across the <abbr title="Artificial Intelligence">AI</abbr> life cycle. This will allow us to monitor the effects of our framework on actors across the <abbr title="Artificial Intelligence">AI</abbr> supply chain on an ongoing basis. We will need a particular focus on foundation models given the potential challenges they pose to life cycle accountability, especially when available as open-source. By centrally evaluating whether there are adequate measures for <abbr title="Artificial Intelligence">AI</abbr> accountability, we can assess the need for further interventions into <abbr title="Artificial Intelligence">AI</abbr> liability across the whole economy and <abbr title="Artificial Intelligence">AI</abbr> life cycle.</p>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>L1. What challenges might arise when regulators apply the principles across different <abbr title="Artificial Intelligence">AI</abbr> applications and systems? How could we address these challenges through our proposed <abbr title="Artificial Intelligence">AI</abbr> regulatory framework?</p>

  <p>L2.1 Do you agree that the implementation of our principles through existing legal frameworks will fairly and effectively allocate legal responsibility for <abbr title="Artificial Intelligence">AI</abbr> across the life cycle?</p>

  <p>L.2.2. How could it be improved, if at all?</p>

  <p>L3. If you work for a business that develops, uses, or sells <abbr title="Artificial Intelligence">AI</abbr>, how do you currently manage <abbr title="Artificial Intelligence">AI</abbr> risk including through the wider supply chain? How could government support effective <abbr title="Artificial Intelligence">AI</abbr>-related risk management?</p>
</div>

<h3 id="section333">3.3.3 Foundation models and the regulatory framework</h3>

<p>86. Foundation models are an emerging type of general purpose <abbr title="Artificial Intelligence">AI</abbr> that are trained on vast quantities of data and can be adapted to a wide range of tasks. The fast-paced development of foundation models brings novel challenges for governments seeking to regulate <abbr title="Artificial Intelligence">AI</abbr>. Despite high levels of interest in the topic, the research community has not found a consensus on how foundation models work, the risks they pose or even the extent of their capabilities.<sup id="fnref:126"><a href="#fn:126" class="footnote" rel="footnote" role="doc-noteref">[footnote 126]</a></sup></p>

<p>87. Foundation models have been described as paradigm shifting and could have significant impacts on society and the economy.<sup id="fnref:127"><a href="#fn:127" class="footnote" rel="footnote" role="doc-noteref">[footnote 127]</a></sup> They can be used for a wide variety of purposes and deployed in many already complex ecosystems. Given the widely acknowledged transformative potential of foundation models, we must give careful attention to how they might interact with our proposed regulatory framework. Our commitment to an adaptable, proportionate approach presents a clear opportunity for the UK to lead the global conversation and set global norms for the future-proof regulation of foundation models.</p>

<p>88. There is a relatively small number of organisations developing foundation models. Some organisations exercise close control over the development and distribution of their foundation models. Other organisations take an open-source approach to the development and distribution of the technology. Open-source models can improve access to the transformational power of foundation models, but can cause harm without adequate guardrails.<sup id="fnref:128"><a href="#fn:128" class="footnote" rel="footnote" role="doc-noteref">[footnote 128]</a></sup> The variation in organisational approaches to developing and supplying foundation models introduces a wide range of complexities for the regulation of <abbr title="Artificial Intelligence">AI</abbr>. The potential opacity of foundation models means that it can also be challenging to identify and allocate accountability for outcomes generated by <abbr title="Artificial Intelligence">AI</abbr> systems that rely on or integrate them.</p>

<p>89. Our proposed framework considers the issues raised by foundation models in light of our life cycle accountability analysis, outlined in <a href="#section332">section 3.3.2</a> above. Given the small number of organisations supplying foundation models and a proportionately larger number of businesses integrating or otherwise deploying foundation models elsewhere in the <abbr title="Artificial Intelligence">AI</abbr> ecosystem, we recognise the important role of tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr>, including assurance techniques and technical standards.</p>

<p>90. The proposed central functions described in <a href="#section331">section 3.3.1</a> will play an important role in informing our approach to regulating foundation models. The central risk function’s proactive, rigorous monitoring of risks associated with foundation models and the horizon scanning function’s identification of related opportunities will be critical to ensuring that we strike the balance needed as part of our proportionate, pro-innovation regulatory approach. It will be crucial to ensure that the proposed monitoring and evaluation function has access to the technical skills and capabilities needed to assess the impact that our framework has on the opportunities and risks presented by foundation models.</p>

<p>91. We recognise that industry, academia, research organisations and global partners are looking for ways to address the challenges related to the regulation of foundation models.<sup id="fnref:129"><a href="#fn:129" class="footnote" rel="footnote" role="doc-noteref">[footnote 129]</a></sup> For example, we know that developers of foundation models are exploring ways to embed alignment theory into their models. This is an important area of research, and government will need to work closely with the <abbr title="Artificial Intelligence">AI</abbr> research community to leverage insights and inform our iteration of the regulatory framework. Our collaborative, adaptable framework will draw on the expertise of those researchers and other stakeholders as we continue to develop policy in this evolving area.</p>

<p>92. The UK is committed to building its capabilities in foundation models. Our Foundation Model Taskforce announced in the Integrated Review Refresh 2023<sup id="fnref:130"><a href="#fn:130" class="footnote" rel="footnote" role="doc-noteref">[footnote 130]</a></sup> will support government to build UK capability and ensure the UK harnesses the benefits presented by this emerging technology. Our proposed framework will ensure we create the right regulatory environment as we move to maximise the transformative potential of foundation models.</p>

<h4 id="case-study-39-life-cycle-accountability-for-large-language-models">Case-study 3.9: Life cycle accountability for large language models</h4>

<div class="call-to-action">
  <p>Large language models (<abbr title="large language models">LLMs</abbr>) are a type of foundation model.<sup id="fnref:131"><a href="#fn:131" class="footnote" rel="footnote" role="doc-noteref">[footnote 131]</a></sup> The potential of <abbr title="large language models">LLMs</abbr> goes beyond reproducing or translating natural language: <abbr title="large language models">LLMs</abbr> also have the power to write software,<sup id="fnref:132"><a href="#fn:132" class="footnote" rel="footnote" role="doc-noteref">[footnote 132]</a></sup> generate stories<sup id="fnref:133"><a href="#fn:133" class="footnote" rel="footnote" role="doc-noteref">[footnote 133]</a></sup> through films and virtual reality,<sup id="fnref:134"><a href="#fn:134" class="footnote" rel="footnote" role="doc-noteref">[footnote 134]</a></sup> and more.<sup id="fnref:135"><a href="#fn:135" class="footnote" rel="footnote" role="doc-noteref">[footnote 135]</a></sup></p>

  <p><abbr title="large language models">LLMs</abbr> fall within the scope of our regulatory framework as they are autonomous and adaptable.</p>

  <p>We are mindful of the rapid technological change in the development of foundation models such as <abbr title="large language models">LLMs</abbr> and the new opportunities that they bring to applications including search engines, medical devices, and financial and legal services. However, <abbr title="large language models">LLMs</abbr> also have limitations, for example, the models are not trained on a sense of truth,<sup id="fnref:136"><a href="#fn:136" class="footnote" rel="footnote" role="doc-noteref">[footnote 136]</a></sup> so they can reproduce inconsistent or false outputs that seem highly credible.<sup id="fnref:137"><a href="#fn:137" class="footnote" rel="footnote" role="doc-noteref">[footnote 137]</a></sup> Because they can be adapted to a wide variety of tasks downstream within an <abbr title="Artificial Intelligence">AI</abbr> supply chain, any improvements or defects in a foundation model could quickly affect all adapted products.</p>

  <p>Under the UK’s pro-innovation <abbr title="Artificial Intelligence">AI</abbr> regulatory framework, regulators may decide to issue specific guidance and requirements for <abbr title="large language model">LLM</abbr> developers and deployers to address risks and implement the cross-cutting principles. This could include guidance on appropriate transparency measures to inform users when <abbr title="Artificial Intelligence">AI</abbr> is being used and the data used to train the model.</p>

  <p>The wide-reaching impact of <abbr title="large language models">LLMs</abbr> through the <abbr title="Artificial Intelligence">AI</abbr> supply chain – together with their general purpose and potential wide ranging application – means they are unlikely to be directly ‘caught’ within the remit of any single regulator. This makes effective governance and supply chain risk-management challenging where <abbr title="large language models">LLMs</abbr> are involved. The <abbr title="Artificial Intelligence">AI</abbr> regulatory framework’s monitoring and evaluation function will therefore need to assess the impacts of <abbr title="large language models">LLMs</abbr>. The cross-cutting accountability and governance principle will encourage regulators and businesses to find ways to demonstrate accountability and good governance in responsible <abbr title="large language model">LLM</abbr> development and use.</p>

  <p>At this point it would be premature to take specific regulatory action in response to foundation models including <abbr title="large language models">LLMs</abbr>. To do so would risk stifling innovation, preventing <abbr title="Artificial Intelligence">AI</abbr> adoption, and distorting the UK’s thriving <abbr title="Artificial Intelligence">AI</abbr> ecosystem.</p>

  <p>However, we are mindful of the rapid rate of advances in the power and application of <abbr title="large language models">LLMs</abbr>, and the potential creation of new or previously unforeseen risks. As such, <abbr title="large language models">LLMs</abbr> will be a core focus of our monitoring and risk assessment functions and we will work with the wider <abbr title="Artificial Intelligence">AI</abbr> community to ensure our adaptive framework is capable of identifying and responding to developments relating to <abbr title="large language models">LLMs</abbr>.</p>

  <p>For example, one way of monitoring the potential impact of <abbr title="large language models">LLMs</abbr> could be by monitoring the amount of compute used to train them, which is much easier to assess and govern than other inputs such as data, or talent. This could involve statutory reporting requirements for models over a certain size. This metric could become less useful as a way of establishing who has access to powerful models if machine learning development becomes increasingly open-source.<sup id="fnref:138"><a href="#fn:138" class="footnote" rel="footnote" role="doc-noteref">[footnote 138]</a></sup></p>

  <p>Life cycle accountability – including the allocation of responsibility and liability for risks arising from the use of foundation models including <abbr title="large language models">LLMs</abbr> – is a priority area for ongoing research and policy development. We will explore the ways in which technical standards and other tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> can support good practices for responsible innovation across the life cycle and supply chain. We will also work with regulators to ensure they are appropriately equipped to engage with actors across the <abbr title="Artificial Intelligence">AI</abbr> supply chain and allocate legal liability appropriately.</p>
</div>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>F1. What specific challenges will foundation models such as large language models (<abbr title="large language models">LLMs</abbr>) or open-source models pose for regulators trying to determine legal responsibility for <abbr title="Artificial Intelligence">AI</abbr> outcomes?</p>

  <p>F2. Do you agree that measuring compute provides a potential tool that could be considered as part of the governance of foundation models?</p>

  <p>F3. Are there other approaches to governing foundation models that would be more effective?</p>
</div>

<h3 id="section334">3.3.4 Artificial intelligence sandboxes and testbeds</h3>

<p>93. Government is committed to supporting innovators by addressing regulatory challenges that prevent new, cutting-edge products from getting to market. Barriers can be particularly high when a path to market requires interaction with multiple regulators or regulatory guidance is nascent. Sir Patrick Vallance’s Digital Report recommends that government works with regulators to develop an <abbr title="Artificial Intelligence">AI</abbr> sandbox to support innovators. At the Budget, government confirmed our commitment to taking forward this recommendation.<sup id="fnref:139"><a href="#fn:139" class="footnote" rel="footnote" role="doc-noteref">[footnote 139]</a></sup></p>

<p>94. The Information Commissioner’s Office (<abbr title="Information Commissioner’s Office">ICO</abbr>) and the Financial Conduct Authority (<abbr title="Financial Conduct Authority">FCA</abbr>) have already successfully piloted digital sandboxes in their sectors.<sup id="fnref:140"><a href="#fn:140" class="footnote" rel="footnote" role="doc-noteref">[footnote 140]</a></sup> The <abbr title="Financial Conduct Authority">FCA</abbr> sandbox has worked with over 800 businesses and accelerated their speed to market by an estimated 40% on average.<sup id="fnref:141"><a href="#fn:141" class="footnote" rel="footnote" role="doc-noteref">[footnote 141]</a></sup> Sandbox participation has also been found to have significant financial benefits, particularly for smaller organisations.<sup id="fnref:142"><a href="#fn:142" class="footnote" rel="footnote" role="doc-noteref">[footnote 142]</a></sup> We have heard from regulators, including those with less experience of taking part in previous initiatives, that they are keen to participate in new <abbr title="Artificial Intelligence">AI</abbr> sandboxes to support their regulated sectors.</p>

<p>95. Regulatory sandboxes and testbeds will play an important role in our proposed regulatory regime. Such initiatives enable government and regulators to:</p>

<ul>
  <li>Support innovators to get novel products and services to market faster, so they can start generating economic and social benefits.<br>
</li>
  <li>Test how the regulatory framework is operating in practice and illuminate unnecessary barriers to innovation that need to be addressed.<br>
</li>
  <li>Identify emerging technology and market trends to which our regulatory framework may need to adapt.<br>
</li>
</ul>

<p>96. To deliver an effective sandbox, we would like to understand more deeply what service focus would be most useful to industry. We are considering 4 options:</p>

<ul>
  <li>Single sector, single regulator: support innovators to bring <abbr title="Artificial Intelligence">AI</abbr> products to the market in collaboration with a single regulator, focusing on only one chosen industry sector.<sup id="fnref:143"><a href="#fn:143" class="footnote" rel="footnote" role="doc-noteref">[footnote 143]</a></sup><br>
</li>
  <li>Multiple sectors, single regulator: support <abbr title="Artificial Intelligence">AI</abbr> innovators in collaboration with a single regulator that is capable of working across multiple industry sectors.<sup id="fnref:144"><a href="#fn:144" class="footnote" rel="footnote" role="doc-noteref">[footnote 144]</a></sup><br>
</li>
  <li>Single sector, multiple regulator: establish a sandbox that only operates in one industry sector but is capable of supporting <abbr title="Artificial Intelligence">AI</abbr> innovators whose path to market requires interaction with one or more regulators operating in that sector.<sup id="fnref:145"><a href="#fn:145" class="footnote" rel="footnote" role="doc-noteref">[footnote 145]</a></sup><br>
</li>
  <li>Multiple sectors, multiple regulators: a sandbox capable of operating with one or more regulators in one or more industry sectors to help <abbr title="Artificial Intelligence">AI</abbr> innovators reach their target market. The <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr> is piloting a version of this model.<sup id="fnref:146"><a href="#fn:146" class="footnote" rel="footnote" role="doc-noteref">[footnote 146]</a></sup><br>
</li>
</ul>

<p>97. We intend to focus an initial pilot on a single sector, multiple regulator sandbox. Recognising the importance of <abbr title="Artificial Intelligence">AI</abbr> innovations that have implications in multiple sectors (like generative <abbr title="Artificial Intelligence">AI</abbr> models), we would look to expand this capability to cover multiple industry sectors over time.</p>

<p>98. Initially, we envisage focusing the sandbox on a sector where there is a high degree of <abbr title="Artificial Intelligence">AI</abbr> investment, industry demand for a sandbox, and appetite for improved collaboration between regulators to help <abbr title="Artificial Intelligence">AI</abbr> innovators take their products to market. We invite consultation feedback on this proposal as well as suggestions for industry sectors that meet these criteria.</p>

<p>99. We would also like to build a deeper understanding of what service offering would be most helpful to industry. Some sandboxes offer supervised real-life or simulated test environments where innovators can trial new products, often under relaxed regulatory requirements.<sup id="fnref:147"><a href="#fn:147" class="footnote" rel="footnote" role="doc-noteref">[footnote 147]</a></sup> In other scenarios, a team of technologists and regulation experts give customised advice and support to participating innovators over a number of months to help them understand and overcome regulatory barriers so they can reach their target market.<sup id="fnref:148"><a href="#fn:148" class="footnote" rel="footnote" role="doc-noteref">[footnote 148]</a></sup> Our current preference is for the customised advice and support model, as we think this is where we can deliver benefits most effectively in the short term. We will explore options for developing a safe test environment capability at a later date, informed by our initial pilot work.</p>

<p>100. The implementation of an <abbr title="Artificial Intelligence">AI</abbr> regulatory sandbox will also be closely informed by Sir Patrick Vallance’s review into digital regulation and his recommendation to establish a multi-regulator sandbox.<sup id="fnref:149"><a href="#fn:149" class="footnote" rel="footnote" role="doc-noteref">[footnote 149]</a></sup> The review sets out a number of design principles, which we will build into our pilot approach. This includes targeting such initiatives at start-ups and small to medium-sized businesses. As a matter of priority, we will engage with businesses to understand how such an approach should be designed and delivered to best support their needs.</p>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>S1. To what extent would the sandbox models described in <a href="#section334">section 3.3.4</a> support innovation?</p>

  <p>S2. What could government do to maximise the benefit of sandboxes to <abbr title="Artificial Intelligence">AI</abbr> innovators?</p>

  <p>S3. What could government do to facilitate participation in an <abbr title="Artificial Intelligence">AI</abbr> regulatory sandbox?</p>

  <p>S4. Which industry sectors or classes of product would most benefit from an <abbr title="Artificial Intelligence">AI</abbr> sandbox?</p>
</div>

<h3 id="regulator-capabilities">3.3.5 Regulator capabilities</h3>

<p>101. Government has prioritised the ongoing assessment of the different capability needs across the regulatory landscape. We will keep this under close review as part of our ongoing monitoring and evaluation activity.</p>

<p>102. While our approach does not currently involve or anticipate extending any regulator’s remit,<sup id="fnref:150"><a href="#fn:150" class="footnote" rel="footnote" role="doc-noteref">[footnote 150]</a></sup> regulating <abbr title="Artificial Intelligence">AI</abbr> uses effectively will require many of our regulators to acquire new skills and expertise. Our research<sup id="fnref:151"><a href="#fn:151" class="footnote" rel="footnote" role="doc-noteref">[footnote 151]</a></sup> has highlighted different levels of capability among regulators when it comes to understanding <abbr title="Artificial Intelligence">AI</abbr> and addressing its unique characteristics. Our engagement has also elicited a wide range of views on the capabilities regulators require to address <abbr title="Artificial Intelligence">AI</abbr> risks and on the best way for regulators to acquire these.</p>

<p>103. We identified potential capability gaps among many, but not all, regulators, primarily in relation to:</p>

<p><strong><abbr title="Artificial Intelligence">AI</abbr> expertise. Particularly:</strong></p>

<ul>
  <li>Technical expertise in <abbr title="Artificial Intelligence">AI</abbr> technology.<sup id="fnref:152"><a href="#fn:152" class="footnote" rel="footnote" role="doc-noteref">[footnote 152]</a></sup> For example, on how <abbr title="Artificial Intelligence">AI</abbr> is being used to deliver products and services and on the development, use and applicability of technical standards.<sup id="fnref:153"><a href="#fn:153" class="footnote" rel="footnote" role="doc-noteref">[footnote 153]</a></sup><br>
</li>
  <li>Expertise on how <abbr title="Artificial Intelligence">AI</abbr> use cases interact across multiple regulatory regimes.<br>
</li>
  <li>Market intelligence on how <abbr title="Artificial Intelligence">AI</abbr> technologies are being used to disrupt existing business models, both in terms of the potential opportunities and risks that can impact regulatory objectives.<br>
</li>
</ul>

<p><strong>Organisational capacity. A regulator’s ability to:</strong></p>

<ul>
  <li>Effectively adapt to the emergence of <abbr title="Artificial Intelligence">AI</abbr> use cases and applications, and assimilate and share this knowledge throughout the organisation.<br>
</li>
  <li>Work with organisations that provide assurance techniques (such as assurance service providers) and develop technical standards (such as standards development organisations), to identify relevant tools and embed them into the regulatory framework and best practice.<br>
</li>
  <li>Work across regulators to share knowledge and cooperate in the regulation of <abbr title="Artificial Intelligence">AI</abbr> use cases that interact across multiple regulatory regimes.<br>
</li>
  <li>Establish relationships and communicate effectively with organisations and groups not normally within their remit.<br>
</li>
</ul>

<p>104. In the initial phases of implementation, government will work collaboratively with key partners to leverage existing work on this topic. For example, the Digital Regulation Cooperation Forum (<abbr title="Digital Regulation Cooperation Forum">DRCF</abbr>) is already exploring ways of addressing capability gaps within its members.</p>

<p>105. There are options for addressing capability gaps within individual regulators and across the wider regulatory landscape, which we will continue to explore. It may, for example, be appropriate to establish a common pool of expertise that could establish best practice for supporting innovation through regulatory approaches and make it easier for regulators to work with each other on common issues. An alternative approach would be to explore and facilitate collaborative initiatives between regulators – including, where appropriate, further supporting existing initiatives such as the <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr> – to share skills and expertise.</p>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>19. As a regulator, what support would you need in order to apply the principles in a proportionate and pro-innovation way?</p>

  <p>20. Do you agree that a pooled team of <abbr title="Artificial Intelligence">AI</abbr> experts would be the most effective way to address capability gaps and help regulators apply the principles?</p>
</div>

<h2 id="part-4-tools-for-trustworthy-ai-to-support-implementation">Part 4: Tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> to support implementation</h2>

<h3 id="partfour">4.1 <abbr title="Artificial Intelligence">AI</abbr> assurance techniques</h3>

<p>106. Tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> including assurance techniques and technical standards will play a critical role in enabling the responsible adoption of <abbr title="Artificial Intelligence">AI</abbr> and supporting the proposed regulatory framework. Industry and civil society were keen to see a range of practical tools to aid compliance. Government is already supporting the development of these tools by publishing a Roadmap to an effective <abbr title="Artificial Intelligence">AI</abbr> assurance ecosystem in the UK<sup id="fnref:154"><a href="#fn:154" class="footnote" rel="footnote" role="doc-noteref">[footnote 154]</a></sup> and establishing the UK <abbr title="Artificial Intelligence">AI</abbr> Standards Hub<sup id="fnref:155"><a href="#fn:155" class="footnote" rel="footnote" role="doc-noteref">[footnote 155]</a></sup> to champion the use of technical standards.<sup id="fnref:156"><a href="#fn:156" class="footnote" rel="footnote" role="doc-noteref">[footnote 156]</a></sup></p>

<p>107. To assure <abbr title="Artificial Intelligence">AI</abbr> systems effectively, we need a toolbox of assurance techniques to measure, evaluate and communicate the trustworthiness of <abbr title="Artificial Intelligence">AI</abbr> systems across the development and deployment life cycle. These techniques include impact assessment, audit, and performance testing along with formal verification methods.</p>

<p>108. It is unlikely that demand for <abbr title="Artificial Intelligence">AI</abbr> assurance can be entirely met through organisations building in-house capability. The emerging market for <abbr title="Artificial Intelligence">AI</abbr> assurance services and expertise will have an important role to play in providing a range of assurance techniques to actors within the <abbr title="Artificial Intelligence">AI</abbr> supply chain. There is an opportunity for the UK to become a global leader in this market as the <abbr title="Artificial Intelligence">AI</abbr> assurance industry develops. This will enable organisations to determine whether <abbr title="Artificial Intelligence">AI</abbr> technologies are aligned with relevant regulatory requirements.</p>

<p>109. To help innovators understand how <abbr title="Artificial Intelligence">AI</abbr> assurance techniques can support wider <abbr title="Artificial Intelligence">AI</abbr> governance, the government will launch a Portfolio of <abbr title="Artificial Intelligence">AI</abbr> assurance techniques in Spring 2023. The Portfolio is a collaboration with industry to showcase how these tools are already being applied by businesses to real-world use cases and how they align with the <abbr title="Artificial Intelligence">AI</abbr> regulatory principles.</p>

<h3 id="ai-technical-standards">4.2 <abbr title="Artificial Intelligence">AI</abbr> technical standards</h3>

<p>110. Assurance techniques need to be underpinned by available technical standards, which provide common understanding across assurance providers. Technical standards and assurance techniques will also enable organisations to demonstrate that their systems are in line with the UK’s <abbr title="Artificial Intelligence">AI</abbr> regulatory principles.</p>

<p>111. Multiple international and regional standards development organisations are developing, or have already released, <abbr title="Artificial Intelligence">AI</abbr>-specific technical standards, addressing topics such as risk management, transparency, bias, safety and robustness. Accordingly, technical standards can be used by regulators to complement sector-specific approaches to <abbr title="Artificial Intelligence">AI</abbr> regulation by providing common benchmarks and practical guidance to organisations.<sup id="fnref:157"><a href="#fn:157" class="footnote" rel="footnote" role="doc-noteref">[footnote 157]</a></sup> Overall, technical standards can embed flexibility<sup id="fnref:158"><a href="#fn:158" class="footnote" rel="footnote" role="doc-noteref">[footnote 158]</a></sup> into regulatory regimes and drive responsible innovation by helping organisations to address <abbr title="Artificial Intelligence">AI</abbr>-related risks.<sup id="fnref:159"><a href="#fn:159" class="footnote" rel="footnote" role="doc-noteref">[footnote 159]</a></sup></p>

<p>112. The UK plays a leading role in the development of international technical standards, working with industry, international and UK partners.<sup id="fnref:160"><a href="#fn:160" class="footnote" rel="footnote" role="doc-noteref">[footnote 160]</a></sup> The government will continue to support the role of technical standards in complementing our approach to <abbr title="Artificial Intelligence">AI</abbr> regulation, including through the UK <a rel="external" href="https://aistandardshub.org/"><abbr title="Artificial Intelligence">AI</abbr> Standards Hub</a>.</p>

<div class="call-to-action">
  <h4 id="box-41-supporting-a-layered-approach-to-ai-technical-standards">Box 4.1: Supporting a layered approach to <abbr title="Artificial Intelligence">AI</abbr> technical standards</h4>

  <p><br>
The government will complement its context-specific approach to <abbr title="Artificial Intelligence">AI</abbr> regulation by proposing a proportionate ‘layered approach’ to applying available <abbr title="Artificial Intelligence">AI</abbr> technical standards. This involves regulators identifying relevant technical standards and encouraging their adoption by actors in the <abbr title="Artificial Intelligence">AI</abbr> life cycle to support the integration of the <abbr title="Artificial Intelligence">AI</abbr> regulation principles into technical and operational business processes:</p>

  <p>Layer 1: To provide consistency and common foundations across regulatory remits, in the first instance regulators could seek to encourage adoption of sector-agnostic standards which can be applied across <abbr title="Artificial Intelligence">AI</abbr> use cases to support the implementation of cross-sectoral principles. For example, management systems, risk management, and quality standards<sup id="fnref:161"><a href="#fn:161" class="footnote" rel="footnote" role="doc-noteref">[footnote 161]</a></sup> can provide industry with good practices for the responsible development of <abbr title="Artificial Intelligence">AI</abbr> systems. The adoption of these standards should be encouraged by multiple regulators as tools for regulated entities to establish common good practices for <abbr title="Artificial Intelligence">AI</abbr> governance.</p>

  <p>Layer 2: To adapt these governance practices to the specific risks raised by <abbr title="Artificial Intelligence">AI</abbr> in a particular context, regulators could look at encouraging adoption of additional standards addressing specific issues such as bias and transparency.<sup id="fnref:162"><a href="#fn:162" class="footnote" rel="footnote" role="doc-noteref">[footnote 162]</a></sup> Such standards would act as tools for industry to operationalise compliance with specific <abbr title="Artificial Intelligence">AI</abbr> regulation principles. As these standards will provide good practices for <abbr title="Artificial Intelligence">AI</abbr> governance applicable to multiple sectors, regulators could complement these with sector-specific guidance.</p>

  <p>For example, standards for bias mitigation could be promoted by the Financial Conduct Authority (<abbr title="Financial Conduct Authority">FCA</abbr>) and the Equality and Human Rights Commission (<abbr title="Equality and Human Rights Commission">EHRC</abbr>) as practical tools for providers of <abbr title="Artificial Intelligence">AI</abbr> scoring models to identify and mitigate relevant sources of bias to ensure the fairness of the outcomes when the <abbr title="Artificial Intelligence">AI</abbr> model is applied to financial services (credit scoring) and <abbr title="Human Resources">HR</abbr> practices (candidate scoring) respectively.</p>

  <p>Layer 3: Where relevant, regulators could encourage adoption of sector-specific technical standards to support compliance with specific regulatory requirements and performance measures.<sup id="fnref:163"><a href="#fn:163" class="footnote" rel="footnote" role="doc-noteref">[footnote 163]</a></sup></p>
</div>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>21. Which non-regulatory tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> would most help organisations to embed the <abbr title="Artificial Intelligence">AI</abbr> regulation principles into existing business processes?</p>
</div>

<h2 id="part-5-territorial-application">Part 5: Territorial application</h2>

<h3 id="territorial-application-of-the-regulatory-framework">5.1 Territorial application of the regulatory framework</h3>

<p>113. Our <abbr title="Artificial Intelligence">AI</abbr> regulation framework applies to the whole of the UK. <abbr title="Artificial Intelligence">AI</abbr> is used in various sectors and impacts on a wide range of policy areas, some of which are reserved and some of which are devolved. We will continue to consider any devolution impacts of <abbr title="Artificial Intelligence">AI</abbr> regulation as the policy develops and in advance of any legislative action. Some regulators share remits with their counterparts in the devolved administrations. Our framework, to be initially set out on a non-statutory basis, will not alter the current territorial arrangement of <abbr title="Artificial Intelligence">AI</abbr> policy. We will rely on the interactions with existing legislation on reserved matters, such as the Data Protection Act 2018 and the Equality Act 2010, to implement our framework.</p>

<p>114. We will continue to engage devolved administrations, businesses, and members of the public from across the UK to ensure that every part of the country benefits from our pro-innovation approach. We will, for example, convene the devolved administrations for views on the functions we expect the government to perform and on the potential implications of introducing a statutory duty on regulators to have due regard to the principles.</p>

<h3 id="extraterritorial-application-of-the-regulatory-framework">5.2 Extraterritorial application of the regulatory framework</h3>

<p>115. While we expect our principles-based approach to influence the global conversation on <abbr title="Artificial Intelligence">AI</abbr> governance, we are not currently proposing the introduction of new legal requirements. Our framework will not therefore change the territorial applicability of existing legislation relevant to <abbr title="Artificial Intelligence">AI</abbr> (including, for example, data protection legislation).</p>

<h2 id="partsix">Part 6: Global interoperability and international engagement</h2>

<h3 id="our-regulatory-framework-on-the-world-stage">6.1 Our regulatory framework on the world stage</h3>

<p>116. Countries and jurisdictions around the world are moving quickly to set the rules that govern <abbr title="Artificial Intelligence">AI</abbr>. The UK is a global leader in <abbr title="Artificial Intelligence">AI</abbr> with a strategic advantage that places us at the forefront of these developments. The UK is ranked third in the world for <abbr title="Artificial Intelligence">AI</abbr> publications and also has the third highest number of <abbr title="Artificial Intelligence">AI</abbr> companies.<sup id="fnref:164"><a href="#fn:164" class="footnote" rel="footnote" role="doc-noteref">[footnote 164]</a></sup> We want to build on this position, making the UK the best place to research <abbr title="Artificial Intelligence">AI</abbr> and to create and build innovative <abbr title="Artificial Intelligence">AI</abbr> companies. At the same time, we recognise the importance of working closely with international partners. As such, the UK’s approach to both our domestic regulation and international discussions will continue to be guided by our ambition to develop <abbr title="Artificial Intelligence">AI</abbr> frameworks that champion our democratic values and economic priorities.</p>

<p>117. In line with our domestic approach, we will focus on supporting the positive global opportunities <abbr title="Artificial Intelligence">AI</abbr> can bring while protecting citizens against the potential harms and risks that can emanate across borders. We will work closely with international partners to both learn from, and influence, regulatory and non-regulatory developments (see examples in <a href="#box61">box 6.1</a>). Given the complex and cross-border nature of <abbr title="Artificial Intelligence">AI</abbr> supply chains, with many <abbr title="Artificial Intelligence">AI</abbr> businesses operating across multiple jurisdictions, close international cooperation will strengthen the impact of our proposed framework.</p>

<p>118. We will promote interoperability and coherence between different approaches, challenging barriers which may stand in the way of businesses operating internationally. We will ensure that the UK’s regulatory framework encourages the development of a responsive and compatible system of global <abbr title="Artificial Intelligence">AI</abbr> governance. We will build our international influence, allowing the UK to engage meaningfully with like-minded partners on issues such as cross-border <abbr title="Artificial Intelligence">AI</abbr> risks and opportunities.</p>

<p>119. The UK will continue to pursue an inclusive, multi-stakeholder approach, from negotiating new global norms to helping partner countries build their awareness and capacity in relation to the benefits and risks of <abbr title="Artificial Intelligence">AI</abbr> technology. We will, for example, support other nations to implement regulation and technical standards that support inclusive, responsible and sustainable artificial intelligence. More widely, the International Tech Strategy will reiterate how we will shape global <abbr title="Artificial Intelligence">AI</abbr> activities in line with UK values and priorities, protecting against efforts to adopt and apply <abbr title="Artificial Intelligence">AI</abbr> technologies in the service of authoritarianism and repression. We will work with UK industry leaders to ensure that we stay at the forefront of <abbr title="Artificial Intelligence">AI</abbr> and share our best practice with like-minded nations. Similarly, we will learn from our international partners, encouraging them to share lessons we can integrate into our framework.</p>

<p>120. Our international approach will include ensuring that proven, effective, and agreed upon assurance techniques and international technical standards play a role in the wider regulatory ecosystem. Such measures will also support cross-border trade by setting out risk management and <abbr title="Artificial Intelligence">AI</abbr> governance practices that are globally recognised by trading partners, reducing technical barriers to trade and increasing market access. We will also use our world-leading innovation provisions in Free Trade Agreements to address the challenges innovators in <abbr title="Artificial Intelligence">AI</abbr> may face and ensure that businesses are able to take advantage of the opportunities it presents.</p>

<p>121. In multilateral engagements, we will work to leverage each forum’s strengths, expertise and membership to ensure they are adding maximum value to global <abbr title="Artificial Intelligence">AI</abbr> governance discussions and are relevant to our democratic values and economic priorities.</p>

<div class="call-to-action">
  <h4 id="box61">Box 6.1: Examples of international engagement and collaboration</h4>

  <p><br>
The UK has played an active and leading role on the international <abbr title="Artificial Intelligence">AI</abbr> stage and will continue to do so. Some (non-exhaustive) examples of activities are:</p>

  <p><strong>Multilateral <abbr title="Artificial Intelligence">AI</abbr> engagement</strong></p>

  <ul>
    <li>
<strong><abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> <abbr title="Artificial Intelligence">AI</abbr> Governance Working Party (<abbr title="Artificial Intelligence">AI</abbr>-GO):</strong> The UK is an active member of the <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr>’s Working Party on <abbr title="Artificial Intelligence">AI</abbr> Governance (AIGO), which supports the implementation of the <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr>’s <abbr title="Artificial Intelligence">AI</abbr> principles and enables the exchange of experience and best practice to advance the responsible stewardship of <abbr title="Artificial Intelligence">AI</abbr>.<sup id="fnref:165"><a href="#fn:165" class="footnote" rel="footnote" role="doc-noteref">[footnote 165]</a></sup>
</li>
    <li>
<strong>Global Partnership on <abbr title="Artificial Intelligence">AI</abbr> (<abbr title="Global Partnership on AI">GPAI</abbr>):</strong> The UK is a key contributor to – and founding member of – the Global Partnership on <abbr title="Artificial Intelligence">AI</abbr> (<abbr title="Global Partnership on AI">GPAI</abbr>), which is an independent organisation consisting of 29 countries and a range of international experts. <abbr title="Global Partnership on AI">GPAI</abbr> was launched in 2020 as the first international multilateral forum to focus solely on <abbr title="Artificial Intelligence">AI</abbr> and the UK has played a significant role in shaping its development and influencing its agenda.<sup id="fnref:166"><a href="#fn:166" class="footnote" rel="footnote" role="doc-noteref">[footnote 166]</a></sup>
      <ul>
        <li>At the 2022 <abbr title="Global Partnership on AI">GPAI</abbr> Ministerial Summit in Japan, we demonstrated the scale of the UK’s <abbr title="Artificial Intelligence">AI</abbr> ambitions by announcing £1.2m of funding to develop a Net Zero Data Space for <abbr title="Artificial Intelligence">AI</abbr> Applications (which will also support our Net Zero policy objectives).<sup id="fnref:167"><a href="#fn:167" class="footnote" rel="footnote" role="doc-noteref">[footnote 167]</a></sup> This is in addition to the previous £1m investment to advance <abbr title="Global Partnership on AI">GPAI</abbr> research on data justice (collaborating with The Alan Turing Institute and 12 pilot partners in low and medium income countries).</li>
      </ul>
    </li>
    <li>
<strong>G7:</strong> The UK is actively engaged in the G7’s work on <abbr title="Artificial Intelligence">AI</abbr> and we are working closely with Japan – which holds the G7 Presidency for 2023 – to encourage greater international collaboration, support the development of consistent, proportionate and interoperable regulatory interventions, and champion the role of tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> where appropriate.</li>
    <li>
<strong>Council of Europe Committee on <abbr title="Artificial Intelligence">AI</abbr> (<abbr title="Council of Europe Committee on AI">CAI</abbr>):</strong> The UK holds a Bureau position and we are working closely with like-minded nations on the proposed Convention on <abbr title="Artificial Intelligence">AI</abbr>, to help protect human rights, democracy and rule of law.<sup id="fnref:168"><a href="#fn:168" class="footnote" rel="footnote" role="doc-noteref">[footnote 168]</a></sup>
</li>
    <li>
<strong>UNESCO:</strong> The UK was actively involved in the development of the UNESCO Ethics of <abbr title="Artificial Intelligence">AI</abbr> Recommendations and UK organisations have been supporting the development of implementation tools.<sup id="fnref:169"><a href="#fn:169" class="footnote" rel="footnote" role="doc-noteref">[footnote 169]</a></sup>
</li>
    <li>
<strong>Global standards development organisations:</strong> The UK will continue to work with international partners and global standards development organisations to develop and promote global technical standards for <abbr title="Artificial Intelligence">AI</abbr>, including through the UK <abbr title="Artificial Intelligence">AI</abbr> Standards Hub.<sup id="fnref:170"><a href="#fn:170" class="footnote" rel="footnote" role="doc-noteref">[footnote 170]</a></sup> For example, the UK is playing a leading role in the International Organisation for Standardisation and International Electrotechnical Commission<sup id="fnref:171"><a href="#fn:171" class="footnote" rel="footnote" role="doc-noteref">[footnote 171]</a></sup> (<abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr>) on 4 active <abbr title="Artificial Intelligence">AI</abbr> projects.<sup id="fnref:172"><a href="#fn:172" class="footnote" rel="footnote" role="doc-noteref">[footnote 172]</a></sup> Through the British Standards Institution (BSI),<sup id="fnref:173"><a href="#fn:173" class="footnote" rel="footnote" role="doc-noteref">[footnote 173]</a></sup> we are also a member of the Open Community for Ethics in Autonomous and Intelligent Systems (<abbr title="Open Community for Ethics in Autonomous and Intelligent Systems">OCEANIS</abbr>).<sup id="fnref:174"><a href="#fn:174" class="footnote" rel="footnote" role="doc-noteref">[footnote 174]</a></sup>
</li>
  </ul>

  <p><strong>Bilateral <abbr title="Artificial Intelligence">AI</abbr> engagement</strong></p>

  <ul>
    <li>The UK is engaging with individual nations and jurisdictions as they develop regulatory and governance approaches to <abbr title="Artificial Intelligence">AI</abbr>. These include the European Union (and its Member States), US, Canada, Singapore, Japan, Australia, Israel, Norway, and Switzerland, amongst many others. We will continue to maintain close dialogues to share information and knowledge, learn from and adapt our approach in collaboration with others, and work together to shape the international landscape.</li>
  </ul>
</div>

<h2 id="part-7-conclusion-and-next-steps">Part 7: Conclusion and next steps</h2>

<h3 id="conclusion-and-next-steps">7.1 Conclusion and next steps</h3>

<p>122. Our proportionate approach to regulating <abbr title="Artificial Intelligence">AI</abbr> is designed to strengthen the UK’s position as a global leader in artificial intelligence, harness <abbr title="Artificial Intelligence">AI</abbr>’s ability to drive growth and prosperity,<sup id="fnref:175"><a href="#fn:175" class="footnote" rel="footnote" role="doc-noteref">[footnote 175]</a></sup> and increase public trust in these technologies. The approach we set out is proportionate, adaptable, and context-sensitive to strike the right balance between responding to risks and maximising opportunities.</p>

<p>123. The proposals set out in this document have been informed by the feedback we received from over 130 respondents as part of our call for views on our 2022 policy paper. We will continue to work closely with businesses and regulators as we start to establish the central functions we have identified. Ongoing engagement with industry will be key to our monitoring and evaluation. Feedback will ensure the framework can adapt to new evidence, future-proofing the UK’s role as a leader in <abbr title="Artificial Intelligence">AI</abbr> innovation and ensuring that we can take a leading role in shaping the global narrative on <abbr title="Artificial Intelligence">AI</abbr> regulation.</p>

<p>124. Given the pace at which <abbr title="Artificial Intelligence">AI</abbr> technologies and risks emerge, and the scale of the opportunities at stake, we know that there is no time to waste if we are to strengthen the UK’s position as one of the best places in the world to start an <abbr title="Artificial Intelligence">AI</abbr> company. In collaboration with regulators, we are already exploring approaches to implementing the framework and will scale up this activity over the coming months. We are committed to an adaptable, iterative approach that allows us to learn and improve the framework. Our sovereign parliamentary system enables us to deliver targeted and proportionate measures – including by adapting existing legislation if necessary – based on emerging evidence.<sup id="fnref:176"><a href="#fn:176" class="footnote" rel="footnote" role="doc-noteref">[footnote 176]</a></sup> There are therefore aspects of our implementation work that will be delivered in parallel with the wider consultation set out in this white paper.</p>

<p>125. In the first 6 months following publication we will:</p>

<ul>
  <li>Engage with industry, the public sector, regulators, academia and civil society through the consultation period.</li>
  <li>Publish the government’s response to this consultation.</li>
  <li>Issue the cross-sectoral principles to regulators, together with initial guidance to regulators for their implementation. We will work with regulators to understand how the description of <abbr title="Artificial Intelligence">AI</abbr>’s characteristics can be applied within different regulatory remits and the impact this will have on the application of the cross-sectoral principles.</li>
  <li>Design and publish an <abbr title="Artificial Intelligence">AI</abbr> Regulation Roadmap with plans for establishing the central functions (detailed in <a href="#section331">section 3.3.1</a>), including monitoring and coordinating implementation of the principles. This roadmap will set out key partner organisations and identify existing initiatives that will be scaled-up or leveraged to deliver the central functions. It will also include plans to pilot a new <abbr title="Artificial Intelligence">AI</abbr> sandbox or testbed.</li>
  <li>
    <p>Analyse findings from commissioned research projects and improve our understanding of:</p>

    <ul>
      <li>Potential barriers faced by businesses seeking to comply with our framework and ways to overcome these.<br>
</li>
      <li>How accountability for regulatory compliance is currently assigned throughout the <abbr title="Artificial Intelligence">AI</abbr> life cycle in real-world scenarios.<br>
</li>
      <li>The ability of key regulators to implement our regulatory framework, and how we can best support them.<br>
</li>
      <li>Best practice in measuring and reporting on <abbr title="Artificial Intelligence">AI</abbr>-related risks across regulatory frameworks.</li>
    </ul>
  </li>
</ul>

<p>126. In the 6 to 12 months after publication we will:</p>

<ul>
  <li>Agree partnership arrangements with leading organisations and existing initiatives to deliver the first central functions.<br>
</li>
  <li>Encourage key regulators to publish guidance on how the cross-sectoral principles apply within their remit.<br>
</li>
  <li>Publish proposals for the design of a central <abbr title="monitoring and evaluation">M&amp;E</abbr> framework including identified metrics, data sources, and any identified thresholds or triggers for further intervention or iteration of the framework. This will be published for consultation.<br>
</li>
  <li>Continue to develop a regulatory sandbox or testbed with innovators and regulators.</li>
</ul>

<p>127. In the longer-term, 12 months or more after publication, we will:</p>

<ul>
  <li>Deliver a first iteration of all the central functions required to ensure the framework is effective.<br>
</li>
  <li>Work with key regulators that have not published guidance on how the cross-sectoral principles apply within their remit to encourage and support them to do so.<br>
</li>
  <li>Publish a draft central, cross-economy <abbr title="Artificial Intelligence">AI</abbr> risk register for consultation.<br>
</li>
  <li>Develop the regulatory sandbox or testbed drawing on insights from the pilot.<br>
</li>
  <li>Publish the first monitoring and evaluation report. This will evaluate how well the cross-sectoral principles are functioning and the delivery of the central functions. Performance will be measured against our framework characteristics: pro-innovation, proportionate, trustworthy, adaptable, clear and collaborative. The report will also consider existing regulatory activity and the role of government in supporting this, including whether appropriate guidance (including joint guidance) has been issued. In the report, we will include considerations on the need for any iteration of the framework, including the need for statutory interventions.<br>
</li>
  <li>Publish an updated <abbr title="Artificial Intelligence">AI</abbr> Regulation Roadmap which will set out plans for the future delivery of the central functions. In particular, it will assess whether a central government team is the most appropriate mechanism for overseeing the central functions in the longer term or if a more independent body would be more effective.</li>
</ul>

<div class="example">
  <p><strong>Consultation questions:</strong></p>

  <p>22. Do you have any other thoughts on our overall approach? Please include any missed opportunities, flaws, and gaps in our framework.</p>
</div>

<h2 id="annex-a-implementation-of-the-principles-by-regulators">Annex A: Implementation of the principles by regulators</h2>

<h3 id="a1-factors-that-government-believes-regulators-may-wish-to-consider-when-providing-guidanceimplementing-each-principle">A.1 Factors that government believes regulators may wish to consider when providing guidance/implementing each principle</h3>

<table>
  <thead>
    <tr>
      <th scope="col"> Principle</th>
      <th scope="col">Implementation considerations</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th scope="row"> Safety, security and robustness</th>
      <td>We anticipate that regulators will need to:<br><br> 1. Provide guidance about this principle including:<br><br> (i) considerations of good cybersecurity practices, such as the <abbr title="National Cyber Security Centre">NCSC</abbr> principles for the security of machine learning,<sup id="fnref:177"><a href="#fn:177" class="footnote" rel="footnote" role="doc-noteref">[footnote 177]</a></sup> as a secured system should be capable of maintaining the integrity of information.<br><br> (ii) considerations of privacy practices such as accessibility only to authorised users and safeguards against bad actors.<br><br> 2. Refer to a risk management framework that <abbr title="Artificial Intelligence">AI</abbr> life cycle actors should apply. Models should be regularly reviewed over time as a mitigation strategy.<br><br> 3. Consider the role of available technical standards, for example addressing <abbr title="Artificial Intelligence">AI</abbr> safety, security, testing, data quality, and robustness (including, <a rel="external" href="https://www.iso.org/standard/79804.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 24029-2</a><em>, <a rel="external" href="https://www.iso.org/standard/81088.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 5259-1</a></em>, <a rel="external" href="https://www.iso.org/standard/81092.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 5259-3</a>* , <a rel="external" href="https://www.iso.org/standard/81093.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 5259-4</a>* , and <a rel="external" href="https://www.iso.org/standard/81283.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 5469</a>*) to clarify regulatory guidance and support the implementation of risk treatment measures.</td>
    </tr>
    <tr>
      <th scope="row"> Appropriate transparency and explainability</th>
      <td>We anticipate that regulators will need to:<br><br>1. Set expectations for <abbr title="Artificial Intelligence">AI</abbr> life cycle actors to proactively or retrospectively provide information relating to:<br><br> (i) the nature and purpose of the <abbr title="Artificial Intelligence">AI</abbr> in question including information relating to any specific outcome,<br><br> (ii) the data being used and information relating to training data,<br><br> (iii) the logic and process used and where relevant information to support explainability of decision-making and outcomes,<br><br> (iiii) accountability for the <abbr title="Artificial Intelligence">AI</abbr> and any specific outcomes.<br><br>2. Set explainability requirements, particularly of higher risk systems, to ensure appropriate balance between information needs for regulatory enforcement (for example, around safety) and technical tradeoffs with system robustness.<br><br>3. Consider the role of available technical standards addressing <abbr title="Artificial Intelligence">AI</abbr> transparency and explainability (such as <a rel="external" href="https://standards.ieee.org/ieee/7001/6929/">IEEE 7001</a>, <a rel="external" href="https://www.iso.org/standard/82148.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TS 6254</a><em>, <a rel="external" href="https://www.iso.org/standard/84111.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 12792</a></em>)<sup id="fnref:178"><a href="#fn:178" class="footnote" rel="footnote" role="doc-noteref">[footnote 178]</a></sup> to clarify regulatory guidance and support the implementation of risk treatment measures.</td>
    </tr>
    <tr>
      <th scope="row"> Fairness</th>
      <td>We anticipate that regulators will need to:<br><br>1. Interpret and articulate ‘fairness’ as relevant to their sector or domain,<br><br>2. Decide in which contexts and specific instances fairness is important and relevant (which it may not always be).<br><br>3. Design, implement and enforce appropriate governance requirements for ‘fairness’ as applicable to the entities that they regulate.<br><br>4. Where a decision involving use of an <abbr title="Artificial Intelligence">AI</abbr> system has a legal or similarly significant effect on an individual, regulators will need to consider the suitability of requiring <abbr title="Artificial Intelligence">AI</abbr> system operators to provide an appropriate justification for that decision to affected parties.<br><br>5. <abbr title="Artificial Intelligence">AI</abbr> systems should comply with regulatory requirements relating to vulnerability of individuals within specific regulatory domains. Regulators will need to consider how use of <abbr title="Artificial Intelligence">AI</abbr> systems may alter individuals’ vulnerability, pursuant to their existing powers and remits.<br><br> 6. Consider the role of available technical standards addressing <abbr title="Artificial Intelligence">AI</abbr> fairness, bias mitigation and ethical considerations (for example, <a rel="external" href="https://www.iso.org/standard/77607.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 24027:2021</a>, <a rel="external" href="https://www.iso.org/standard/84110.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 12791</a>*, <a rel="external" href="https://www.iso.org/standard/78507.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 24368:2022</a>) to clarify regulatory guidance and support the implementation of risk treatment measures.</td>
    </tr>
    <tr>
      <th scope="row"> Accountability and governance</th>
      <td>We anticipate that regulators will need to:<br><br>1. Determine who is accountable for compliance with existing regulation and the principles. In the initial stages of implementation, regulators might provide guidance on how to demonstrate accountability. In the medium to long term, government may issue additional guidance on how accountability applies to specific actors within the ecosystem.<br><br>2. Provide guidance on governance mechanisms including, potentially, activities in scope of appropriate risk management and governance processes (including reporting duties).<br><br>3. Consider how available technical standards addressing <abbr title="Artificial Intelligence">AI</abbr> governance, risk management, transparency and other issues can support responsible behaviour and maintain accountability within an organisation (for example, <a rel="external" href="https://www.iso.org/standard/77304.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 23894</a><em>, <a rel="external" href="https://www.iso.org/standard/81230.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 42001</a></em>, <a rel="external" href="https://www.iso.org/standard/82148.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TS 6254</a><em>, <a rel="external" href="https://www.iso.org/standard/81283.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 5469</a></em> , <a rel="external" href="https://www.iso.org/standard/80655.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 25059</a>*).</td>
    </tr>
    <tr>
      <th scope="row"> Contestability and redress</th>
      <td>We anticipate that regulators will need to:<br><br>1. Create or update guidance with relevant information on where to direct a complaint or dispute for those affected by <abbr title="Artificial Intelligence">AI</abbr> harms. Guidance should clarify existing ‘formal’ routes of redress offered by regulators in certain scenarios.<br><br>2. Clarify interactions with requirements of appropriate transparency and explainability, acting as pre-conditions of effective redress and contestability.</td>
    </tr>
  </tbody>
</table>

<h2 id="annex-b-stakeholder-engagement">Annex B: Stakeholder engagement</h2>

<h3 id="b1-summary">B.1 Summary</h3>

<p>In July 2022, we published a policy paper outlining our proposals for <a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement#executive-summary">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr>.</a><sup id="fnref:179"><a href="#fn:179" class="footnote" rel="footnote" role="doc-noteref">[footnote 179]</a></sup> We proposed a non-statutory framework underpinned by a set of cross-sectoral principles including transparency, safety, and security. The principles were intended to guide how regulators approach <abbr title="Artificial Intelligence">AI</abbr> risks. We outlined our intention for the framework to be coherent, proportionate and adaptable, with regulatory coordination to reduce burdens on business and agility to keep pace with rapid technological advancements. Our proposals were designed to strengthen the UK’s position as a global leader in <abbr title="Artificial Intelligence">AI</abbr> by ensuring the UK is the best place to develop and use <abbr title="Artificial Intelligence">AI</abbr> technologies.</p>

<p>We launched a call for views on the proposals outlined in our policy paper to capture feedback from stakeholders between July and September 2022. We received responses from over 130 different stakeholders. There were some clear themes amongst the responses, with stakeholders noting the importance of regulatory coordination and asking for further details on how this will be achieved.</p>

<p>The 2023 <abbr title="Artificial Intelligence">AI</abbr> regulation white paper sets out our latest position based on the feedback we received. In particular, we have considered the need for new central functions to undertake activities such as system-wide risk monitoring and evaluation of the <abbr title="Artificial Intelligence">AI</abbr> regulation framework.</p>

<p>We welcome feedback on our latest proposals and will actively engage stakeholders as part of a consultation running to 21st June. See <a href="#annexc">Annex C</a> for more details on how to contribute to this consultation.</p>

<h3 id="b2-background">B.2 Background</h3>

<p>In July 2022, we opened a public call for views on our policy paper: <a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement#executive-summary">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>. We invited stakeholder views on how the UK can best set the rules for regulating <abbr title="Artificial Intelligence">AI</abbr> in a way that drives innovation and growth while also protecting our fundamental values. Feedback was collected to inform the development of the white paper.</p>

<p>We welcomed reflections on our proposed approach and specifically invited views and supporting evidence on the following questions:<br><br>1. What are the most important challenges with our existing approach to regulating <abbr title="Artificial Intelligence">AI</abbr>? Do you have views on the most important gaps, overlaps or contradictions?<br><br>2. Do you agree with the context-driven approach delivered through the UK’s established regulators set out in this paper? What do you see as the benefits of this approach? What are the disadvantages?<br><br>3. Do you agree that we should establish a set of cross-sectoral principles to guide our overall approach? Do the proposed cross-sectoral principles cover the common issues and risks posed by <abbr title="Artificial Intelligence">AI</abbr> technologies? What, if anything, is missing?<br><br>4. Do you have any early views on how we best implement our approach? In your view, what are some of the key practical considerations? What will the regulatory system need to deliver on our approach? How can we best streamline and coordinate guidance on <abbr title="Artificial Intelligence">AI</abbr> from regulators?<br><br>5. Do you anticipate any challenges for businesses operating across multiple jurisdictions? Do you have any early views on how our approach could help support cross-border trade and international cooperation in the most effective way?<br><br>6. Are you aware of any robust data sources to support monitoring the effectiveness of our approach, both at an individual regulator and system level?</p>

<p>The call for views and evidence was open for 10 weeks, closing on 26 September 2022. In this period we met with 39 stakeholders to capture detailed feedback on our proposals. In total, we received responses from over 130 stakeholders. Stakeholders represented a range of perspectives, from start-ups to Big Tech, and included developers, deployers, and funders from across the <abbr title="Artificial Intelligence">AI</abbr> life cycle. We also heard from researchers, regulators, lawyers, trade bodies and unions as well as representatives from the devolved administrations, local government, and wider public sector.</p>

<p>We have carefully analysed all the views and evidence submitted. We are grateful for the time and effort our stakeholders committed during this process, which has informed and strengthened our policy position as outlined in the white paper.</p>

<h3 id="b3-responses">B.3 Responses</h3>

<p>Overall, there was strong support for context specific regulation implemented by existing regulators and many noted that this approach would drive innovation. Stakeholders felt our proposals were a proportionate way to establish regulatory best practice in a fast-changing landscape. However, responses also asked for more practical detail, particularly around risk tolerance, compliance measures, and the overall coherence of the framework.</p>

<p>Our analysis found 6 overarching themes raised by stakeholders:<br><br><strong>1. Articulating the intended societal benefits of <abbr title="Artificial Intelligence">AI</abbr> is key to a future-proofed regulatory vision that works for citizens as well as businesses.</strong></p>

<p>Stakeholders were keen to see a long-term vision that set out our ambition to unlock societal benefits alongside economic opportunities. Stakeholders broadly agreed that the principles addressed the key risks posed by <abbr title="Artificial Intelligence">AI</abbr>. A number of stakeholders commented that our approach should explicitly reference human rights. While stakeholders welcomed our alignment with the <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> framework, many felt further use of international approaches by organisations such as the <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> or UNESCO would add more human focused benefits and aid companies working across jurisdictions. A small number of stakeholders noted that environmental sustainability was missing from our principles. Some suggested that it should be included as a core principle, while others recommended that environmental outcomes should be measured through impact assessments.</p>

<p><strong>Government response:</strong> We have analysed our principles in consideration of both stakeholder feedback and our risk assessment work. The white paper clarifies the substance of the principles in <a href="#section323">section 3.2.3</a>. Human rights and environmental sustainability are not explicitly named in the revised principles as we expect regulators to adhere to existing law when implementing the principles. We have emphasised the social benefits alongside the economic opportunities we intend to unlock with our pro-innovation approach to <abbr title="Artificial Intelligence">AI</abbr> regulation.<br><br><strong>2. Offering greater central clarity around the scope of the regime is critical to ensuring business confidence</strong></p>

<p>A number of stakeholders praised our description of <abbr title="Artificial Intelligence">AI</abbr> for capturing the distinct regulatory challenges that <abbr title="Artificial Intelligence">AI</abbr> poses and our proposed characteristics were largely considered to be fit for purpose. There were some concerns that the definition was not ‘user-friendly’ on its own. While many felt that creating a more specific definition of <abbr title="Artificial Intelligence">AI</abbr> would be difficult and some noted it could be unhelpful, there was clear appetite for further detail on how regulators will maintain a coherent definition of <abbr title="Artificial Intelligence">AI</abbr> within and across sectors. Use cases were suggested as a means of illustrating <abbr title="Artificial Intelligence">AI</abbr> technologies within scope.</p>

<p>Many stakeholders, especially from industry, were keen to see a clear and transparent risk management framework with assessment criteria. In particular, multiple stakeholders felt that it would be beneficial for central government or a central body to provide a clear description of what constitutes ‘unacceptable risk’. Some suggested this could complement more detailed risk analysis by regulators to ensure a coordinated and coherent approach – as well as effectively identifying any gaps. Stakeholders indicated that greater clarity on risk would support business development and could also promote high standards, public trust, and the adoption of <abbr title="Artificial Intelligence">AI</abbr>.</p>

<p><strong>Government response:</strong> We stress-tested our proposed characteristics of <abbr title="Artificial Intelligence">AI</abbr> against stakeholder feedback and found that concerns centred on how we would ensure coherence across sectors and regulators. We recognise a trade-off between the certainty provided by a blanket approach, such as a singular definition and central risk framework, and the agility enabled by sector-specific expertise, including regulator-refined definitions. Given the fast pace of technological development and stakeholder praise for a future-proofed approach, we have retained our core, defining characteristics for <abbr title="Artificial Intelligence">AI</abbr>, see <a href="#section321">section 3.2.1</a>. We have considered how regulators can be given the technical capability necessary to create clear definitions for <abbr title="Artificial Intelligence">AI</abbr> in and across their sectors, see <a href="#section321">section 3.2.1</a>. In <a href="#section331">section 3.3.1</a> of the white paper, we outline how new central functions will help identify conflicts or gaps in regulator definitions of <abbr title="Artificial Intelligence">AI</abbr>. Acknowledging feedback that a central steer on ‘acceptable’ risk would provide business confidence and investment, we have proposed that centralised risk monitoring and horizon scanning would be key central functions.<br><br><strong>3. A principles-based approach will enable regulation to keep pace with a fast-evolving technology</strong></p>

<p>Stakeholders generally agreed that a principles-based approach implemented by regulators would offer a proportionate way to build best practice. Stakeholders felt the principles address the key risks that <abbr title="Artificial Intelligence">AI</abbr> poses while allowing regulators to tailor approaches to their sectors. Stakeholders welcomed our use of the <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> principles as a means of promoting international alignment and interoperability.</p>

<p>While stakeholders recognised the benefits that a flexible non-statutory approach offers, some stakeholders were concerned that a non-statutory approach would be unenforceable. A few stakeholders suggested clarifying how <abbr title="Artificial Intelligence">AI</abbr> regulation dovetails with existing legislation and defining thresholds for when our regime may shift to statutory implementation.</p>

<p><strong>Government response:</strong> We appreciate the praise of our adaptation of the multilaterally agreed <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> principles. We further outline our international approach in the white paper, recognising that interoperability will help ensure that UK businesses can continue to innovate. While we continue with a non-statutory approach for initial implementation, reflecting on stakeholder concerns around enforceability, we anticipate that introducing a statutory duty to have due regard on regulators might be needed to strengthen the framework. A duty to have due regard to our cross-sector principles will provide a legislative incentive while maintaining flexibility for the framework to adapt to technological changes. We will monitor the implementation of the framework to assess whether it is effective without the need to implement a statutory duty and will also review responses to the white paper consultation.<br><br><strong>4. Providing centralised coordination and oversight will be essential to regulatory coherence and horizon scanning</strong></p>

<p>Stakeholders voiced concerns that regulators did not have the capability to ensure a coherent compliance process, especially for businesses operating across or between industry sectors or regulatory remits. Stakeholders reported expensive, time-consuming confusion when there was not clear regulatory ownership of a technology or issue. Some criticised communication and knowledge-sharing between regulators. One stakeholder explained that joint guidance had previously been very useful. Others suggested that regulators should have more stringent duties to collaborate to ensure consistency and shared best practice.</p>

<p>A number of stakeholders were supportive of a central coordination function for existing regulators, as opposed to a new regulator for <abbr title="Artificial Intelligence">AI</abbr>. Many stressed the importance of a coordination function to aid navigation of trade-offs and conflicts (such as between the need to collect data to minimise bias and the need to refrain from collecting data in the interest of privacy). While many stakeholders stated the need for central coordination, many were solution-agnostic. Proposals included:</p>

<ul>
  <li>An expanded role for the <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr>. Some stakeholders suggested the <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr> was well-positioned to take on a coordination function but others questioned the <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr>’s suitability. In particular, it was felt that the <abbr title="Digital Regulation Cooperation Forum">DRCF</abbr> would require more capacity to fulfil a coordination role.</li>
  <li>A new central body to undertake coordination. Stakeholders suggested establishing a new body, such as a ‘Centre for <abbr title="Artificial Intelligence">AI</abbr> Governance’, to undertake functions such as: conducting cross-sector risk-mapping; conducting regulatory gap analyses and horizon scanning; monitoring the applicability of emerging <abbr title="Artificial Intelligence">AI</abbr> standards; supplying training; and monitoring international approaches.</li>
  <li>Appointing an existing regulator as ‘lead regulator’ for <abbr title="Artificial Intelligence">AI</abbr>. Some stakeholders felt that regulators should have more incentives to work together and the entire regulatory landscape could learn from more advanced regulators.</li>
</ul>

<p>Stakeholders stated the importance of clarifying regulator remits and addressing gaps, noting the fast pace of change for <abbr title="Artificial Intelligence">AI</abbr> technologies. Some suggested that a coordination body should be responsible for a horizon scanning function that monitors and evaluates risks.</p>

<p><strong>Government response:</strong> Building on reflections from stakeholders, we identified a small range of regulators with remits that are likely to be significantly affected by <abbr title="Artificial Intelligence">AI</abbr> and conducted analysis of their capability to implement our policy paper proposals. We found varied readiness, with some regulators already demonstrating world-leading approaches to regulating <abbr title="Artificial Intelligence">AI</abbr> and others asking for further support. Similarly, knowledge and information sharing mechanisms were not uniform across regulators and we identified a need for coordination mechanisms to streamline compliance processes for business and ensure regulation provides system-wide coverage of current and future opportunities. We considered multiple options for coordination functions, in line with stakeholder suggestions, and incorporated feedback into our analysis. We outline our proposals for central functions in <a href="#section331">section 3.3.1</a> in the white paper.<br><br><strong>5. Streamlining liability and tailoring reporting obligations will be key to enabling responsible innovation</strong></p>

<p>While stakeholders were strongly supportive of compliance and assurance as a means of facilitating public trust and the wider adoption of <abbr title="Artificial Intelligence">AI</abbr> technologies, many were keen to limit the burden of reporting obligations, particularly for startups and <abbr title="small and medium-sized enterprises">SMEs</abbr>. Industry stakeholders noted that the costs of reporting burdens would be passed onto consumers. Some stakeholders emphasised that the government should have a role in providing education and support for small businesses.</p>

<p>There was interest in regulatory sandboxes as a way to enable investment and establish best practice. Generally there was a strong appetite for industry-led solutions and a less burdensome or ‘tick box’ approach to compliance. Stakeholders were strongly supportive of standards as a way to drive accountability, adoption, and good consumer outcomes. Stakeholders suggested sector compliance templates and voluntary industry forums as ways to share knowledge and reduce the burden of establishing best practice.</p>

<p>Some stakeholders felt the paper lacked a position on liability and argued a clear allocation of legal responsibility would enable effective enforcement and unlock investment. More specifically, some stakeholders suggested that, when appropriate, targeting foundation models (often developed by larger organisations) would increase innovation and competition by reducing liability burdens on smaller companies. Stakeholders often suggested impact assessments could be used to help address liability issues at all stages of the <abbr title="Artificial Intelligence">AI</abbr> life cycle.</p>

<p><strong>Government response:</strong> We welcomed the thoughtful suggestions from respondents regarding innovative compliance measures. We noted the significant appetite for regulatory sandboxes and have outlined our proposals in the white paper, see <a href="#section334">section 3.3.4</a>. We agree that reporting burdens should be proportionate and give detail on how we will continue to work with regulators to ensure compliance measures are streamlined. We acknowledge that regulation measures can affect competition and innovation by creating undue burdens on start-ups and <abbr title="small and medium-sized enterprises">SMEs</abbr>. We are confident that regulators will oversee proportionate and innovation friendly measures in their remits, with a central function undertaking activity to streamline and ensure coherence. We recognise that liability is complicated by a complex <abbr title="Artificial Intelligence">AI</abbr> value-chain that can incorporate many different actors in different roles. As such, we believe that regulators are best positioned to begin allocating liability in their sectors, adopting a context-based approach that builds on best practice. Our proposal setting out activities to be undertaken centrally will ensure that regulators’ approaches to liability are proportionate, coherent across sectors, and supportive of innovation.<br><br><strong>6. Establishing interoperability will be critical to ensuring an internationally competitive approach</strong></p>

<p>Stakeholders welcomed the UK’s relatively flexible approach but many were concerned that the need for interoperability across jurisdictions would result in businesses conforming to the strictest regulation. Stakeholders warned that international divergence could create more burdens than advantages for businesses. Many stakeholders wanted friction minimised to ensure export prospects for British businesses, with support for an international agreement on <abbr title="Artificial Intelligence">AI</abbr> regulation equivalence, where <abbr title="Artificial Intelligence">AI</abbr> systems authorised on key international markets would be permitted for trade in the UK. Many stakeholders also wanted to see the UK maintain its position as a global leader in <abbr title="Artificial Intelligence">AI</abbr> discussions. Stakeholders emphasised the importance of alignment with international partners such as the EU and US to ensure global <abbr title="Artificial Intelligence">AI</abbr> governance supports our common democratic values.</p>

<p><strong>Government response:</strong> In the white paper, we set out our vision for <abbr title="Artificial Intelligence">AI</abbr> regulation to ensure that the UK is the best place to start and grow an <abbr title="Artificial Intelligence">AI</abbr> business. We share stakeholder concerns on interoperability and plan to continue using our leading role in international forums such as the <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr>, G7, and Council of Europe to promote pro-innovation approaches to regulation that capitalise on the potential social and economic benefits of <abbr title="Artificial Intelligence">AI</abbr> while addressing the new risks the technology can pose. Our plan for international engagement, detailed in <a href="#partsix">part 6</a>, clarifies our approach with an emphasis on interoperability.</p>

<h2 id="annexc">Annex C: How to respond to this consultation</h2>

<p>We are inviting individuals and organisations to provide their views by responding to the questions set out in this consultation. The questions are listed below.</p>

<p>The consultation will be open for 12 weeks, until 21 June.</p>

<p>You can respond online via the <a rel="external" href="https://dcms.eu.qualtrics.com/jfe/form/SV_cBDeiMplOHExtYO">consultation survey link here</a>.</p>

<p>Our <a href="https://www.gov.uk/government/publications/office-for-artificial-intelligence-information-collection-and-analysis-privacy-notice">privacy statement is set out here</a>.</p>

<p>If for exceptional reasons, you are unable to use the online system, for example because you use specialist accessibility software that is not compatible with the system, you may request and complete a word document version of the form.</p>

<p><strong>By email</strong><br>
<a href="mailto:evidence@officeforai.gov.uk">evidence@officeforai.gov.uk</a></p>

<p><strong>By post</strong></p>
<div class="address"><div class="adr org fn"><p>
Office for Artificial Intelligence<br>Department for Science, Innovation and Technology<br>100 Parliament Street<br>London<br>SW1A 2BQ
</p></div></div>

<div class="example">
  <h3 id="questions">Questions:</h3>

  <h4 id="the-revised-cross-sectoral-ai-principles">The revised cross-sectoral <abbr title="Artificial Intelligence">AI</abbr> principles</h4>

  <p><br>
1. Do you agree that requiring organisations to make it clear when they are using <abbr title="Artificial Intelligence">AI</abbr> would adequately ensure transparency?</p>

  <p>2. What other transparency measures would be appropriate, if any?</p>

  <p>3. Do you agree that current routes to contestability or redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms are adequate?</p>

  <p>4. How could routes to contestability or redress for <abbr title="Artificial Intelligence">AI</abbr>-related harms be improved, if at all?</p>

  <p>5. Do you agree that, when implemented effectively, the revised cross-sectoral principles will cover the risks posed by <abbr title="Artificial Intelligence">AI</abbr> technologies?</p>

  <p>6. What, if anything, is missing from the revised principles?</p>

  <h4 id="a-statutory-duty-to-regard">A statutory duty to regard</h4>

  <p><br>
7. Do you agree that introducing a statutory duty on regulators to have due regard to the principles would clarify and strengthen regulators’ mandates to implement our principles, while retaining a flexible approach to implementation?</p>

  <p>8. Is there an alternative statutory intervention that would be more effective?</p>

  <h4 id="new-central-functions-to-support-the-framework">New central functions to support the framework</h4>

  <p><br>
9. Do you agree that the functions outlined in <a href="#section331">section 3.3.1</a> would benefit our <abbr title="Artificial Intelligence">AI</abbr> regulation framework if delivered centrally?</p>

  <p>10. What, if anything, is missing from the central functions?</p>

  <p>11. Do you know of any existing organisations who should deliver one or more of our proposed central functions?</p>

  <p>12. Are there additional activities that would help businesses confidently innovate and use <abbr title="Artificial Intelligence">AI</abbr> technologies?</p>

  <p>12.1. If so, should these activities be delivered by government, regulators or a different organisation?</p>

  <p>13. Are there additional activities that would help individuals and consumers confidently use <abbr title="Artificial Intelligence">AI</abbr> technologies?</p>

  <p>13.1. If so, should these activities be delivered by government, regulators or a different organisation?</p>

  <p>14. How can we avoid overlapping, duplicative or contradictory guidance on <abbr title="Artificial Intelligence">AI</abbr> issued by different regulators?</p>

  <h4 id="monitoring-and-evaluation-of-the-framework">Monitoring and evaluation of the framework</h4>

  <p><br>
15. Do you agree with our overall approach to monitoring and evaluation?</p>

  <p>16. What is the best way to measure the impact of our framework?</p>

  <p>17. Do you agree that our approach strikes the right balance between supporting <abbr title="Artificial Intelligence">AI</abbr> innovation; addressing known, prioritised risks; and future-proofing the <abbr title="Artificial Intelligence">AI</abbr> regulation framework?</p>

  <p>18. Do you agree that regulators are best placed to apply the principles and government is best placed to provide oversight and deliver central functions?</p>

  <h4 id="regulator-capabilities-1">Regulator capabilities</h4>

  <p><br>
19. As a regulator, what support would you need in order to apply the principles in a proportionate and pro-innovation way?</p>

  <p>20. Do you agree that a pooled team of <abbr title="Artificial Intelligence">AI</abbr> experts would be the most effective way to address capability gaps and help regulators apply the principles?</p>

  <h4 id="tools-for-trustworthy-ai">Tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr>
</h4>

  <p><br>
21. Which non-regulatory tools for trustworthy <abbr title="Artificial Intelligence">AI</abbr> would most help organisations to embed the <abbr title="Artificial Intelligence">AI</abbr> regulation principles into existing business processes?</p>

  <h4 id="final-thoughts">Final thoughts</h4>

  <p><br>
22. Do you have any other thoughts on our overall approach? Please include any missed opportunities, flaws, and gaps in our framework.</p>

  <h4 id="legal-responsibility-for-ai">Legal responsibility for <abbr title="Artificial Intelligence">AI</abbr>
</h4>

  <p><br>
L1. What challenges might arise when regulators apply the principles across different <abbr title="Artificial Intelligence">AI</abbr> applications and systems? How could we address these challenges through our proposed <abbr title="Artificial Intelligence">AI</abbr> regulatory framework?</p>

  <p>L2.i. Do you agree that the implementation of our principles through existing legal frameworks will fairly and effectively allocate legal responsibility for <abbr title="Artificial Intelligence">AI</abbr> across the life cycle?</p>

  <p>L.2.ii. How could it be improved, if at all?</p>

  <p>L3.&nbsp;If you are a business that develops, uses, or sells <abbr title="Artificial Intelligence">AI</abbr>, how do you currently manage <abbr title="Artificial Intelligence">AI</abbr> risk including through the wider supply chain? How could government support effective <abbr title="Artificial Intelligence">AI</abbr>-related risk management?</p>

  <h4 id="foundation-models-and-the-regulatory-framework">Foundation models and the regulatory framework</h4>

  <p><br>
F1. What specific challenges will foundation models such as large language models (<abbr title="large language models">LLMs</abbr>) or open-source models pose for regulators trying to determine legal responsibility for <abbr title="Artificial Intelligence">AI</abbr> outcomes?</p>

  <p>F2. Do you agree that measuring compute provides a potential tool that could be considered as part of the governance of foundation models?</p>

  <p>F3. Are there other approaches to governing foundation models that would be more effective?</p>

  <h4 id="ai-sandboxes-and-testbeds">
<abbr title="Artificial Intelligence">AI</abbr> sandboxes and testbeds</h4>

  <p><br>
S1. Which of the sandbox models described in <a href="#section334">section 3.3.4</a> would be most likely to support innovation?</p>

  <p>S2. What could government do to maximise the benefit of sandboxes to <abbr title="Artificial Intelligence">AI</abbr> innovators</p>

  <p>S3. What could government do to facilitate participation in an <abbr title="Artificial Intelligence">AI</abbr> regulatory sandbox?</p>

  <p>S4. Which industry sectors or classes of product would most benefit from an <abbr title="Artificial Intelligence">AI</abbr> sandbox?</p>
</div>

<div class="footnotes" role="doc-endnotes">
  <ol>
    <li id="fn:1">
      <p><a rel="external" href="https://www.insiderintelligence.com/insights/artificial-intelligence-healthcare/">The use of <abbr title="Artificial Intelligence">AI</abbr> in healthcare and medicine is booming</a>, Insider Intelligence, 2023.&nbsp;<a href="#fnref:1" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:2">
      <p><a rel="external" href="https://www.forbes.com/sites/markminevich/2022/07/08/how-to-fight-climate-change-using-ai/?sh=5f274222a838">How to fight climate change using <abbr title="Artificial Intelligence">AI</abbr></a>, Forbes, 2022; <a rel="external" href="https://arxiv.org/abs/1906.05433">Tackling Climate Change with Machine Learning</a>, Rolnick et al., 2019.&nbsp;<a href="#fnref:2" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:3">
      <p><a rel="external" href="https://www.newscientist.com/article/2330866-deepminds-protein-folding-ai-cracks-biologys-biggest-problem/">DeepMind’s protein-folding <abbr title="Artificial Intelligence">AI</abbr> cracks biology’s biggest problem, New Scientist,</a> 2022; <a rel="external" href="https://www.nature.com/articles/s41586-019-1923-7">Improved protein structure prediction using potentials from deep learning</a>, Senior et al., 2020.&nbsp;<a href="#fnref:3" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:4">
      <p><a href="https://www.gov.uk/government/publications/uk-science-and-technology-framework/the-uk-science-and-technology-framework#regulation-and-standards">The UK Science and Technology Framework</a>, Department for Science, Innovation and Technology, 2023.&nbsp;<a href="#fnref:4" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:5">
      <p><a rel="external" href="https://technologymagazine.com/articles/six-of-the-best-future-uses-for-artificial-intelligence?utm_campaign=Artificial%2BIntelligence%2BWeekly&amp;utm_medium=email&amp;utm_source=Artificial_Intelligence_Weekly_316">Six of the best future uses of Artificial Intelligence</a>, Technology Magazine, 2023; <a rel="external" href="https://www.sciencedirect.com/science/article/abs/pii/S026840121930917X?via%3Dihub">Multidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy</a>, Dwivedi et al., 2021.&nbsp;<a href="#fnref:5" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:6">
      <p>Large dedicated <abbr title="Artificial Intelligence">AI</abbr> companies make a major contribution to the UK economy, with GVA (gross value added) per employee estimated to be £400k, more than double that of comparable estimates of large dedicated firms in other sectors. See <a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022"><abbr title="Artificial Intelligence">AI</abbr> Sector Study 2022, DSIT, 2023</a>.&nbsp;<a href="#fnref:6" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:7">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:7" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:8">
      <p><a href="https://www.gov.uk/government/publications/ai-barometer-2021/ai-barometer-part-4-transport-and-logistics#risks"><abbr title="Artificial Intelligence">AI</abbr> Barometer Part 4 –Transport and logistics</a>, Centre for Data Ethics and Innovation, 2021.&nbsp;<a href="#fnref:8" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:9">
      <p><a rel="external" href="https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html">How TikTok Reads Your Mind</a>, New York Times, 2021.&nbsp;<a href="#fnref:9" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:10">
      <p><a rel="external" href="https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html">Privacy Considerations in Large Language Models</a>, Google Research, 2020.&nbsp;<a href="#fnref:10" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:11">
      <p><a rel="external" href="https://www.turing.ac.uk/sites/default/files/2021-03/cahai_feasibility_study_primer_final.pdf">Artificial Intelligence, Human Rights, Democracy, and the Rule of Law,</a> Alan Turing Institute and Council of Europe, 2021.&nbsp;<a href="#fnref:11" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:12">
      <p><a rel="external" href="https://www.oecd-ilibrary.org/science-and-technology/demand-for-ai-skills-in-jobs_3ed32d94-en">Demand for <abbr title="Artificial Intelligence">AI</abbr> skills in jobs</a>, <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> iLibrary, 2021.&nbsp;<a href="#fnref:12" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:13">
      <p><a href="https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance">Public expectations for <abbr title="Artificial Intelligence">AI</abbr> governance (transparency, fairness and accountability)</a>, Centre for Data Ethics and Innovation, 2023.&nbsp;<a href="#fnref:13" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:14">
      <p>The <abbr title="Artificial Intelligence">AI</abbr> sector is estimated to contribute £3.7bn in GVA (Gross Value Added) to the UK economy. <a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022"><abbr title="Artificial Intelligence">AI</abbr> Sector Study 2022</a>, DSIT, 2023.&nbsp;<a href="#fnref:14" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:15">
      <p><a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>, Office for Artificial Intelligence, 2022.&nbsp;<a href="#fnref:15" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:16">
      <p><a rel="external" href="https://www.tortoisemedia.com/intelligence/global-ai/">Global <abbr title="Artificial Intelligence">AI</abbr> Index,</a> Tortoise Media, 2022.&nbsp;<a href="#fnref:16" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:17">
      <p>Transport apps like <a rel="external" href="http://maps.google.com">Google Maps</a>, and <a rel="external" href="http://citymapper.com">CityMapper</a>, use <abbr title="Artificial Intelligence">AI</abbr>.&nbsp;<a href="#fnref:17" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:18">
      <p><a rel="external" href="https://researchberg.com/index.php/rrst/article/view/37">Artificial Intelligence in Banking Industry: A Review on Fraud Detection, Credit Management, and Document Processing</a>, ResearchBerg Review of Science and Technology, 2018.&nbsp;<a href="#fnref:18" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:19">
      <p><a rel="external" href="https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control">Accelerating fusion science through learned plasma control</a>, Deepmind, 2022; <a rel="external" href="https://www.nature.com/articles/s41586-021-04301-9">Magnetic control of tokamak plasmas through deep reinforcement learning</a>, Degrave et al., 2022.&nbsp;<a href="#fnref:19" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:20">
      <p><a rel="external" href="https://www.morganstanley.com/ideas/ai-drug-discovery#:~:text=Biotechs%20are%20applying%20AI%20and,new%20drugs%20is%20costly%20guesswork.">Why Artificial Intelligence Could Speed Drug Discovery,</a> Morgan Stanley, 2022.&nbsp;<a href="#fnref:20" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:21">
      <p><a rel="external" href="https://www.bcg.com/publications/2022/how-ai-can-help-climate-change"><abbr title="Artificial Intelligence">AI</abbr> Is Essential for Solving the Climate Crisis</a>, BCG, 2022.&nbsp;<a href="#fnref:21" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:22">
      <p><a rel="external" href="https://www.nber.org/papers/w11093">General Purpose Technologies – Handbook of Economic Growth</a>, National Bureau of Economic Research, 2005.&nbsp;<a href="#fnref:22" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:23">
      <p><a href="https://www.gov.uk/government/publications/uk-science-and-technology-framework">The UK Science and Technology Framework</a>, Department for Science, Innovation and Technology, 2023.&nbsp;<a href="#fnref:23" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:24">
      <p>In 2022 annual revenues generated by UK <abbr title="Artificial Intelligence">AI</abbr> companies totalled an estimated £10.6 billion. <a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022"><abbr title="Artificial Intelligence">AI</abbr> Sector Study 2022</a>, DSIT, 2023.&nbsp;<a href="#fnref:24" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:25">
      <p>DSIT analysis estimates over 50,000 full time workers are employed in <abbr title="Artificial Intelligence">AI</abbr> roles in <abbr title="Artificial Intelligence">AI</abbr> companies. <a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022"><abbr title="Artificial Intelligence">AI</abbr> Sector Study 2022</a>, DSIT, 2023.&nbsp;<a href="#fnref:25" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:26">
      <p>For example, <abbr title="Artificial Intelligence">AI</abbr> can potentially improve health and safety in mining while also improving efficiency. See <a rel="external" href="https://www.axora.com/insights/how-ai-is-being-used-to-improve-health-and-safety-in-mining/"><abbr title="Artificial Intelligence">AI</abbr> on-side: how artificial intelligence is being used to improve health and safety in mining</a>, Axora, 2023. <a href="#box11">Box 1.1</a> gives further examples of <abbr title="Artificial Intelligence">AI</abbr> driving efficiency improvements.&nbsp;<a href="#fnref:26" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:27">
      <p><a rel="external" href="https://www.forbes.com/sites/garydrenik/2023/01/11/large-language-models-will-define-artificial-intelligence/">Large Language Models Will Define Artificial Intelligence</a>, Forbes, 2023; <a rel="external" href="https://arxiv.org/abs/2112.04426">Scaling Language Models: Methods, Analysis &amp; Insights from Training Gopher</a>, Borgeaud et al., 2022.&nbsp;<a href="#fnref:27" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:28">
      <p>See, for example, <a rel="external" href="https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/">What are Large Language Models used for?</a> NVIDIA, 2023.&nbsp;<a href="#fnref:28" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:29">
      <p><a rel="external" href="https://www.nature.com/articles/d41586-019-01155-0">Black hole pictured for first time – in spectacular detail</a>, Nature, 2019.&nbsp;<a href="#fnref:29" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:30">
      <p><a rel="external" href="https://unfolded.deepmind.com/stories/unlocking-a-decade-of-data-to-fight-antibiotic-resistance">Accelerating the race against antibiotic resistance</a>, Deepmind, 2022.&nbsp;<a href="#fnref:30" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:31">
      <p><a rel="external" href="https://unfolded.deepmind.com/stories/matthew-higgins-is-unlocking-a-new-path-to-stop-malaria-in-its-tracks">Stopping malaria in its tracks</a>, Deepmind, 2022.&nbsp;<a href="#fnref:31" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:32">
      <p><a rel="external" href="https://unfolded.deepmind.com/stories/accelerating-the-fight-against-plastic-pollution">Creating plastic-eating enzymes that could save us from pollution</a>, Deepmind, 2022.&nbsp;<a href="#fnref:32" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:33">
      <p><a rel="external" href="https://transform.england.nhs.uk/ai-lab/explore-all-resources/understand-ai/mia-mammography-intelligent-assessment/">Mia mammography intelligent assessment</a>, <abbr title="National Health Service">NHS</abbr> England, 2021.&nbsp;<a href="#fnref:33" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:34">
      <p><a rel="external" href="https://link.springer.com/article/10.1007/s43154-022-00077-6">Robotics and Autonomous Systems for Net Zero Agriculture</a>, Pearson et al., 2022.&nbsp;<a href="#fnref:34" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:35">
      <p><a rel="external" href="https://www.ingentaconnect.com/content/ben/cdt/2021/00000022/00000006/art00005">Artificial intelligence, big data and machine learning approaches to precision medicine and drug discovery</a>, Current Drug Targets, 2021.&nbsp;<a href="#fnref:35" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:36">
      <p><a rel="external" href="https://www.biorxiv.org/content/10.1101/2023.01.08.523187v1.full.pdf">Unlocking de novo antibody design with generative artificial intelligence</a>, Shanehsazzadeh et al., 2023.&nbsp;<a href="#fnref:36" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:37">
      <p><a href="https://www.gov.uk/government/news/pioneering-new-tools-to-be-rolled-out-in-fight-against-child-abusers">Pioneering new tools to be rolled out in fight against child abusers</a>, Home Office, 2019.&nbsp;<a href="#fnref:37" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:38">
      <p><a rel="external" href="https://www.ncsc.gov.uk/collection/intelligent-security-tools">Intelligent security tools</a>, National Cyber Security Centre, 2019.&nbsp;<a href="#fnref:38" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:39">
      <p><a rel="external" href="https://www.vox.com/recode/2023/1/5/23539055/generative-ai-chatgpt-stable-diffusion-lensa-dall-e">What is generative <abbr title="Artificial Intelligence">AI</abbr>, and why is it suddenly everywhere?</a>, Vox, 2023.&nbsp;<a href="#fnref:39" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:40">
      <p>See, for example, <a href="https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-workstream-spring-2022/the-benefits-and-harms-of-algorithms-a-shared-perspective-from-the-four-digital-regulators">The Benefits and Harms of Algorithms</a>, The Digital Regulation Cooperation Forum, 2022; <a rel="external" href="https://www.nber.org/system/files/working_papers/w29247/w29247.pdf">Harms of <abbr title="Artificial Intelligence">AI</abbr></a>, Acemoglu, 2021.&nbsp;<a href="#fnref:40" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:41">
      <p><a rel="external" href="https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/"><abbr title="Artificial Intelligence">AI</abbr> Accidents: An Emerging Threat</a>, Center for Security and Emerging Technology, 2021.&nbsp;<a href="#fnref:41" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:42">
      <p><a rel="external" href="https://www.nature.com/articles/s42256-021-00338-7"><abbr title="Artificial Intelligence">AI</abbr> for radiographic COVID-19 detection selects shortcuts over signal</a>, DeGrave, Janizek and Lee, 2021; <a rel="external" href="https://5rightsfoundation.com/uploads/Pathways-how-digital-design-puts-children-at-risk.pdf">Pathways: How digital design puts children at risk</a>, 5Rights Foundation, 2021.&nbsp;<a href="#fnref:42" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:43">
      <p><a rel="external" href="https://maliciousaireport.com/">The Malicious Use of Artificial Intelligence</a>, Malicious <abbr title="Artificial Intelligence">AI</abbr> Report, 2018.&nbsp;<a href="#fnref:43" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:44">
      <p><a rel="external" href="https://www.cambridge.org/core/books/constitutional-challenges-in-the-algorithmic-society/831B39F76C7870B330052D852D598F98">Constitutional Challenges in the Algorithmic Society</a>, Micklitz et al., 2022.&nbsp;<a href="#fnref:44" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:45">
      <p><a href="https://www.gov.uk/government/publications/cdei-publishes-its-first-series-of-three-snapshot-papers-ethical-issues-in-ai/snapshot-paper-smart-speakers-and-voice-assistants">Smart Speakers and Voice Assistants</a>, CDEI, 2019; <a href="https://www.gov.uk/government/publications/cdei-publishes-its-first-series-of-three-snapshot-papers-ethical-issues-in-ai/snapshot-paper-deepfakes-and-audiovisual-disinformation">Deepfakes and Audiovisual disinformation</a>, CDEI, 2019.&nbsp;<a href="#fnref:45" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:46">
      <p><a rel="external" href="https://www.turing.ac.uk/sites/default/files/2021-03/cahai_feasibility_study_primer_final.pdf">Artificial Intelligence, Human Rights, Democracy and the Rule of Law</a>, Leslie et al., 2021.&nbsp;<a href="#fnref:46" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:47">
      <p>Government has already committed to addressing some of these issues more broadly. See, for example, the <a href="https://www.gov.uk/government/publications/inclusive-britain-action-plan-government-response-to-the-commission-on-race-and-ethnic-disparities/inclusive-britain-government-response-to-the-commission-on-race-and-ethnic-disparities">Inclusive Britain report</a>, Race Disparity Unit, 2022.&nbsp;<a href="#fnref:47" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:48">
      <p><a rel="external" href="https://techcrunch.com/2023/01/11/chatgpt-cybersecurity-threat/?guccounter=1&amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8&amp;guce_referrer_sig=AQAAAFztF-r2eW_zAxjt0v_rnED11-KLH5D1tr27pMb-xJwpH0yky8pGrLvLjlRloW03L-Adh7fEsTlJUGR32p00S09VfBlalHUC0Xl1YbV5JTZqCzXlEkFVKkCa7J33Y-A5we3JMmCIyNmK9UeDWE_Hoiz5p6rUFWBdReh8twfUQBiy#:~:text=One%20hacker%20on%20a%20dark,script%20they%20had%20ever%20created.">‘Is ChatGPT a cybersecurity threat?</a>’ TechCrunch, 2023.&nbsp;<a href="#fnref:48" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:49">
      <p><a rel="external" href="https://research.checkpoint.com/2023/opwnai-cybercriminals-starting-to-use-chatgpt/">OPWNAI: Cybercriminals starting to use ChatGPT</a>, Check Point Research, 2023.&nbsp;<a href="#fnref:49" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:50">
      <p>These are not intended to be legal definitions for the purposes of the framework.&nbsp;<a href="#fnref:50" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:51">
      <p><a rel="external" href="https://www.adalovelaceinstitute.org/blog/value-chain-general-purpose-ai/">The value chain of general-purpose <abbr title="Artificial Intelligence">AI</abbr></a>, Ada Lovelace Institute, 2023.&nbsp;<a href="#fnref:51" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:52">
      <p><a rel="external" href="https://www.globalinnovationindex.org/gii-2022-report">Global Innovation Index 2022,</a> GII 2022; <a rel="external" href="https://rulemaking.worldbank.org/en/data/explorecountries/united-kingdom">Global Indicators of Regulatory Governance, World Bank, 2023</a>.&nbsp;<a href="#fnref:52" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:53">
      <p><a rel="external" href="https://www.oecd-ilibrary.org/science-and-technology/demand-for-ai-skills-in-jobs_3ed32d94-en">Demand for <abbr title="Artificial Intelligence">AI</abbr> skills in jobs</a>, <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> Science, Technology and Industry Working Papers, 2021.&nbsp;<a href="#fnref:53" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:54">
      <p>The protected characteristics are age, disability, gender reassignment, marriage and civil partnership, race, religion or belief, sex, and sexual orientation.&nbsp;<a href="#fnref:54" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:55">
      <p><a rel="external" href="https://www.legislation.gov.uk/eur/2016/679/article/5">Article 5(1)(a) Principles relating to processing of personal data</a>, HM Government, 2016.&nbsp;<a href="#fnref:55" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:56">
      <p><a rel="external" href="https://www.legislation.gov.uk/uksi/2016/1101/contents">Electrical Equipment (Safety) Regulations</a>, HM Government, 2016.&nbsp;<a href="#fnref:56" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:57">
      <p><a rel="external" href="https://www.legislation.gov.uk/uksi/2002/618/contents/made">Medical Devices Regulation</a>, HM Government, 2002.&nbsp;<a href="#fnref:57" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:58">
      <p><a rel="external" href="https://www.legislation.gov.uk/uksi/2011/1881/contents">Toys (Safety) Regulations</a>, HM Government, 2011.&nbsp;<a href="#fnref:58" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:59">
      <p><a rel="external" href="https://www.legislation.gov.uk/ukpga/2015/15/contents">Consumer Rights Act 2015</a>; <a rel="external" href="https://www.legislation.gov.uk/uksi/2008/1277/contents">Consumer Protection from Unfair Trading Regulations</a>, HM Government, 2008.&nbsp;<a href="#fnref:59" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:60">
      <p>Such as the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2000/8/contents">Financial Services and Markets Act</a>, HM Government, 2000.&nbsp;<a href="#fnref:60" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:61">
      <p><a href="https://www.gov.uk/government/publications/evidence-to-support-the-analysis-of-impacts-for-artificial-intelligence-governance">Evidence to support the analysis of impacts for <abbr title="Artificial Intelligence">AI</abbr> governance</a>, Frontier Economics, 2023.&nbsp;<a href="#fnref:61" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:62">
      <p>In 2019, 98.8% of businesses in the digital sector had less than 50 employees. <a href="https://www.gov.uk/government/collections/dcms-sectors-economic-estimates">DCMS Sectors Economic Estimates 2019: Business Demographics</a>, ONS, 2022.&nbsp;<a href="#fnref:62" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:63">
      <p>The <abbr title="Artificial Intelligence">AI</abbr> Sector Study found that almost 90% of businesses in the <abbr title="Artificial Intelligence">AI</abbr> sector are small or micro in size. <a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022"><abbr title="Artificial Intelligence">AI</abbr> Sector Study 2022</a>, DSIT, 2023&nbsp;<a href="#fnref:63" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:64">
      <p><a rel="external" href="https://www.digitalregulations.innovation.nhs.uk/about-this-service/"><abbr title="Artificial Intelligence">AI</abbr> and Digital Regulations Service</a>, Care Quality Commission, Health Research Authority, Medicines and Healthcare Products Regulatory Agency, National Institute for Health and Care Excellence, 2023.&nbsp;<a href="#fnref:64" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:65">
      <p><a rel="external" href="https://www.legislation.gov.uk/ukpga/2000/8/contents">Financial Services and Markets Act</a>, HM Government, 2000&nbsp;<a href="#fnref:65" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:66">
      <p><a href="https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme/software-and-ai-as-a-medical-device-change-programme-roadmap">Software and <abbr title="Artificial Intelligence">AI</abbr> as a Medical Device Change Programme – Roadmap</a>, <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr>, 2022.&nbsp;<a href="#fnref:66" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:67">
      <p>The exact relation between the concepts ‘interpretability’ and ‘explainability’ is the subject of ongoing academic debate. See <a rel="external" href="https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1493">Interpretable and explainable machine learning: A methods-centric overview with concrete examples</a>, Marcinkevics and Vogt, 2023. We use ‘explainability’ as the key term in our <abbr title="Artificial Intelligence">AI</abbr> principle in alignment with the <a rel="external" href="https://oecd.ai/en/ai-principles"><abbr title="Organisation for Economic Co-operation and Development">OECD</abbr></a>.&nbsp;<a href="#fnref:67" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:68">
      <p><a rel="external" href="https://www.nesta.org.uk/report/the-impact-of-regulation-on-innovation/">The impact of regulation on innovation</a>, Nesta, 2012.&nbsp;<a href="#fnref:68" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:69">
      <p><a href="https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance">Public expectations for <abbr title="Artificial Intelligence">AI</abbr> governance (transparency, fairness and accountability)</a>, Centre for Data Ethics and Innovation, 2023.&nbsp;<a href="#fnref:69" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:70">
      <p><a href="https://www.gov.uk/government/publications/national-ai-strategy">National <abbr title="Artificial Intelligence">AI</abbr> Strategy</a>, Office for Artificial Intelligence, 2021.&nbsp;<a href="#fnref:70" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:71">
      <p><a href="https://www.gov.uk/government/publications/digital-regulation-driving-growth-and-unlocking-innovation">Plan for Digital Regulation</a>, DSIT (formerly DCMS), 2022.&nbsp;<a href="#fnref:71" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:72">
      <p><a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>, Office for Artificial Intelligence, 2022.&nbsp;<a href="#fnref:72" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:73">
      <p><a rel="external" href="https://www.europarl.europa.eu/RegData/etudes/BRIE/2019/637967/EPRS_BRI(2019)637967_EN.pdf">Economic impacts of artificial intelligence</a>, European Parliament, 2019.&nbsp;<a href="#fnref:73" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:74">
      <p>The UK is ranked near the top of the Global <abbr title="Artificial Intelligence">AI</abbr> Index, third only to the US and China. <a rel="external" href="https://www.tortoisemedia.com/intelligence/global-ai/">Global <abbr title="Artificial Intelligence">AI</abbr> Index</a>, Tortoise Media, 2022.&nbsp;<a href="#fnref:74" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:75">
      <p><a rel="external" href="https://kpmg.com/de/en/home/insights/2021/06/artificial-intelligence-five-country-study.html">Trust in Artificial Intelligence: a five country study</a>, KPMG and the University of Queensland, 2021; <a href="https://www.gov.uk/government/publications/evidence-to-support-the-analysis-of-impacts-for-artificial-intelligence-governance">Evidence to support the analysis of impacts for <abbr title="Artificial Intelligence">AI</abbr> governance</a>, Frontier Economics, 2023.&nbsp;<a href="#fnref:75" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:76">
      <p>“Building on the UK’s strengths in the professional services and technology sectors, <abbr title="Artificial Intelligence">AI</abbr> assurance will also become a significant economic activity in its own right, with the potential for the UK to be a global leader in a new multi-billion pound industry.” See <a href="https://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-ecosystem">The roadmap to an effective <abbr title="Artificial Intelligence">AI</abbr> assurance ecosystem</a>, Centre for Data Ethics and Innovation, 2021.&nbsp;<a href="#fnref:76" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:77">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:77" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:78">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:78" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:79">
      <p>These characteristics are aligned with existing principles set out in the <a href="https://www.gov.uk/government/publications/digital-regulation-driving-growth-and-unlocking-innovation">Plan for Digital Regulation</a>, the <a href="https://www.gov.uk/government/publications/taskforce-on-innovation-growth-and-regulatory-reform-independent-report">report of the independent Taskforce on Innovation, Growth and Regulatory Reform</a> and with the findings of the <a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, published in March 2023, which called for a proportionate and agile regulatory approach and acknowledged the importance of achieving a “balance between providing clarity and building public trust, while also enabling development, experimentation, and deployment.”&nbsp;<a href="#fnref:79" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:80">
      <p><a rel="external" href="https://carnegieendowment.org/2022/10/06/one-of-biggest-problems-in-regulating-ai-is-agreeing-on-definition-pub-88100">One of the biggest problems in regulating <abbr title="Artificial Intelligence">AI</abbr> is agreeing on a definition</a>, Carnegie Endowment for International Peace, 2022.&nbsp;<a href="#fnref:80" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:81">
      <p><a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>, Office for Artificial Intelligence, 2022.&nbsp;<a href="#fnref:81" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:82">
      <p>As stated in government guidance on using <abbr title="Artificial Intelligence">AI</abbr> in the public sector, we consider machine learning to be a subset of <abbr title="Artificial Intelligence">AI</abbr>. While machine learning is the most widely-used form of <abbr title="Artificial Intelligence">AI</abbr> and will be captured within our framework, our adaptive and autonomous characteristics ensure any current or future <abbr title="Artificial Intelligence">AI</abbr> system that meets this criteria will be within scope. See <a href="https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-public-sector">A guide to using artificial intelligence in the public sector</a>, Government Digital Service and Office for Artificial Intelligence, 2019.&nbsp;<a href="#fnref:82" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:83">
      <p>See <a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>, Office for Artificial Intelligence, 2022. The context-based approach received wide support in feedback received following publication of this policy paper.&nbsp;<a href="#fnref:83" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:84">
      <p><a rel="external" href="https://smartwatermagazine.com/news/fido-tech/fido-direct-launched-end-end-solution-solve-water-loss">FIDO Direct launched as end-to-end solution to solve water loss</a>, Smart Water Magazine, 2023.&nbsp;<a href="#fnref:84" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:85">
      <p><a rel="external" href="https://www.axora.com/insights/how-ai-is-being-used-to-improve-health-and-safety-in-mining/"><abbr title="Artificial Intelligence">AI</abbr> on-side: how artificial intelligence is being used to improve health and safety in mining</a>, Axora, 2023.&nbsp;<a href="#fnref:85" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:86">
      <p><a href="https://www.gov.uk/government/publications/digital-regulation-driving-growth-and-unlocking-innovation">Plan for Digital Regulation</a>, DSIT (formerly DCMS), 2021.&nbsp;<a href="#fnref:86" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:87">
      <p><a href="https://www.gov.uk/government/publications/taskforce-on-innovation-growth-and-regulatory-reform-independent-report">The Taskforce on Innovation, Growth and Regulatory Reform independent report</a>, 10 Downing Street, 2021. The report argues for UK regulation that is: proportionate, forward-looking, outcome-focussed, collaborative, experimental, and responsive.&nbsp;<a href="#fnref:87" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:88">
      <p><a rel="external" href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1083582/closing-the-gap-regulation-full-report.pdf">Closing the gap: getting from principles to practices for innovation friendly regulation</a>, Regulatory Horizons Council, 2022.&nbsp;<a href="#fnref:88" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:89">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:89" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:90">
      <p><a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>, Office for Artificial Intelligence, 2022.&nbsp;<a href="#fnref:90" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:91">
      <p>The Centre for Data Ethics and Innovation (CDEI) has engaged with the public to understand their expectations for <abbr title="Artificial Intelligence">AI</abbr> governance. This engagement has informed our policy development. Participants also referred to a privacy principle, which is embedded in the broader regulatory considerations as regulators and <abbr title="Artificial Intelligence">AI</abbr> life cycle actors are expected to comply with the UK’s data protection framework. <a href="https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance">Public expectations for <abbr title="Artificial Intelligence">AI</abbr> governance (transparency, fairness and accountability)</a>, Centre for Data Ethics and Innovation, 2023.&nbsp;<a href="#fnref:91" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:92">
      <p><a rel="external" href="https://www.ncsc.gov.uk/collection/machine-learning">Principles for the security of machine learning</a>, National Cyber Security Centre, 2022&nbsp;<a href="#fnref:92" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:93">
      <p>For example, digital security can affect the safety of connected products such as automobiles and home appliances if risks are not appropriately managed. See <a rel="external" href="https://oecd.ai/en/dashboards/ai-principles/P8">Principle 1.4:Robustness, security and safety, <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> <abbr title="Artificial Intelligence">AI</abbr>, 2019</a>.&nbsp;<a href="#fnref:93" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:94">
      <p><a rel="external" href="https://standards.ieee.org/ieee/7001/6929/">Adapted from IEEE 7001-2021, Standard for Transparency of Autonomous Systems</a>.&nbsp;<a href="#fnref:94" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:95">
      <p>For example <a rel="external" href="https://standards.ieee.org/ieee/7001/6929/">IEEE 7001-2021</a> (Active Standard) describes measurable, testable levels of transparency so that autonomous systems can be objectively assessed, and levels of compliance determined; <a rel="external" href="https://www.iso.org/standard/82148.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TS6254</a> (Under development) will describe approaches and methods that can be used to achieve explainability objectives of stakeholders with regards to ML models and <abbr title="Artificial Intelligence">AI</abbr> system’s behaviours, outputs, and results.&nbsp;<a href="#fnref:95" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:96">
      <p><a href="https://www.gov.uk/government/publications/cdei-publishes-commissioned-research-on-algorithmic-transparency-in-the-public-sector">BritainThinks: Complete transparency, complete simplicity</a>, CDEI and CDDO, 2021.&nbsp;<a href="#fnref:96" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:97">
      <p><a rel="external" href="https://home.kpmg/au/en/home/insights/2021/03/artificial-intelligence-five-country-study.html">Trust in Artificial Intelligence: a five country study</a>, KPMG and the University of Queensland, 2021; <a href="https://www.gov.uk/government/publications/evidence-to-support-the-analysis-of-impacts-for-artificial-intelligence-governance">Evidence to support the analysis of impacts for <abbr title="Artificial Intelligence">AI</abbr> governance</a>, Frontier Economics, 2023.&nbsp;<a href="#fnref:97" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:98">
      <p><a rel="external" href="https://hai.stanford.edu/news/should-ai-models-be-explainable-depends">Should <abbr title="Artificial Intelligence">AI</abbr> models be explainable? That depends,</a> Stanford Institute for Human-Centered Artificial Intelligence, 2021.&nbsp;<a href="#fnref:98" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:99">
      <p>For example, <a rel="external" href="https://www.iso.org/standard/77607.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 24027:2021</a> describes measurement techniques and methods for assessing bias in <abbr title="Artificial Intelligence">AI</abbr> systems across their life cycle, especially in <abbr title="Artificial Intelligence">AI</abbr>-aided decision-making.&nbsp;<a href="#fnref:99" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:100">
      <p><a rel="external" href="https://bills.parliament.uk/bills/3430">The Data Protection and Digital Information (No. 2) Bill</a> reforms the UK’s data protection regime (<a rel="external" href="https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted">Data Protection Act 2018</a> and the <a rel="external" href="https://www.legislation.gov.uk/eur/2016/679/contents">UK <abbr title="UK General Data Protection Regulation">GDPR</abbr></a>).&nbsp;<a href="#fnref:100" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:101">
      <p>Guidance on vulnerability includes: <a rel="external" href="https://www.fca.org.uk/publication/guidance-consultation/gc19-03.pdf"><abbr title="Financial Conduct Authority">FCA</abbr> guidance on vulnerable consumers,</a> <abbr title="Financial Conduct Authority">FCA</abbr>, 2019; <a rel="external" href="https://www.ofgem.gov.uk/energy-policy-and-regulation/policy-and-regulatory-programmes/consumer-vulnerability-protections">Consumer vulnerability protections</a>, Ofgem, 2020; <a href="https://www.gov.uk/government/publications/vulnerable-consumers">Vulnerable consumers</a>, CMA, 2018.&nbsp;<a href="#fnref:101" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:102">
      <p><abbr title="Artificial Intelligence">AI</abbr> has the potential to learn to solve problems without human intervention instructing it to do so, or cope with situations the systems have not encountered before, producing potentially different associated risks that require clear lines of accountability and governance mechanisms to be in place. For example, see ​​<a rel="external" href="https://www.technologyreview.com/2021/05/27/1025453/artificial-intelligence-learning-create-itself-agi/"><abbr title="Artificial Intelligence">AI</abbr> is learning how to create itself</a>, MIT Technology Review, 2021.&nbsp;<a href="#fnref:102" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:103">
      <p>For example, <a rel="external" href="https://www.iso.org/standard/81230.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 42001</a> (Under development) will provide guidance for establishing, implementing and maintaining an <abbr title="Artificial Intelligence">AI</abbr> management system within an organisation to develop or use <abbr title="Artificial Intelligence">AI</abbr> systems responsibly. <a rel="external" href="https://www.iso.org/obp/ui/#iso:std:iso-iec:23894:ed-1:v1:en"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 23894</a> (Under development) will provide guidance for establishing <abbr title="Artificial Intelligence">AI</abbr> risk management principles and processes within an organisation.&nbsp;<a href="#fnref:103" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:104">
      <p>While this activity is likely to be led centrally (see <a href="#section331">section 3.3.1</a>), this will involve continuation of the existing collaboration across government to ensure alignment with (and appropriate leveraging of) existing work being undertaken in relation to the <a href="https://www.gov.uk/government/publications/uk-national-cyber-strategy-2022">National Cyber Strategy</a>, <a rel="external" href="https://safeandtrustedai.org/">UKRI work on Safe and Trusted <abbr title="Artificial Intelligence">AI</abbr></a>, the work of the <a href="https://www.gov.uk/government/organisations/centre-for-connected-and-autonomous-vehicles">Centre for Connected and Autonomous Vehicles</a>, the <a rel="external" href="https://transform.england.nhs.uk/ai-lab/"><abbr title="National Health Service">NHS</abbr> <abbr title="Artificial Intelligence">AI</abbr> Lab</a> and other examples.&nbsp;<a href="#fnref:104" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:105">
      <p><a href="https://www.gov.uk/government/publications/responsible-innovation-in-self-driving-vehicles">Responsible Innovation in Self-Driving Vehicles</a>, CDEI, 2022.&nbsp;<a href="#fnref:105" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:106">
      <p><a rel="external" href="https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/explaining-decisions-made-with-ai/">Explaining decisions made with <abbr title="Artificial Intelligence">AI</abbr></a>, <abbr title="Information Commissioner’s Office">ICO</abbr> and the Alan Turing Institute, 2021.&nbsp;<a href="#fnref:106" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:107">
      <p><a href="https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme/software-and-ai-as-a-medical-device-change-programme-roadmap#wp-10-project-glass-box-ai-interpretability">Software and <abbr title="Artificial Intelligence">AI</abbr> as a Medical Device Change Programme – Roadmap</a>, <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr>, 2022.&nbsp;<a href="#fnref:107" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:108">
      <p>Following publication of our policy paper in July 2022.&nbsp;<a href="#fnref:108" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:109">
      <p>Pro-innovation, proportionate, adaptable, trustworthy, clear and collaborative – see <a href="#paragraph37">paragraph 37</a> above&nbsp;<a href="#fnref:109" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:110">
      <p>For example, there are only 6 specific legal services activities that are overseen by regulators in the legal services sector. These ‘reserved legal activities’ are set out in the <a rel="external" href="https://www.legislation.gov.uk/ukpga/2007/29/contents">Legal Services Act</a>, HM Government, 2007 and can only be carried out by those who are authorised (or exempt). <abbr title="Artificial Intelligence">AI</abbr>-driven systems could offer other services like writing wills or contracts (which many might consider to be legal services) without being subject to oversight from legal services regulators&nbsp;<a href="#fnref:110" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:111">
      <p><a href="https://www.gov.uk/government/publications/regulators-code">Regulators’ Code</a>, Office for Product Safety and Standards, 2014.&nbsp;<a href="#fnref:111" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:112">
      <p><a rel="external" href="https://www.ucl.ac.uk/constitution-unit/explainers/what-uk-constitution">What is the UK Constitution?</a>, The Constitution Unit, University College London, 2023.&nbsp;<a href="#fnref:112" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:113">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation regulation of technologies review: digital technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:113" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:114">
      <p><a href="https://www.gov.uk/government/news/uk-on-the-cusp-of-a-transport-revolution-as-self-driving-vehicles-set-to-be-worth-nearly-42-billion-by-2035">UK on the cusp of a transport revolution</a>, Department for Transport, 2021.&nbsp;<a href="#fnref:114" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:115">
      <p><a rel="external" href="https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1099173/cam-2025-realising-benefits-self-driving-vehicles.pdf">Connected &amp; Automated Mobility 2025</a>, Department for Transport, 2022.&nbsp;<a href="#fnref:115" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:116">
      <p><a href="https://www.gov.uk/government/publications/regulators-code">Regulators’ Code</a>, Office for Product Safety and Standards, 2014.&nbsp;<a href="#fnref:116" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:117">
      <p><a rel="external" href="https://www.legislation.gov.uk/ukpga/1998/42/contents">Human Rights Act</a>, HM Government, 1998.&nbsp;<a href="#fnref:117" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:118">
      <p><a href="#box3-3">See Box 3.3</a>.&nbsp;<a href="#fnref:118" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:119">
      <p><a href="#box-32-supporting-coherence-in-risk-assessment">See Box 3.2</a>.&nbsp;<a href="#fnref:119" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:120">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:120" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:121">
      <p>The Centre for Data Ethics and Innovation (CDEI) Public attitudes report states that the public continue to have limited awareness of <abbr title="Artificial Intelligence">AI</abbr>, with knowledge mainly of low-risk use cases that are already in use but showing low familiarity with more complex <abbr title="Artificial Intelligence">AI</abbr> applications. <a href="https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance">Public expectations for <abbr title="Artificial Intelligence">AI</abbr> governance (transparency, fairness and accountability)</a>, Centre for Data Ethics and Innovation, 2023.&nbsp;<a href="#fnref:121" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:122">
      <p>For example, stakeholders have outlined proposals for governments’ roles in monitoring the wider <abbr title="Artificial Intelligence">AI</abbr> ecosystem as a means of addressing challenging policy issues. See <a rel="external" href="https://www.cser.ac.uk/resources/why-and-how-governments-should-monitor-ai-development/">Why and how governments should monitor <abbr title="Artificial Intelligence">AI</abbr> development</a>, Whittlestone and Clark, 2021.&nbsp;<a href="#fnref:122" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:123">
      <p><a rel="external" href="https://www.digitalregulations.innovation.nhs.uk/about-this-service/"><abbr title="Artificial Intelligence">AI</abbr> and Digital Regulations Service</a>, Care Quality Commission, Health Research Authority, Medicines and Healthcare Products Regulatory Agency, National Institute for Health and Care Excellence, 2023.&nbsp;<a href="#fnref:123" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:124">
      <p><a href="https://www.gov.uk/government/publications/projects-selected-for-the-regulators-pioneer-fund/projects-selected-for-the-regulators-pioneer-fund-2022#project-led-by-the-information-commissioners-office">Enabling innovation – piloting a multi-agency advice service for digital innovators</a>, Regulators’ Pioneer Fund, 2022 (an <abbr title="Information Commissioner’s Office">ICO</abbr>-led project).&nbsp;<a href="#fnref:124" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:125">
      <p><a rel="external" href="https://www.ucl.ac.uk/constitution-unit/explainers/what-uk-constitution">What is the UK constitution?</a> The Constitution Unit, University College London, 2023.&nbsp;<a href="#fnref:125" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:126">
      <p><a rel="external" href="https://arxiv.org/abs/2108.07258">On the opportunities and risks of foundation models</a>, Bommasani et al., 2022; <a rel="external" href="https://www.adalovelaceinstitute.org/report/regulating-ai-in-europe/">Expert opinion: Regulating <abbr title="Artificial Intelligence">AI</abbr> in Europe</a>, Edwards, 2022.&nbsp;<a href="#fnref:126" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:127">
      <p><a rel="external" href="https://dl.acm.org/doi/10.1145/3531146.3533088">Taxonomy of Risks posed by Language Models</a>, Weidinger et al., 2022.&nbsp;<a href="#fnref:127" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:128">
      <p><a rel="external" href="https://www.adalovelaceinstitute.org/blog/value-chain-general-purpose-ai/">The value chain of general-purpose <abbr title="Artificial Intelligence">AI</abbr></a>, Ada Lovelace Institute, 2023.&nbsp;<a href="#fnref:128" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:129">
      <p>See for example, <a rel="external" href="https://www.adalovelaceinstitute.org/blog/value-chain-general-purpose-ai/">The value chain of general-purpose <abbr title="Artificial Intelligence">AI</abbr></a>, Ada Lovelace Institute, 2023; <a rel="external" href="https://www.conjecture.dev/ai-alignment-overview">An overview of <abbr title="Artificial Intelligence">AI</abbr> alignment</a>, Conjecture, 2023; <a rel="external" href="https://www.anthropic.com/research">Make safe systems and deploy them reliably</a>, Anthropic, 2023.&nbsp;<a href="#fnref:129" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:130">
      <p><a href="https://www.gov.uk/government/speeches/foreign-secretary-statement-to-parliament-on-the-integrated-review-refresh-2023">Integrated Review Refresh 2023</a>, Prime Minister’s Office, 10 Downing Street, Foreign, Commonwealth and Development Office, Ministry of Defence 2023.&nbsp;<a href="#fnref:130" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:131">
      <p><a rel="external" href="https://arxiv.org/abs/2108.07258">On the opportunities and risks of foundation models</a>, Bommasani et al., 2022.&nbsp;<a href="#fnref:131" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:132">
      <p><a rel="external" href="https://arxiv.org/pdf/2112.02969.pdf">Jigsaw: Large Language Models meet Program Synthesis</a>, Jain et al., 2021.&nbsp;<a href="#fnref:132" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:133">
      <p><a rel="external" href="https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress">Huge ‘foundation models’ are turbo-charging <abbr title="Artificial Intelligence">AI</abbr> progress</a>, The Economist, 2022.&nbsp;<a href="#fnref:133" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:134">
      <p><a rel="external" href="https://openai.com/blog/gpt-3-apps/">GPT-3 Powers the Next Generation of Apps</a>, Open <abbr title="Artificial Intelligence">AI</abbr>, 2021.&nbsp;<a href="#fnref:134" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:135">
      <p><a rel="external" href="https://venturebeat.com/ai/large-language-models-broaden-ais-reach-in-industry-and-enterprises/">Large language models broaden <abbr title="Artificial Intelligence">AI</abbr>’s reach in industry and enterprise</a>, Venture Beat, 2022.&nbsp;<a href="#fnref:135" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:136">
      <p><a rel="external" href="https://time.com/6252404/mira-murati-chatgpt-openai-interview/">The Creator of ChatGPT Thinks <abbr title="Artificial Intelligence">AI</abbr> Should Be Regulated,</a> Time, 2023.&nbsp;<a href="#fnref:136" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:137">
      <p><a rel="external" href="https://openai.com/blog/chatgpt/">ChatGPT</a> by OpenAI; <a rel="external" href="https://www.newyorker.com/culture/cultural-comment/the-chatbot-problem">The Chatbot Problem</a>, The New Yorker, 2023.&nbsp;<a href="#fnref:137" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:138">
      <p>See <a rel="external" href="https://www.longtermresilience.org/post/future-of-compute-review-submission-of-evidence">Future of Compute Review: Submission of Evidence</a>, Centre for Long Term Resilience, 2022.&nbsp;<a href="#fnref:138" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:139">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">HM Government Response to Sir Patrick Vallance’s Pro-Innovation Regulation of Technologies Review</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:139" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:140">
      <p><a rel="external" href="https://ico.org.uk/for-organisations/regulatory-sandbox/">Regulatory Sandbox</a>, <abbr title="Information Commissioner’s Office">ICO</abbr>, 2022; <a rel="external" href="https://www.fca.org.uk/firms/innovation/regulatory-sandbox">Regulatory Sandbox</a>, <abbr title="Financial Conduct Authority">FCA</abbr>, 2022.&nbsp;<a href="#fnref:140" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:141">
      <p><a rel="external" href="https://www.fca.org.uk/data/innovation-market-insights">Innovation Hub: Market Insights</a>, <abbr title="Financial Conduct Authority">FCA</abbr>. 2023; <a rel="external" href="https://www.cambridge.org/core/journals/european-journal-of-risk-regulation/article/sandbox-%20approach-to-regulating-highrisk-artificial-intelligence-%20applications/C350EADFB379465E7F4A95B973A4977D">A Sandbox Approach to Regulating High-Risk Artificial Intelligence Applications</a>, Tuby et al, 2021.&nbsp;<a href="#fnref:141" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:142">
      <p><a rel="external" href="https://www.bis.org/publ/work901.htm">Inside the regulatory sandbox: effects on fintech funding</a>, Cornelli et al, 2020.&nbsp;<a href="#fnref:142" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:143">
      <p>For an existing example of this type of model see <a rel="external" href="https://www.fca.org.uk/firms/innovation/regulatory-sandbox">Regulatory Sandbox</a>, <abbr title="Financial Conduct Authority">FCA</abbr>, 2022.&nbsp;<a href="#fnref:143" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:144">
      <p>For an existing example of this type of model see <a rel="external" href="https://ico.org.uk/for-organisations/regulatory-sandbox/">Regulatory Sandbox</a>, <abbr title="Information Commissioner’s Office">ICO</abbr>, 2023&nbsp;<a href="#fnref:144" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:145">
      <p>For a report on a pilot of this type of model see: <a rel="external" href="https://www.cqc.org.uk/sites/default/files/20200324%20CQC%20sandbox%20report_machine%20learning%20in%20diagnostic%20services.pdf">Using machine learning in diagnostic services</a>, CQC, 2020.&nbsp;<a href="#fnref:145" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:146">
      <p><a href="https://www.gov.uk/government/publications/projects-selected-for-the-regulators-pioneer-fund/projects-selected-for-the-regulators-pioneer-fund-2022#project-led-by-the-information-commissioners-office">Enabling innovation – piloting a multi-agency advice service for digital innovators</a>, Regulators’ Pioneer Fund, 2022.&nbsp;<a href="#fnref:146" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:147">
      <p>The <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr>’s ‘airlock process’ is an example of this kind of service, designed for <abbr title="Artificial Intelligence">AI</abbr> products meeting certain criteria. See: <a href="https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme/software-and-ai-as-a-medical-device-change-programme-roadmap#wp2-02-secondary-legislation-and-process">Software and <abbr title="Artificial Intelligence">AI</abbr> as a medical device change programme</a>, <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr>, 2022.&nbsp;<a href="#fnref:147" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:148">
      <p>For an example, see: <a rel="external" href="https://innovation.nhs.uk/"><abbr title="National Health Service">NHS</abbr> Innovation Service</a>, Accelerated Access Collaborative, 2023. For <abbr title="Artificial Intelligence">AI</abbr> projects, see: <a rel="external" href="https://www.digitalregulations.innovation.nhs.uk/about-this-service/"><abbr title="Artificial Intelligence">AI</abbr> and Digital Regulations Service</a>, Care Quality Commission, Health Research Authority, Medicines and Healthcare Products Regulatory Agency, National Institute for Health and Care Excellence, 2023.&nbsp;<a href="#fnref:148" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:149">
      <p><a href="https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies">Pro-innovation Regulation of Technologies Review: Digital Technologies</a>, HM Treasury, 2023.&nbsp;<a href="#fnref:149" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:150">
      <p>Any attempt by a regulator to enforce a principle beyond its existing remit and powers may be legally challenged on the basis of going beyond its legal authority.&nbsp;<a href="#fnref:150" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:151">
      <p>Including but not limited to <a rel="external" href="https://www.turing.ac.uk/news/publications/common-regulatory-capacity-ai">Common Regulatory Capacity for <abbr title="Artificial Intelligence">AI</abbr></a>, The Alan Turing Institute, 2022.&nbsp;<a href="#fnref:151" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:152">
      <p>There is evidence that this is predominantly a recruitment problem. Regulators are trying to recruit but often cannot find the right candidates as they are competing for a limited supply of suitable candidates.&nbsp;<a href="#fnref:152" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:153">
      <p>Evidence showed that technical standards expertise varies across regulators. <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr> regularly uses and designates standards to clarify legal requirements, provide presumptive conformity and demonstrate the state of the art. Other regulators recognise their potential to support regulatory guidance but their thinking is nascent.&nbsp;<a href="#fnref:153" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:154">
      <p><a href="https://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-ecosystem">Roadmap to an effective <abbr title="Artificial Intelligence">AI</abbr> assurance ecosystem in the UK</a>, DSIT (formerly DCMS), 2021.&nbsp;<a href="#fnref:154" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:155">
      <p>The <abbr title="Artificial Intelligence">AI</abbr> Standards Hub is led by The Alan Turing Institute in partnership with the British Standards Institution (BSI) and the National Physical Laboratory (NPL) and supported by the UK Government.&nbsp;<a href="#fnref:155" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:156">
      <p>Technical standards are generally voluntary and developed through an industry-led process in global standards development organisations (SDOs), based on the principles of consensus, openness, and transparency, and benefiting from global technical expertise and best practice. In this paper, when referring to ‘technical standards’, we are referring to standards developed in standards development organisations.&nbsp;<a href="#fnref:156" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:157">
      <p><abbr title="Artificial Intelligence">AI</abbr>-specific standards addressing trustworthiness characteristics such as safety, transparency and robustness, amongst others, have been developed or are currently being developed (‘<em>’ indicates standards which are under development at the time of writing) in SDOs such as <abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> and IEEE (for example, <a rel="external" href="https://standards.ieee.org/ieee/7001/6929/">IEEE 7001</a>, <a rel="external" href="https://www.iso.org/standard/82148.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TS 6254</a></em>, <a rel="external" href="https://www.iso.org/standard/81283.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 5469</a><em>, <a rel="external" href="https://www.iso.org/standard/79804.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 24029-2</a></em>).&nbsp;<a href="#fnref:157" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:158">
      <p>Technical standards can be updated as good practices and the technology develop, allowing flexibility for requirements to adapt to technological change.&nbsp;<a href="#fnref:158" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:159">
      <p>Standards help organisations to manage and mitigate risks, as well as helping to unlock and scale the benefits of their products and services. In doing so, standards play a role in responsible innovation both as tools supporting good governance and as mechanisms for enabling and accelerating innovation.&nbsp;<a href="#fnref:159" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:160">
      <p>The UK government established a <a rel="external" href="https://www.npl.co.uk/news/unlocking-standards-for-4th-industrial-revolution">strategic coordination initiative</a> with the British Standards Institution (BSI) and the National Physical Laboratory (NPL) to step up UK’s engagement in the global development of standards.&nbsp;<a href="#fnref:160" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:161">
      <p>For example, these include <a rel="external" href="https://www.iso.org/standard/81230.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> DIS 42001</a>* , <a rel="external" href="https://www.iso.org/standard/77304.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> FDIS 23894</a>* and <a rel="external" href="https://www.iso.org/standard/80655.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> DIS 25059</a>.&nbsp;<a href="#fnref:161" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:162">
      <p>For example, transparency standards include <a rel="external" href="https://www.iso.org/standard/84111.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> AWI 12792</a><em>, <a rel="external" href="https://standards.ieee.org/ieee/7001/6929/">IEEE P7001-2021</a> and <a rel="external" href="https://www.iso.org/standard/82148.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> AWI TS 6254</a></em>. Bias mitigation standards include <a rel="external" href="https://www.iso.org/standard/77607.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> TR 24027:2021</a> and <a rel="external" href="https://www.iso.org/standard/84110.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> AWI TS 12791</a>*.&nbsp;<a href="#fnref:162" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:163">
      <p>For example, safety in healthcare can be addressed by the joint application of management system, risk management and quality standards along with horizontal thematic safety standards (such as, <a rel="external" href="https://www.iso.org/standard/81283.html"><abbr title="International Organisation for Standardisation">ISO</abbr> 5469</a><em>) and sector specific standards (such as, <a rel="external" href="https://standardsdevelopment.bsigroup.com/projects/2021-00605#/section">BS 30440</a></em>). Accordingly, regulators such as <abbr title="Medicines and Healthcare products Regulatory Agency">MHRA</abbr> might decide to reference sector-specific standards in their regulatory guidance as tools for <abbr title="Artificial Intelligence">AI</abbr> providers to demonstrate compliance with regulatory requirements for <abbr title="Artificial Intelligence">AI</abbr> as a medical device.&nbsp;<a href="#fnref:163" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:164">
      <p><a rel="external" href="https://www.tortoisemedia.com/intelligence/global-ai/">Global <abbr title="Artificial Intelligence">AI</abbr> Index</a>, Tortoise Media, 2022; <a rel="external" href="https://airankings.org/"><abbr title="Artificial Intelligence">AI</abbr> rankings by country</a>, <abbr title="Artificial Intelligence">AI</abbr> Rankings, 2023.&nbsp;<a href="#fnref:164" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:165">
      <p><a rel="external" href="https://oecd.ai/en/network-of-experts"><abbr title="Organisation for Economic Co-operation and Development">OECD</abbr> Working Party and Network of Experts on Artificial Intelligence Governance (AIGO)</a>, <abbr title="Organisation for Economic Co-operation and Development">OECD</abbr>, 2023.&nbsp;<a href="#fnref:165" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:166">
      <p><a rel="external" href="https://gpai.ai/">Global Partnership on Artificial Intelligence</a>, <abbr title="Global Partnership on AI">GPAI</abbr>, 2023.&nbsp;<a href="#fnref:166" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:167">
      <p><a rel="external" href="https://www.gpai.ai/projects/climate-change-and-ai.pdf">Climate Change and <abbr title="Artificial Intelligence">AI</abbr>: Recommendations for Government Action, Global Partnership on <abbr title="Artificial Intelligence">AI</abbr></a> <abbr title="Global Partnership on AI">GPAI</abbr>, Climate Change <abbr title="Artificial Intelligence">AI</abbr> and the Centre for <abbr title="Artificial Intelligence">AI</abbr> &amp; Climate, 2021.&nbsp;<a href="#fnref:167" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:168">
      <p><a rel="external" href="https://www.coe.int/en/web/artificial-intelligence/cai">Committee on Artificial Intelligence (<abbr title="Council of Europe Committee on AI">CAI</abbr>)</a>, Council of Europe, 2023.&nbsp;<a href="#fnref:168" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:169">
      <p><a rel="external" href="https://en.unesco.org/artificial-intelligence">Artificial Intelligence</a>, UNESCO, 2023.&nbsp;<a href="#fnref:169" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:170">
      <p><a rel="external" href="https://aistandardshub.org/"><abbr title="Artificial Intelligence">AI</abbr> Standards Hub</a>, 2023&nbsp;<a href="#fnref:170" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:171">
      <p><a rel="external" href="https://www.iso.org/home.html">International Organisation for Standardisation and International Electrotechnical Commission</a>, <abbr title="International Organisation for Standardisation">ISO</abbr>, 2023.&nbsp;<a href="#fnref:171" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:172">
      <p>The <abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> work programme, which the UK is contributing to alongside our partners, includes the development of an <a rel="external" href="https://www.iso.org/management-system-standards-list.html"><abbr title="Artificial Intelligence">AI</abbr> Management System Standard</a> (MSS), which intends to help solve some of the implementation challenges relating to <abbr title="Artificial Intelligence">AI</abbr> governance. This standard will be known as <a rel="external" href="https://www.iso.org/standard/81230.html"><abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr> 42001</a> and will help organisations develop or use artificial intelligence responsibly when pursuing their objectives, and fulfil their obligations to interested parties. Additionally, through BSI, the UK is leading the development of <abbr title="Artificial Intelligence">AI</abbr> international standards in concepts and terminology at <abbr title="International Organisation for Standardisation">ISO</abbr>/<abbr title="International Electrotechnical Commission">IEC</abbr>, including those on data, bias, governance implications, and data life cycles. At the European Telecommunications Standards Institute (ETSI) we have led the creation of documents including the <a rel="external" href="https://www.etsi.org/deliver/etsi_gr/SAI/001_099/002/01.01.01_60/gr_SAI002v010101p.pdf">ETSI GR SAI 002 on Data Supply Chain Security</a>, out of the UK’s National Cyber Security Centre.&nbsp;<a href="#fnref:172" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:173">
      <p><a rel="external" href="https://www.bsigroup.com/en-GB/">British Standards Institution</a>, 2023.&nbsp;<a href="#fnref:173" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:174">
      <p><a rel="external" href="https://ethicsstandards.org/">The Open Community for Ethics in Autonomous and Intelligent Systems (<abbr title="Open Community for Ethics in Autonomous and Intelligent Systems">OCEANIS</abbr>),</a> 2023.&nbsp;<a href="#fnref:174" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:175">
      <p>The <abbr title="Artificial Intelligence">AI</abbr> sector is estimated to contribute £3.7 billion in GVA (Gross Value Added) to the UK economy. <a href="https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022"><abbr title="Artificial Intelligence">AI</abbr> Sector Study 2022</a>, DSIT, 2023.&nbsp;<a href="#fnref:175" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:176">
      <p><a rel="external" href="https://www.ucl.ac.uk/constitution-unit/explainers/what-uk-constitution">What is the UK Constitution?</a>, The Constitution Unit, Uiniversity College London, 2023.&nbsp;<a href="#fnref:176" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:177">
      <p><a rel="external" href="https://www.ncsc.gov.uk/collection/machine-learning">Principles for the security of machine learning</a>, National Cyber Security Centre, 2022.&nbsp;<a href="#fnref:177" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:178">
      <p>Technical standards marked with ‘*’ are under development.&nbsp;<a href="#fnref:178" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
    <li id="fn:179">
      <p><a href="https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement">Establishing a pro-innovation approach to regulating <abbr title="Artificial Intelligence">AI</abbr></a>, Office for Artificial Intelligence, 2022.&nbsp;<a href="#fnref:179" class="reversefootnote" role="doc-backlink" aria-label="go to where this is referenced">↩</a></p>
    </li>
  </ol>
</div>
</div>


</div>
</div>
    </div>
  </div>

  <div class="govuk-grid-row">
    
<a class="govuk-link gem-c-back-to-top-link govuk-!-display-none-print" href="#contents">
    <svg class="gem-c-back-to-top-link__icon" xmlns="http://www.w3.org/2000/svg" width="13" height="17" viewBox="0 0 13 17" aria-hidden="true" focusable="false">
      <path fill="currentColor" d="M6.5 0L0 6.5 1.4 8l4-4v12.7h2V4l4.3 4L13 6.4z"></path>
    </svg>
    Back to top
</a>
  </div>
</div>


    </main>
  </div>

      <div class="govuk-width-container">
        
<div data-module="feedback ga4-event-tracker" class="gem-c-feedback govuk-!-display-none-print" data-feedback-module-started="true" data-ga4-event-tracker-module-started="true">
  
<div class="gem-c-feedback__prompt gem-c-feedback__js-show js-prompt" tabindex="-1">
  <div class="gem-c-feedback__prompt-content">
    <div class="gem-c-feedback__prompt-questions js-prompt-questions">
      <div class="gem-c-feedback__prompt-question-answer">
        <h2 class="gem-c-feedback__prompt-question">Is this page useful?</h2>
        <ul class="gem-c-feedback__option-list">
          <li class="gem-c-feedback__option-list-item govuk-visually-hidden" hidden="">
            <a class="gem-c-feedback__prompt-link" role="button" hidden="hidden" aria-hidden="true" href="/contact/govuk">
              Maybe
</a>          </li>
          <li class="gem-c-feedback__option-list-item">
            <button class="govuk-button gem-c-feedback__prompt-link js-page-is-useful" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;Yes&quot;,&quot;section&quot;:&quot;Is this page useful?&quot;,&quot;tool_name&quot;:&quot;Is this page useful?&quot;}">
              Yes <span class="govuk-visually-hidden">this page is useful</span>
</button>          </li>
          <li class="gem-c-feedback__option-list-item">

            <button class="govuk-button gem-c-feedback__prompt-link js-toggle-form js-page-is-not-useful" aria-controls="page-is-not-useful" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;No&quot;,&quot;section&quot;:&quot;Is this page useful?&quot;,&quot;tool_name&quot;:&quot;Is this page useful?&quot;}">
              No <span class="govuk-visually-hidden">this page is not useful</span>
</button>          </li>
        </ul>
      </div>
    </div>

    <div class="gem-c-feedback__prompt-questions gem-c-feedback__prompt-success js-prompt-success" role="alert" hidden="">
      Thank you for your feedback
    </div>

    <div class="gem-c-feedback__prompt-questions gem-c-feedback__prompt-questions--something-is-wrong js-prompt-questions">
      <button class="govuk-button gem-c-feedback__prompt-link js-toggle-form js-something-is-wrong" aria-controls="something-is-wrong" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;Report a problem with this page&quot;,&quot;section&quot;:&quot;Is this page useful?&quot;,&quot;tool_name&quot;:&quot;Is this page useful?&quot;}">
        Report a problem with this page
</button>    </div>
  </div>
</div>

  <form action="/contact/govuk/problem_reports" id="something-is-wrong" class="gem-c-feedback__form js-feedback-form" method="post" hidden="">

  <div class="govuk-grid-row">
    <div class="govuk-grid-column-two-thirds">
      <div class="gem-c-feedback__error-summary gem-c-feedback__js-show js-errors" tabindex="-1" hidden=""></div>

      <input type="hidden" name="url" value="https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper">

      <h2 class="gem-c-feedback__form-heading">Help us improve GOV.UK</h2>
      <p id="feedback_explanation" class="gem-c-feedback__form-paragraph">Don’t include personal or financial information like your National Insurance number or credit card details.</p>

      <div class="govuk-visually-hidden" aria-hidden="true">
        <label for="giraffe">This field is for robots only. Please leave blank</label>
        <input id="giraffe" name="giraffe" type="text" pattern=".{0}" tabindex="-1" autocomplete="off">
      </div>

      <div class="gem-c-textarea govuk-form-group govuk-!-margin-bottom-6">
    
  <label for="textarea-17b73ab1" class="gem-c-label govuk-label">What were you doing?</label>





  <textarea name="what_doing" class="govuk-textarea" id="textarea-17b73ab1" rows="3" spellcheck="true" aria-describedby="feedback_explanation"></textarea>
     
</div>

      <div class="gem-c-textarea govuk-form-group govuk-!-margin-bottom-6">
    
  <label for="textarea-edd9d867" class="gem-c-label govuk-label">What went wrong?</label>





  <textarea name="what_wrong" class="govuk-textarea" id="textarea-edd9d867" rows="3" spellcheck="true"></textarea>
     
</div>


      

  <button class="gem-c-button govuk-button" type="submit" data-ga4-event="{&quot;event_name&quot;:&quot;form_submit&quot;,&quot;type&quot;:&quot;feedback&quot;,&quot;text&quot;:&quot;Send&quot;,&quot;section&quot;:&quot;Help us improve GOV.UK&quot;,&quot;tool_name&quot;:&quot;Help us improve GOV.UK&quot;}">Send</button>



      <button class="govuk-button govuk-button--secondary gem-c-feedback__close gem-c-feedback__js-show js-close-form" aria-controls="something-is-wrong">
        Cancel
      </button>
    </div>
  </div>
<input type="hidden" name="javascript_enabled" value="true"><input type="hidden" name="referrer" value="unknown"><input type="hidden" name="timer" value="0"></form>


 <script nonce="">
//<![CDATA[
  document.addEventListener("DOMContentLoaded", function () {
    var input = document.querySelector("#giraffe"),
      form = document.querySelector("#something-is-wrong")

    form.addEventListener("submit", spamCapture);

    function spamCapture(e) {
      if (input.value.length !== 0) return;
      e.preventDefault();
    }
  });

//]]>
</script>
  <div id="page-is-not-useful" class="gem-c-feedback__form gem-c-feedback__form--email gem-c-feedback__js-show js-feedback-form" hidden="">
  <div class="govuk-grid-row">
    <div class="govuk-grid-column-two-thirds" id="survey-wrapper">
      <div class="gem-c-feedback__error-summary js-errors" tabindex="-1" hidden=""></div>

      <h2 class="gem-c-feedback__form-heading">Help us improve GOV.UK</h2>
      <p id="survey_explanation" class="gem-c-feedback__form-paragraph">
        To help us improve GOV.UK, we’d like to know more about your visit today.
        <a href="https://www.smartsurvey.co.uk/s/gov-uk-banner/?c=/government/publications/ai-regulation-a-pro-innovation-approach/white-paper" class="govuk-link" target="_blank" rel="noopener noreferrer external">Please fill in this survey (opens in a new tab<noscript> and requires JavaScript</noscript>)</a>.
      </p>
      <button class="govuk-button govuk-button--secondary js-close-form" aria-controls="page-is-not-useful">
        Cancel
      </button>
    </div>
  </div>
</div>

</div>
      </div>

      <footer data-module="ga4-link-tracker" class="gem-c-layout-footer govuk-footer" data-ga4-link-tracker-module-started="true">
  <div class="govuk-width-container">
    <svg xmlns="http://www.w3.org/2000/svg" focusable="false" role="presentation" viewBox="0 0 64 60" height="30" width="32" fill="currentcolor" class="govuk-footer__crown">
      <g>
        <circle cx="20" cy="17.6" r="3.7"></circle>
        <circle cx="10.2" cy="23.5" r="3.7"></circle>
        <circle cx="3.7" cy="33.2" r="3.7"></circle>
        <circle cx="31.7" cy="30.6" r="3.7"></circle>
        <circle cx="43.3" cy="17.6" r="3.7"></circle>
        <circle cx="53.2" cy="23.5" r="3.7"></circle>
        <circle cx="59.7" cy="33.2" r="3.7"></circle>
        <circle cx="31.7" cy="30.6" r="3.7"></circle>
        <path d="M33.1,9.8c.2-.1.3-.3.5-.5l4.6,2.4v-6.8l-4.6,1.5c-.1-.2-.3-.3-.5-.5l1.9-5.9h-6.7l1.9,5.9c-.2.1-.3.3-.5.5l-4.6-1.5v6.8l4.6-2.4c.1.2.3.3.5.5l-2.6,8c-.9,2.8,1.2,5.7,4.1,5.7h0c3,0,5.1-2.9,4.1-5.7l-2.6-8ZM37,37.9s-3.4,3.8-4.1,6.1c2.2,0,4.2-.5,6.4-2.8l-.7,8.5c-2-2.8-4.4-4.1-5.7-3.8.1,3.1.5,6.7,5.8,7.2,3.7.3,6.7-1.5,7-3.8.4-2.6-2-4.3-3.7-1.6-1.4-4.5,2.4-6.1,4.9-3.2-1.9-4.5-1.8-7.7,2.4-10.9,3,4,2.6,7.3-1.2,11.1,2.4-1.3,6.2,0,4,4.6-1.2-2.8-3.7-2.2-4.2.2-.3,1.7.7,3.7,3,4.2,1.9.3,4.7-.9,7-5.9-1.3,0-2.4.7-3.9,1.7l2.4-8c.6,2.3,1.4,3.7,2.2,4.5.6-1.6.5-2.8,0-5.3l5,1.8c-2.6,3.6-5.2,8.7-7.3,17.5-7.4-1.1-15.7-1.7-24.5-1.7h0c-8.8,0-17.1.6-24.5,1.7-2.1-8.9-4.7-13.9-7.3-17.5l5-1.8c-.5,2.5-.6,3.7,0,5.3.8-.8,1.6-2.3,2.2-4.5l2.4,8c-1.5-1-2.6-1.7-3.9-1.7,2.3,5,5.2,6.2,7,5.9,2.3-.4,3.3-2.4,3-4.2-.5-2.4-3-3.1-4.2-.2-2.2-4.6,1.6-6,4-4.6-3.7-3.7-4.2-7.1-1.2-11.1,4.2,3.2,4.3,6.4,2.4,10.9,2.5-2.8,6.3-1.3,4.9,3.2-1.8-2.7-4.1-1-3.7,1.6.3,2.3,3.3,4.1,7,3.8,5.4-.5,5.7-4.2,5.8-7.2-1.3-.2-3.7,1-5.7,3.8l-.7-8.5c2.2,2.3,4.2,2.7,6.4,2.8-.7-2.3-4.1-6.1-4.1-6.1h10.6,0Z"></path>
      </g>
    </svg>
      <div class="govuk-footer__navigation">
            <div class="govuk-grid-column-two-thirds govuk-!-display-none-print">
              <h2 class="govuk-footer__heading govuk-heading-m">Services and information</h2>
                <ul class="govuk-footer__list govuk-footer__list--columns-2">
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/benefits">Benefits</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/births-deaths-marriages">Births, death, marriages and care</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/business">Business and self-employed</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/childcare-parenting">Childcare and parenting</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/citizenship">Citizenship and living in the UK</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;6&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/justice">Crime, justice and the law</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;7&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/disabilities">Disabled people</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;8&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/driving">Driving and transport</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;9&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/education">Education and learning</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;10&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/employing-people">Employing people</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;11&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/environment-countryside">Environment and countryside</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;12&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/housing-local-services">Housing and local services</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;13&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/tax">Money and tax</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;14&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/abroad">Passports, travel and living abroad</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;15&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/visas-immigration">Visas and immigration</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;16&quot;,&quot;index_section&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;16&quot;,&quot;section&quot;:&quot;Services and information&quot;}" href="/browse/working">Working, jobs and pensions</a>
                      </li>
                </ul>
            </div>
            <div class="govuk-grid-column-one-third govuk-!-display-none-print">
              <h2 class="govuk-footer__heading govuk-heading-m">Government activity</h2>
                <ul class="govuk-footer__list govuk-footer__list--columns-1">
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/organisations">Departments</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/news-and-communications">News</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/guidance-and-regulation">Guidance and regulation</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/research-and-statistics">Research and statistics</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/policy-papers-and-consultations">Policy papers and consultations</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;6&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/search/transparency-and-freedom-of-information-releases">Transparency</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;7&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/how-government-works">How government works</a>
                      </li>
                      <li class="govuk-footer__list-item">
                        <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;8&quot;,&quot;index_section&quot;:&quot;2&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Government activity&quot;}" href="/government/get-involved">Get involved</a>
                      </li>
                </ul>
            </div>
      </div>

      <hr class="govuk-footer__section-break govuk-!-display-none-print">
    <div class="govuk-footer__meta">
      <div class="govuk-footer__meta-item govuk-footer__meta-item--grow">
          <h2 class="govuk-visually-hidden">Support links</h2>
          <ul class="govuk-footer__inline-list govuk-!-display-none-print">
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help">Help</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;2&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/privacy-notice">Privacy</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;3&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/cookies">Cookies</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;4&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/accessibility-statement">Accessibility statement</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;5&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/contact">Contact</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;6&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/help/terms-conditions">Terms and conditions</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" lang="cy" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;7&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/cymraeg">Rhestr o Wasanaethau Cymraeg</a>
              </li>
              <li class="govuk-footer__inline-list-item">
                <a class="govuk-footer__link" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;type&quot;:&quot;footer&quot;,&quot;index_link&quot;:&quot;8&quot;,&quot;index_section&quot;:&quot;3&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;index_total&quot;:&quot;8&quot;,&quot;section&quot;:&quot;Support links&quot;}" href="/government/organisations/government-digital-service">Government Digital Service</a>
              </li>
          </ul>
          <svg aria-hidden="true" focusable="false" class="govuk-footer__licence-logo" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 483.2 195.7" height="17" width="41">
            <path fill="currentColor" d="M421.5 142.8V.1l-50.7 32.3v161.1h112.4v-50.7zm-122.3-9.6A47.12 47.12 0 0 1 221 97.8c0-26 21.1-47.1 47.1-47.1 16.7 0 31.4 8.7 39.7 21.8l42.7-27.2A97.63 97.63 0 0 0 268.1 0c-36.5 0-68.3 20.1-85.1 49.7A98 98 0 0 0 97.8 0C43.9 0 0 43.9 0 97.8s43.9 97.8 97.8 97.8c36.5 0 68.3-20.1 85.1-49.7a97.76 97.76 0 0 0 149.6 25.4l19.4 22.2h3v-87.8h-80l24.3 27.5zM97.8 145c-26 0-47.1-21.1-47.1-47.1s21.1-47.1 47.1-47.1 47.2 21 47.2 47S123.8 145 97.8 145"></path>
          </svg>
          <span class="govuk-footer__licence-description" data-ga4-track-links-only="" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;section&quot;:&quot;Licence&quot;,&quot;index_section&quot;:&quot;4&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;text&quot;:&quot;Open Government Licence v3.0&quot;,&quot;index_total&quot;:&quot;1&quot;,&quot;type&quot;:&quot;footer&quot;}">
            All content is available under the <a class="govuk-footer__link" href="https://www.nationalarchives.gov.uk/doc/open-government-licence/version/3/" rel="license">Open Government Licence v3.0</a>, except where otherwise stated
          </span>
      </div>
      <div class="govuk-footer__meta-item" data-ga4-link="{&quot;event_name&quot;:&quot;navigation&quot;,&quot;section&quot;:&quot;Copyright&quot;,&quot;index_section&quot;:&quot;5&quot;,&quot;index_link&quot;:&quot;1&quot;,&quot;index_section_count&quot;:&quot;5&quot;,&quot;text&quot;:&quot;© Crown copyright&quot;,&quot;index_total&quot;:&quot;1&quot;,&quot;type&quot;:&quot;footer&quot;}">
        <a class="govuk-footer__link govuk-footer__copyright-logo" href="https://www.nationalarchives.gov.uk/information-management/re-using-public-sector-information/uk-government-licensing-framework/crown-copyright/">© Crown copyright</a>
      </div>
    </div>
  </div>
</footer>
    <script src="/assets/static/application-797926e5a7d2db83c4d02a72bd4d31233750842fb58a04eeb1fdd2aab63d21b1.js" type="module"></script>
<script src="/assets/government-frontend/application-7c9db67f200545f0155ed150d5333b474479b0b3eb482c430af7f62aa560e563.js" type="module"></script><script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "FAQPage",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper"
  },
  "name": "A pro-innovation approach to AI regulation",
  "datePublished": "2023-03-29T11:12:11+01:00",
  "dateModified": "2023-08-03T10:29:45+01:00",
  "text": null,
  "publisher": {
    "@type": "Organization",
    "name": "GOV.UK",
    "url": "https://www.gov.uk",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-logo-b8553f688131fad665e52a8c2df7633f9cd1c0fffb9f69703cc68c728e7b3b74.png"
    }
  },
  "image": [
    "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-schema-placeholder-1x1-2672c0fb7a5d5f947d880522c509ebe7f2be090885883cc94418f6860e812e15.png",
    "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-schema-placeholder-4x3-194fde4197f00e669f6f52c182df2ed707bfb2024c9ef39f7a2ed20da62b90eb.png",
    "https://www.gov.uk/assets/government-frontend/govuk_publishing_components/govuk-schema-placeholder-16x9-30e6c0e035636ee6b9dc72ae254bcd4a925182805afe7c5b7170cf2394894b28.png"
  ],
  "author": {
    "@type": "Organization",
    "name": "Department for Science, Innovation and Technology",
    "url": "https://www.gov.uk/government/organisations/department-for-science-innovation-and-technology"
  },
  "mainEntity": [
    {
      "@type": "Question",
      "name": "Correction slip",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#correction-slip",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#correction-slip",
        "text": "\u003cp\u003eCorrection:\u003c/p\u003e\u003cp\u003eText currently reads \u003cstrong\u003e\u003ca href=\"#annexc\"\u003ein Annex C\u003c/a\u003e\u003c/strong\u003e:\u003c/p\u003e\u003cp\u003e1. Do you agree that requiring organisations to make it clear when they are using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would adequately ensure transparency?\u003c/p\u003e\u003cp\u003e2. What other transparency measures would be appropriate, if any?\u003c/p\u003e\u003cp\u003e3. Do you agree that current routes to contestability or redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms are adequate?\u003c/p\u003e\u003cp\u003e4. How could routes to contestability or redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms be improved, if at all?\u003c/p\u003e\u003cp\u003e[…]\u003c/p\u003e\u003cp\u003eL3. If you are a business that develops, uses, or sells \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, how do you currently manage \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk including through the wider supply chain? How could government support effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risk management?\u003c/p\u003e\u003cp\u003e[…]\u003c/p\u003e\u003cp\u003eS1. Which of the sandbox models described in \u003ca href=\"https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#section334\"\u003esection 3.3.4\u003c/a\u003e would be most likely to support innovation?\u003c/p\u003e\u003cp\u003eText should read:\u003c/p\u003e\u003cp\u003e1: Do you agree that requiring organisations to make it clear when they are using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would improve transparency?\u003c/p\u003e\u003cp\u003e2: Are there other measures we could require of organisations to improve transparency for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e?\u003c/p\u003e\u003cp\u003e3: Do you agree that current routes to contest or get redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms are adequate?\u003c/p\u003e\u003cp\u003e4: How could current routes to contest or seek redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms be improved, if at all?\u003c/p\u003e\u003cp\u003e[…]\u003c/p\u003e\u003cp\u003eL3: If you work for a business that develops, uses, or sells \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, how do you currently manage \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk including through the wider supply chain? How could government support effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risk management?\u003c/p\u003e\u003cp\u003e[…]\u003c/p\u003e\u003cp\u003eS1: To what extent would the sandbox models described in section 3.3.4 support innovation?\u003c/p\u003e\u003cp\u003eDate of correction: 4 July 2023\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Ministerial foreword",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#ministerial-foreword",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#ministerial-foreword",
        "text": "\u003cfigure class=\"image embedded\"\u003e\u003cdiv class=\"img\"\u003e\u003cimg src=\"https://assets.publishing.service.gov.uk/media/6420d13632a8e0000cfa94cc/s960_Minister_Michelle_Donelan_UK_Parliament_960px_640px.jpg\" alt=\"\"\u003e\u003c/div\u003e\n\u003cfigcaption\u003e\u003cp\u003eThe Rt Hon Michelle Donelan MP, Secretary of State for Science, Innovation and Technology\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cp\u003eI believe that a common-sense, outcomes-oriented approach is the best way to get right to the heart of delivering on the priorities of people across the UK. Better public services, high quality jobs and opportunities to learn the skills that will power our future – these are the priorities that will drive our goal to become a science and technology superpower by 2030.\u003c/p\u003e\u003cp\u003eArtificial Intelligence (\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e) will play a central part in delivering and enabling these goals, and this white paper will ensure we are putting the UK on course to be the best place in the world to build, test and use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technology. But we are not starting from zero. Having invested over £2.5 billion in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e since 2014, this paper builds on our recent announcements of £110 million for our \u003ca rel=\"external\" href=\"https://www.ukri.org/news/250m-to-secure-the-uks-world-leading-position-in-technologies-of-tomorrow/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Tech Missions Fund\u003c/a\u003e, £900 million to establish a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Research Resource and to develop an exascale supercomputer capable of running large \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e models – backed up by our new £8 million \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Global Talent Network and £117 million of existing funding to create hundreds of new \u003cabbr title=\"Doctorate of Philosophy\"\u003ePhDs\u003c/abbr\u003e for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e researchers.\u003c/p\u003e\u003cp\u003eMost of us are only now beginning to understand the transformative potential of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as the technology rapidly improves. But in many ways, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is already delivering fantastic social and economic benefits for real people – from improving \u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e medical care to making transport safer. Recent advances in things like generative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e give us a glimpse into the enormous opportunities that await us in the near future if we are prepared to lead the world in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sector with our values of transparency, accountability and innovation.\u003c/p\u003e\u003cp\u003eMy vision for an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-enabled country is one where our \u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e heroes are able to save lives using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies that were unimaginable just a few decades ago. I want our police, transport networks and climate scientists and many more to be empowered by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies that will make the UK the smartest, healthiest, safest and happiest place to live and work. That is why \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is one of this government’s 5 technologies of tomorrow – bringing stronger growth, better jobs, and bold new discoveries. It is a vision that has been shaped by stakeholders and experts in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, whose expertise and ideas I am determined to see reflected in our department.\u003c/p\u003e\u003cp\u003eThe UK has been at the forefront of this progress, placing third in the world for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e research and development. We are home to a third of Europe’s total \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies and twice as many as any other European country. Our world-leading status is down to our thriving research base and the pipeline of expertise graduating through our universities, the ingenuity of our innovators and the government’s long-term commitment to invest in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003eTo ensure we become an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e superpower, though, it is crucial that we do all we can to create the right environment to harness the benefits of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and remain at the forefront of technological developments. That includes getting regulation right so that innovators can thrive and the risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can be addressed.\u003c/p\u003e\u003cp\u003eThese risks could include anything from physical harm, an undermining of national security, as well as risks to mental health. The development and deployment of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can also present ethical challenges which do not always have clear answers. Unless we act, household consumers, public services and businesses will not trust the technology and will be nervous about adopting it. Unless we build public trust, we will miss out on many of the benefits on offer.\u003c/p\u003e\u003cp\u003eIndeed, the pace of change itself can be unsettling. Some fear a future in which \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e replaces or displaces jobs, for example. Our white paper and our vision for a future \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-enabled country is one in which our ways of working are complemented by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e rather than disrupted by it. In the modern world, too much of our professional lives are taken up by monotonous tasks – inputting data, filling out paperwork, scanning through documents for one piece of information and so on. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in the workplace has the potential to free us up from these tasks, allowing us to spend more time doing the things we trained for – teachers with more time to teach, clinicians with more time to spend with patients, police officers with more time on the beat rather than behind a desk – the list goes on.\u003c/p\u003e\u003cp\u003eIndeed, since \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is already in our day-to-day lives, there are numerous examples that can help to illustrate the real, tangible benefits that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can bring once any risks are mitigated. Streaming services already use advanced \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to recommend TV shows and films to us. Our \u003cabbr title=\"satellite navigation\"\u003esatnav\u003c/abbr\u003e uses \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to plot the fastest routes for our journeys, or helps us avoid traffic by intelligently predicting where congestion will be on our journey. And of course, almost all of us carry a smartphone in our pockets that uses advanced \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in all sorts of ways. These common devices all carried risks at one time or another, but today they benefit us enormously.\u003c/p\u003e\u003cp\u003eThat is why our white paper details how we intend to support innovation while providing a framework to ensure risks are identified and addressed. However, a heavy-handed and rigid approach can stifle innovation and slow \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption. That is why we set out a proportionate and pro-innovation regulatory framework. Rather than target specific technologies, it focuses on the context in which \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is deployed. This enables us to take a balanced approach to weighing up the benefits versus the potential risks.\u003c/p\u003e\u003cp\u003eWe recognise that particular \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies, foundation models for example, can be applied in many different ways and this means the risks can vary hugely. For example, using a chatbot to produce a summary of a long article presents very different risks to using the same technology to provide medical advice. We understand the need to monitor these developments in partnership with innovators while also avoiding placing unnecessary regulatory burdens on those deploying \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003eTo ensure our regulatory framework is effective, we will leverage the expertise of our world class regulators. They understand the risks in their sectors and are best placed to take a proportionate approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. This will mean supporting innovation and working closely with business, but also stepping in to address risks when necessary. By underpinning the framework with a set of principles, we will drive consistency across regulators while also providing them with the flexibility needed.\u003c/p\u003e\u003cp\u003eFor innovators working at the cutting edge and developing novel technologies, navigating regulatory regimes can be challenging. That’s why we are confirming our commitment to taking forward a key recommendation made by Sir Patrick Vallance to establish a regulatory sandbox for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. This will bring together regulators to support innovators directly and help them get their products to market. The sandbox will also enable us to understand how regulation interacts with new technologies and refine this interaction where necessary.\u003c/p\u003e\u003cp\u003eHaving exited the European Union we are free to establish a regulatory approach that enables us to establish the UK as an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e superpower. It is an approach that will actively support innovation while addressing risks and public concerns. The UK is home to thriving start-ups, which our framework will support to scale-up and compete internationally. Our pro-innovation approach will also act as a strong incentive when it comes to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e businesses based overseas establishing a presence in the UK. The white paper sets out our commitment to engaging internationally to support interoperability across different regulatory regimes. Not only will this ease the burden on business but it will also allow us to embed our values as global approaches to governing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e develop.\u003c/p\u003e\u003cp\u003eOur approach relies on collaboration between government, regulators and business. Initially, we do not intend to introduce new legislation. By rushing to legislate too early, we would risk placing undue burdens on businesses. But alongside empowering regulators to take a lead, we are also setting expectations. Our new monitoring functions will provide a real time assessment of how the regulatory framework is performing so that we can be confident that it is proportionate. The pace of technological development also means that we need to understand new and emerging risks, engaging with experts to ensure we take action where necessary. A critical component of this activity will be engaging with the public to understand their expectations, raising awareness of the potential of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and demonstrating that we are responding to concerns.\u003c/p\u003e\u003cp\u003eThe framework set out in this white paper is deliberately designed to be flexible. As the technology evolves, our regulatory approach may also need to adjust. Our principles-based approach, with central functions to monitor and drive collaboration, will enable us to adapt as needed while providing industry with the clarity needed to innovate. We will continue to develop our approach, building on our commitment to making the UK the best place in the world to be a business developing and using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Responses to the consultation will inform how we develop the regulatory framework – I encourage all of those with an interest to respond.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Executive summary",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#executive-summary",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#executive-summary",
        "text": "\u003ch3 id=\"artificial-intelligence--the-opportunity-and-the-challenge\"\u003eArtificial intelligence – the opportunity and the challenge\u003c/h3\u003e\u003cp\u003e1. Artificial intelligence (\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e) is already delivering wide societal benefits, from medical advances\u003csup id=\"fnref:1\"\u003e\u003ca href=\"#fn:1\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 1]\u003c/a\u003e\u003c/sup\u003e  to mitigating climate change.\u003csup id=\"fnref:2\"\u003e\u003ca href=\"#fn:2\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 2]\u003c/a\u003e\u003c/sup\u003e For example, an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technology developed by DeepMind, a UK-based business, can now predict the structure of almost every protein known to science.\u003csup id=\"fnref:3\"\u003e\u003ca href=\"#fn:3\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 3]\u003c/a\u003e\u003c/sup\u003e This breakthrough will accelerate scientific research and the development of life-saving medicines – it has already helped scientists to make huge progress in combating malaria, antibiotic resistance, and plastic waste.\u003c/p\u003e\u003cp\u003e2. The UK Science and Technology Framework\u003csup id=\"fnref:4\"\u003e\u003ca href=\"#fn:4\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 4]\u003c/a\u003e\u003c/sup\u003e sets out government’s strategic vision and identifies \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as one of 5 critical technologies. The framework notes the role of regulation in creating the environment for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to flourish. We know that we have yet to see \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies reach their full potential. Under the right conditions, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e will transform all areas of life\u003csup id=\"fnref:5\"\u003e\u003ca href=\"#fn:5\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 5]\u003c/a\u003e\u003c/sup\u003e and stimulate the UK economy by unleashing innovation and driving productivity,\u003csup id=\"fnref:6\"\u003e\u003ca href=\"#fn:6\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 6]\u003c/a\u003e\u003c/sup\u003e creating new jobs and improving the workplace.\u003c/p\u003e\u003cp\u003e3. Across the world, countries and regions are beginning to draft the rules for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. The UK needs to act quickly to continue to lead the international conversation on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance and demonstrate the value of our pragmatic, proportionate regulatory approach. The need to act was highlighted by Sir Patrick Vallance in his recent Regulation for Innovation review. The report identifies the short time frame for government intervention to provide a clear, pro-innovation regulatory environment in order to make the UK one of the top places in the world to build foundational \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies.\u003csup id=\"fnref:7\"\u003e\u003ca href=\"#fn:7\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 7]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e4. While we should capitalise on the benefits of these technologies, we should also not overlook the new risks that may arise from their use, nor the unease that the complexity of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies can produce in the wider public. We already know that some uses of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could damage our physical\u003csup id=\"fnref:8\"\u003e\u003ca href=\"#fn:8\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 8]\u003c/a\u003e\u003c/sup\u003e and mental health, \u003csup id=\"fnref:9\"\u003e\u003ca href=\"#fn:9\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 9]\u003c/a\u003e\u003c/sup\u003e infringe on the privacy of individuals\u003csup id=\"fnref:10\"\u003e\u003ca href=\"#fn:10\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 10]\u003c/a\u003e\u003c/sup\u003e and undermine human rights.\u003csup id=\"fnref:11\"\u003e\u003ca href=\"#fn:11\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 11]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e5. Public trust in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e will be undermined unless these risks, and wider concerns about the potential for bias and discrimination, are addressed. By building trust, we can accelerate the adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e across the UK to maximise the economic and social benefits that the technology can deliver, while attracting investment and stimulating the creation of high-skilled \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e jobs.\u003csup id=\"fnref:12\"\u003e\u003ca href=\"#fn:12\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 12]\u003c/a\u003e\u003c/sup\u003e In order to maintain the UK’s position as a global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e leader, we need to ensure that the public continues to see how the benefits of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can outweigh the risks.\u003csup id=\"fnref:13\"\u003e\u003ca href=\"#fn:13\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 13]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e6. Responding to risk and building public trust are important drivers for regulation. But clear and consistent regulation can also support business investment and build confidence in innovation. Throughout our extensive engagement, industry repeatedly emphasised that consumer trust is key to the success of innovation economies. We therefore need a clear, proportionate approach to regulation that enables the responsible application of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to flourish. Instead of creating cumbersome rules applying to all \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies, our framework ensures that regulatory measures are proportionate to context and outcomes, by focusing on the use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e rather than the technology itself.\u003c/p\u003e\u003cp\u003e7. People and organisations develop and use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in the UK within the rules set by our existing laws, informed by standards, guidance and other tools. But \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is a general purpose technology and its uses can cut across regulatory remits. As a result, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies are currently regulated through a complex patchwork of legal requirements. We are concerned by feedback from across industry that the absence of cross-cutting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation creates uncertainty and inconsistency which can undermine business and consumer confidence in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, and stifle innovation. By providing a clear and unified approach to regulation, our framework will build public confidence, making it clear that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies are subject to cross-cutting, principles-based regulation.\u003c/p\u003e\u003ch3 id=\"our-pro-innovation-framework\"\u003eOur pro-innovation framework\u003c/h3\u003e\u003cp\u003e8. The government will put in place a new framework to bring clarity and coherence to the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory landscape. This regime is designed to make responsible innovation easier. It will strengthen the UK’s position as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, harness \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s ability to drive growth and prosperity,\u003csup id=\"fnref:14\"\u003e\u003ca href=\"#fn:14\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 14]\u003c/a\u003e\u003c/sup\u003e and increase public trust in its use and application.\u003c/p\u003e\u003cp\u003e9. We are taking a deliberately agile and iterative approach, recognising the speed at which these technologies are evolving. Our framework is designed to build the evidence base so that we can learn from experience and continuously adapt to develop the best possible regulatory regime. Industry has praised our pragmatic and proportionate approach.\u003c/p\u003e\u003cp\u003e10. Our framework is underpinned by 5 principles to guide and inform the responsible development and use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in all sectors of the economy:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eSafety, security and robustness\u003c/li\u003e\n  \u003cli\u003eAppropriate transparency and explainability\u003c/li\u003e\n  \u003cli\u003eFairness\u003c/li\u003e\n  \u003cli\u003eAccountability and governance\u003c/li\u003e\n  \u003cli\u003eContestability and redress\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e11. We will not put these principles on a statutory footing initially. New rigid and onerous legislative requirements on businesses could hold back \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation and reduce our ability to respond quickly and in a proportionate way to future technological advances. Instead, the principles will be issued on a non-statutory basis and implemented by existing regulators. This approach makes use of regulators’ domain-specific expertise to tailor the implementation of the principles to the specific context in which \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is used. During the initial period of implementation, we will continue to collaborate with regulators to identify any barriers to the proportionate application of the principles, and evaluate whether the non-statutory framework is having the desired effect.\u003c/p\u003e\u003cp\u003e12. Following this initial period of implementation, and when parliamentary time allows, we anticipate introducing a statutory duty on regulators requiring them to have due regard to the principles. Some feedback from regulators, industry and academia suggested we should implement further measures to support the enforcement of the framework. A duty requiring regulators to have regard to the principles should allow regulators the flexibility to exercise judgement when applying the principles in particular contexts, while also strengthening their mandate to implement them. In line with our proposal to work collaboratively with regulators and take an adaptable approach, we will not move to introduce such a statutory duty if our monitoring of the framework shows that implementation is effective without the need to legislate.\u003c/p\u003e\u003cp\u003e13. In the 2022 \u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation policy paper\u003c/a\u003e,\u003csup id=\"fnref:15\"\u003e\u003ca href=\"#fn:15\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 15]\u003c/a\u003e\u003c/sup\u003e we proposed a small coordination layer within the regulatory architecture. Industry and civil society were supportive of our intention to ensure coherence across the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework. However, feedback often argued strongly for greater central coordination to support regulators on issues requiring cross-cutting collaboration and ensure that the overall regulatory framework functions as intended.\u003c/p\u003e\u003cp\u003e14. We have identified a number of central support functions required to make sure that the overall framework offers a proportionate but effective response to risk while promoting innovation across the regulatory landscape:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eMonitoring and evaluation of the overall regulatory framework’s effectiveness and the implementation of the principles, including the extent to which implementation supports innovation. This will allow us to remain responsive and adapt the framework if necessary, including where it needs to be adapted to remain effective in the context of developments in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s capabilities and the state of the art.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eAssessing and monitoring risks across the economy arising from \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eConducting horizon scanning and gap analysis, including by convening industry, to inform a coherent response to emerging \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technology trends.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eSupporting testbeds and sandbox initiatives to help \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators get new technologies to market.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eProviding education and awareness to give clarity to businesses and empower citizens to make their voices heard as part of the ongoing iteration of the framework.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003ePromoting interoperability with international regulatory frameworks.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e15. The central support functions will initially be provided from within government but will leverage existing activities and expertise from across the broader economy. The activities described above will neither replace nor duplicate the work undertaken by regulators and will not involve the creation of a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulator.\u003c/p\u003e\u003cp\u003e16. Our proportionate approach recognises that regulation is not always the most effective way to support responsible innovation. The proposed framework is aligned with, and supplemented by, a variety of tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, such as assurance techniques, voluntary guidance and technical standards. Government will promote the use of such tools. We are collaborating with partners like the \u003ca rel=\"external\" href=\"https://aistandardshub.org/\"\u003eUK \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Standards Hub\u003c/a\u003e to ensure that our overall governance framework encourages responsible \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation (see \u003ca href=\"#partfour\"\u003epart 4\u003c/a\u003e for details).\u003c/p\u003e\u003cp\u003e17. In keeping with the global nature of these technologies, we will also continue to work with international partners to deliver interoperable measures that incentivise the responsible design, development and application of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. During our call for views, industry, academia and civil society stressed that international alignment should support UK businesses to capitalise on global markets and protect UK citizens from cross-border harms.\u003c/p\u003e\u003cp\u003e18. The UK is frequently ranked third in the world across a range of measures, including level of investment, innovation and implementation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003csup id=\"fnref:16\"\u003e\u003ca href=\"#fn:16\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 16]\u003c/a\u003e\u003c/sup\u003e To make the UK the most attractive place in the world for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation and support UK companies wishing to export and attract international investment, we must ensure international compatibility between approaches. Countries around the world, as well as multilateral forums, are exploring approaches to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Thanks to our reputation for pragmatic regulation, the UK is rightly seen by international partners as a leader in this global conversation.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 1: Introduction",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-1-introduction",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-1-introduction",
        "text": "\u003ch3 id=\"the-power-and-potential-of-artificial-intelligence\"\u003e1.1 The power and potential of artificial intelligence\u003c/h3\u003e\u003cp\u003e19. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is already delivering major advances and efficiencies in many areas. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e quietly automates aspects of our everyday activities, from systems that monitor traffic to make our commutes smoother,\u003csup id=\"fnref:17\"\u003e\u003ca href=\"#fn:17\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 17]\u003c/a\u003e\u003c/sup\u003e to those that detect fraud in our bank accounts.\u003csup id=\"fnref:18\"\u003e\u003ca href=\"#fn:18\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 18]\u003c/a\u003e\u003c/sup\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e has revolutionised large-scale safety-critical practices in industry, like controlling the process of nuclear fusion.\u003csup id=\"fnref:19\"\u003e\u003ca href=\"#fn:19\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 19]\u003c/a\u003e\u003c/sup\u003e And it has also been used to accelerate scientific advancements, such as the discovery of new medicine\u003csup id=\"fnref:20\"\u003e\u003ca href=\"#fn:20\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 20]\u003c/a\u003e\u003c/sup\u003e or the technologies we need to tackle climate change.\u003csup id=\"fnref:21\"\u003e\u003ca href=\"#fn:21\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 21]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e20. But this is just the beginning. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can be used in a huge variety of settings and has the extraordinary potential to transform our society and economy.\u003csup id=\"fnref:22\"\u003e\u003ca href=\"#fn:22\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 22]\u003c/a\u003e\u003c/sup\u003e It could have as much impact as electricity or the internet, and has been identified as one of the 5 critical technologies in the UK Science and Technology Framework.\u003csup id=\"fnref:23\"\u003e\u003ca href=\"#fn:23\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 23]\u003c/a\u003e\u003c/sup\u003e As \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e becomes more powerful, and as innovators explore new ways to use it, we will see more applications of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e emerge. As a result, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e has a huge potential to drive growth\u003csup id=\"fnref:24\"\u003e\u003ca href=\"#fn:24\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 24]\u003c/a\u003e\u003c/sup\u003e and create jobs.\u003csup id=\"fnref:25\"\u003e\u003ca href=\"#fn:25\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 25]\u003c/a\u003e\u003c/sup\u003e It will support people to carry out their existing jobs, by helping to improve workforce efficiency and workplace safety.\u003csup id=\"fnref:26\"\u003e\u003ca href=\"#fn:26\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 26]\u003c/a\u003e\u003c/sup\u003e To remain world leaders in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, attract global talent and create high-skilled jobs in the UK, we must create a regulatory environment where such innovation can thrive.\u003c/p\u003e\u003cp\u003e21. Technological advances like large language models (\u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e) are an indication of the transformative developments yet to come.\u003csup id=\"fnref:27\"\u003e\u003ca href=\"#fn:27\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 27]\u003c/a\u003e\u003c/sup\u003e \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e provide substantial opportunities to transform the economy and society. For example, \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e can automate the process of writing code and fixing programming bugs. The technology can support genetic medicine by identifying links between genetic sequences and medical conditions. It can support people to review and summarise key points from lengthy documents. In the last 4 years, \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e have been developed beyond expectations and they are becoming applicable to an increasingly wide range of tasks.\u003csup id=\"fnref:28\"\u003e\u003ca href=\"#fn:28\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 28]\u003c/a\u003e\u003c/sup\u003e We expand on the development of \u003cabbr title=\"large language model\"\u003eLLM\u003c/abbr\u003e and other foundation models in \u003ca href=\"#section333\"\u003esection 3.3.3\u003c/a\u003e below.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box11\"\u003eBox 1.1: Examples of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e opportunities\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e helps piece together the first complete image of a black hole\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can enable scientific discovery. A computer vision model was used to piece together the first ever image of a black hole 55 million light years away, combining images from 8 telescopes around the world.\u003csup id=\"fnref:29\"\u003e\u003ca href=\"#fn:29\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 29]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e solves decades old protein-folding puzzle\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eAn \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e company based in the UK trained neural networks to predict the structures of proteins, solving a problem that had long stumped scientists. The predictions are advancing the field of structural biology: scientists have already used them to prevent antibiotic resistance,\u003csup id=\"fnref:30\"\u003e\u003ca href=\"#fn:30\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 30]\u003c/a\u003e\u003c/sup\u003e advance disease research,\u003csup id=\"fnref:31\"\u003e\u003ca href=\"#fn:31\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 31]\u003c/a\u003e\u003c/sup\u003e and accelerate the fight against plastic pollution.\u003csup id=\"fnref:32\"\u003e\u003ca href=\"#fn:32\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 32]\u003c/a\u003e\u003c/sup\u003e As we find more uses for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, it will rewrite scientific fields and change the way we learn about our world.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eDeep learning \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could improve breast cancer screening\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could transform how diseases are detected, prevented, and treated. Doctors are testing if deep learning can be applied to breast cancer screening. Currently, every mammogram is double-checked by radiologists but this is labour-intensive and causes diagnosis delays. A UK medical technology company is working with the \u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e to test \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e for the second screening, meaning greater numbers of patients could be screened faster and clinicians could spend more time with patients and provide faster access to treatment.\u003csup id=\"fnref:33\"\u003e\u003ca href=\"#fn:33\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 33]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eFarming efficiency increased by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e robots\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eApplying robotics and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to field management can make farming more efficient, sustainable and productive. Lightweight, autonomous mapping and monitoring robots operating across the UK can spend hours on the field in all conditions and significantly reduce soil compaction. These systems can digitise the field, providing farmers with data to improve weed and pest management. If these systems become widely used, they could contribute to agricultural and horticultural productivity, reduce the pressure of labour shortages and better preserve the environment.\u003csup id=\"fnref:34\"\u003e\u003ca href=\"#fn:34\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 34]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e helps accelerate the discovery of new medicines\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eSignificant time and resources are currently needed to develop new and effective medicines. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can accelerate the discovery of new medicines by quickly identifying potential biologically active compounds from millions of candidates within a short period.\u003csup id=\"fnref:35\"\u003e\u003ca href=\"#fn:35\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 35]\u003c/a\u003e\u003c/sup\u003e Scientists may also have succeeded in using generative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to design antibodies that bind to a human protein linked to cancer.\u003csup id=\"fnref:36\"\u003e\u003ca href=\"#fn:36\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 36]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is used in the fight against the most serious and harmful crimes\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe Child Images Abuse Database\u003csup id=\"fnref:37\"\u003e\u003ca href=\"#fn:37\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 37]\u003c/a\u003e\u003c/sup\u003e uses the powerful data processing capabilities of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to identify victims and perpetrators of child sexual abuse. The quick and effective identification of victims and perpetrators in digital abuse images allows for real world action to remove victims from harm and ensure their abusers are held to account. The use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e increases the scale and speed of analysis while protecting staff welfare by reducing their exposure to distressing content.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e increases cyber security capabilities\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003ca rel=\"external\" href=\"https://darktrace.com/news/darktrace-artificial-intelligence-autonomously-stops-consequences-of-fast-moving-cyber-attack-at-major-italian-electronics-distributor-6\"\u003eCompanies providing cyber security services are increasingly using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e to analyse large amounts of data about malware and respond to vulnerabilities in network security at faster-than-human speeds.\u003csup id=\"fnref:38\"\u003e\u003ca href=\"#fn:38\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 38]\u003c/a\u003e\u003c/sup\u003e As the complexity of the cyber threat landscape evolves, the pattern-recognition and recursive learning capabilities of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e are likely to play an increasingly significant role in proactive cyber defence against malicious actors.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"managing-ai-risks\"\u003e1.2 Managing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks\u003c/h3\u003e\u003cp\u003e22. The concept of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is not new, but recent advances in data generation and processing have changed the field and the technology it produces. For example, while recent developments in the capabilities of generative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e models have created exciting opportunities, they have also sparked new debates about potential \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks.\u003csup id=\"fnref:39\"\u003e\u003ca href=\"#fn:39\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 39]\u003c/a\u003e\u003c/sup\u003e As \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e research and development continues at pace and scale, we expect to see even greater impact and public awareness of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks.\u003csup id=\"fnref:40\"\u003e\u003ca href=\"#fn:40\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 40]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e23. We know that not all \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks arise from the deliberate action of bad actors. Some \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks can emerge as an unintended consequence or from a lack of appropriate controls to ensure responsible \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use.\u003csup id=\"fnref:41\"\u003e\u003ca href=\"#fn:41\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 41]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e24. We have made an initial assessment of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-specific risks and their potential to cause harm, with reference in our analysis to the values that they threaten if left unaddressed. These values include safety, security, fairness, privacy and agency, human rights, societal well-being and prosperity.\u003c/p\u003e\u003cp\u003e25. Our assessment of cross-cutting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk identified a range of high-level risks that our framework will seek to prioritise and mitigate with proportionate interventions. For example, safety risks include physical damage to humans and property, as well as damage to mental health.\u003csup id=\"fnref:42\"\u003e\u003ca href=\"#fn:42\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 42]\u003c/a\u003e\u003c/sup\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e creates a range of new security risks to individuals, organisations, and critical infrastructure.\u003csup id=\"fnref:43\"\u003e\u003ca href=\"#fn:43\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 43]\u003c/a\u003e\u003c/sup\u003e Without government action, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could cause and amplify discrimination that results in, for example, unfairness in the justice system.\u003csup id=\"fnref:44\"\u003e\u003ca href=\"#fn:44\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 44]\u003c/a\u003e\u003c/sup\u003e Similarly, without regulatory oversight, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies could pose risks to our privacy and human dignity, potentially harming our fundamental liberties.\u003csup id=\"fnref:45\"\u003e\u003ca href=\"#fn:45\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 45]\u003c/a\u003e\u003c/sup\u003e Our regulatory intervention will ensure that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e does not cause harm at a societal level, threatening democracy\u003csup id=\"fnref:46\"\u003e\u003ca href=\"#fn:46\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 46]\u003c/a\u003e\u003c/sup\u003e or UK values.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box-12-illustrative-ai-risks\"\u003eBox 1.2: Illustrative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nThe patchwork of legal frameworks that currently regulate some uses of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e may not sufficiently address the risks that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can pose. The following examples are \u003cstrong\u003ehypothetical scenarios\u003c/strong\u003e designed to illustrate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s potential to create harm.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRisks to human rights\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eGenerative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is used to generate deepfake pornographic video content, potentially damaging the reputation, relationships and dignity of the subject.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRisks to safety\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eAn \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assistant based on \u003cabbr title=\"large language model\"\u003eLLM\u003c/abbr\u003e technology recommends a dangerous activity that it has found on the internet, without understanding or communicating the context of the website where the activity was described. The user undertakes this activity causing physical harm.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRisks to fairness\u003c/strong\u003e\u003csup id=\"fnref:47\"\u003e\u003ca href=\"#fn:47\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 47]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eAn \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e tool assessing credit-worthiness of loan applicants is trained on incomplete or biased data, leading the company to offer loans to individuals on different terms based on characteristics like race or gender.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRisks to privacy and agency\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eConnected devices in the home may constantly gather data, including conversations, potentially creating a near-complete portrait of an individual’s home life. Privacy risks are compounded the more parties can access this data.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRisks to societal wellbeing\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eDisinformation generated and propagated by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could undermine access to reliable information and trust in democratic institutions and processes.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRisks to security\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e tools can be used to automate, accelerate and magnify the impact of highly targeted cyber attacks, increasing the severity of the threat from malicious actors. The emergence of \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e enables hackers\u003csup id=\"fnref:48\"\u003e\u003ca href=\"#fn:48\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 48]\u003c/a\u003e\u003c/sup\u003e with little technical knowledge or skill to generate phishing campaigns with malware delivery capabilities.\u003csup id=\"fnref:49\"\u003e\u003ca href=\"#fn:49\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 49]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"a-note-on-terminology\"\u003e1.3 A note on terminology\u003c/h3\u003e\u003cp\u003eTerminology used in this paper:\u003csup id=\"fnref:50\"\u003e\u003ca href=\"#fn:50\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 50]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/strong\u003e or \u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system\u003c/strong\u003e or \u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies\u003c/strong\u003e: products and services that are ‘adaptable’ and ‘autonomous’ in the sense outlined in our definition in \u003ca href=\"#section321\"\u003esection 3.2.1.\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supplier\u003c/strong\u003e: any organisation or individual who plays a role in the research, development, training, implementation, deployment, maintenance, provision or sale of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e user\u003c/strong\u003e: any individual or organisation that uses an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e product.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle\u003c/strong\u003e: all events and processes that relate to an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system’s lifespan, from inception to decommissioning, including its design, research, training, development, deployment, integration, operation, maintenance, sale, use and governance.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem\u003c/strong\u003e: the complex network of actors and processes that enable the use and supply of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e throughout the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle (including supply chains, markets, and governance mechanisms).\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eFoundation model\u003c/strong\u003e: a type of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e model that is trained on a vast quantity of data and is adaptable for use on a wide range of tasks. Foundation models can be used as a base for building more specific \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e models. Foundation models are discussed in more detail in section \u003ca href=\"#section333\"\u003e3.3.3\u003c/a\u003e below.\u003csup id=\"fnref:51\"\u003e\u003ca href=\"#fn:51\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 51]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eImpacted third party\u003c/strong\u003e: an individual or company that is impacted by the outcomes of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems that they do not use or supply themselves.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 2: The current regulatory environment",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-2-the-current-regulatory-environment",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-2-the-current-regulatory-environment",
        "text": "\u003ch3 id=\"navigating-the-current-landscape\"\u003e2.1 Navigating the current landscape\u003c/h3\u003e\u003cp\u003e26. The UK’s \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e success is, in part, due to our reputation for high-quality regulators and our strong approach to the rule of law, supported by our technology-neutral legislation and regulations. UK laws, regulators and courts already address some of the emerging risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies (see \u003ca href=\"#box21\"\u003ebox 2.1\u003c/a\u003e for examples). This strong legal foundation encourages investment in new technologies, enabling \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation to thrive,\u003csup id=\"fnref:52\"\u003e\u003ca href=\"#fn:52\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 52]\u003c/a\u003e\u003c/sup\u003e and high-quality jobs to flourish.\u003csup id=\"fnref:53\"\u003e\u003ca href=\"#fn:53\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 53]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box21\"\u003eBox 2.1: Example of legal coverage of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in the UK and potential gaps\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nDiscriminatory outcomes that result from the use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e may contravene the protections set out in the Equality Act 2010.\u003csup id=\"fnref:54\"\u003e\u003ca href=\"#fn:54\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 54]\u003c/a\u003e\u003c/sup\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems are also required by data protection law to process personal data fairly.\u003csup id=\"fnref:55\"\u003e\u003ca href=\"#fn:55\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 55]\u003c/a\u003e\u003c/sup\u003e However, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can increase the risk of unfair bias or discrimination across a range of indicators or characteristics. This could undermine public trust in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\n\n  \u003cp\u003eProduct safety laws ensure that goods manufactured and placed on the market in the UK are safe. Product-specific legislation (such as for electrical and electronic equipment,\u003csup id=\"fnref:56\"\u003e\u003ca href=\"#fn:56\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 56]\u003c/a\u003e\u003c/sup\u003e medical devices,\u003csup id=\"fnref:57\"\u003e\u003ca href=\"#fn:57\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 57]\u003c/a\u003e\u003c/sup\u003e and toys\u003csup id=\"fnref:58\"\u003e\u003ca href=\"#fn:58\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 58]\u003c/a\u003e\u003c/sup\u003e) may apply to some products that include integrated \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. However, safety risks specific to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies should be monitored closely. As the capability and adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e increases, it may pose new and substantial risks that are unaddressed by existing rules.\u003c/p\u003e\n\n  \u003cp\u003eConsumer rights law\u003csup id=\"fnref:59\"\u003e\u003ca href=\"#fn:59\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 59]\u003c/a\u003e\u003c/sup\u003e may protect consumers where they have entered into a sales contract for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-based products and services. Certain contract terms (for example, that goods are of satisfactory quality, fit for a particular purpose, and as described) are relevant to consumer contracts. Similarly, businesses are prohibited from including certain terms in consumer contracts. Tort law provides a complementary regime that may provide redress where a civil wrong has caused harm. It is not yet clear whether consumer rights law will provide the right level of protection in the context of products that include integrated \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e or services based on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, or how tort law may apply to fill any gap in consumer rights law protection.\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e27. While \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is currently regulated through existing legal frameworks like financial services regulation,\u003csup id=\"fnref:60\"\u003e\u003ca href=\"#fn:60\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 60]\u003c/a\u003e\u003c/sup\u003e some \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks arise across, or in the gaps between, existing regulatory remits. Industry told us that conflicting or uncoordinated requirements from regulators create unnecessary burdens and that regulatory gaps may leave risks unmitigated, harming public trust and slowing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption.\u003c/p\u003e\u003cp\u003e28. Industry has warned us that regulatory incoherence could stifle innovation and competition by causing a disproportionate amount of smaller businesses to leave the market. If regulators are not proportionate and aligned in their regulation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, businesses may have to spend excessive time and money complying with complex rules instead of creating new technologies. Small businesses and start-ups often do not have the resources to do both.\u003csup id=\"fnref:61\"\u003e\u003ca href=\"#fn:61\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 61]\u003c/a\u003e\u003c/sup\u003e With the vast majority of digital technology businesses employing under 50 people,\u003csup id=\"fnref:62\"\u003e\u003ca href=\"#fn:62\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 62]\u003c/a\u003e\u003c/sup\u003e it is important to ensure that regulatory burdens do not fall disproportionately on smaller companies, which play an essential role in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation ecosystem and act as engines for economic growth and job creation.\u003csup id=\"fnref:63\"\u003e\u003ca href=\"#fn:63\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 63]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e29. Regulatory coordination will support businesses to invest confidently in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation and build public trust by ensuring real risks are effectively addressed. While some regulators already work together to ensure regulatory coherence for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e through formal networks like the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and digital regulations service in the health sector\u003csup id=\"fnref:64\"\u003e\u003ca href=\"#fn:64\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 64]\u003c/a\u003e\u003c/sup\u003e and the Digital Regulation Cooperation Forum (\u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e), other regulators have limited capacity and access to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e expertise. This creates the risk of inconsistent enforcement across regulators. There is also a risk that some regulators could begin to dominate and interpret the scope of their remit or role more broadly than may have been intended in order to fill perceived gaps in a way that increases incoherence and uncertainty. Industry asked us to support further system-wide coordination to clarify who is responsible for addressing cross-cutting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks and avoid duplicate requirements across multiple regulators.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-21-addressing-ai-fairness-under-the-existing-legal-and-regulatory-framework\"\u003eCase study 2.1: Addressing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e fairness under the existing legal and regulatory framework\u003c/h4\u003e\n\n  \u003cp\u003eA fictional company, ‘\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited’, is designing a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-driven algorithm to set prices for insurance premiums that accurately reflect a client’s risk. Setting fair prices and building consumer trust is a key component of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited’s brand so ensuring it complies with the relevant legislation and guidance is a priority.\u003c/p\u003e\n\n  \u003cp\u003eFairness in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems is covered by a variety of regulatory requirements and best practice. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited’s use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to set prices for insurance premiums could be subject to a range of legal frameworks, including data protection, equality, and general consumer protection laws. It could also be subject to sectoral rules like the Financial Services and Markets Act 2000.\u003csup id=\"fnref:65\"\u003e\u003ca href=\"#fn:65\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 65]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eIt can be challenging for a company like \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited to identify which rules are relevant and confidently apply them to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases. There is currently a lack of support for businesses like \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited to navigate the regulatory landscape, with no cross-cutting principles and limited system-wide coordination.\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e30. Government intervention is needed to improve the regulatory landscape. We intend to leverage and build on existing regimes, maximising the benefits of what we already have, while intervening in a proportionate way to address regulatory uncertainty and gaps. This will deliver a pro-innovation regulatory framework that is designed to be adaptable and future-proof, supported by tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e including assurance techniques and technical standards. This approach will provide more clarity and encourage collaboration between government, regulators and industry to unlock innovation.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-22-adapting-regulatory-approaches-to-ai--ai-as-a-medical-device\"\u003eCase study 2.2: Adapting regulatory approaches to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e – \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as a medical device\u003c/h4\u003e\n\n  \u003cp\u003eSome UK regulators have led the way and proactively adapted their approaches to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-enabled technologies.\u003c/p\u003e\n\n  \u003cp\u003eIn 2022, the \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e (Medicines and Healthcare products Regulatory Agency) published a roadmap clarifying in guidance the requirements for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and software used in medical devices.\u003csup id=\"fnref:66\"\u003e\u003ca href=\"#fn:66\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 66]\u003c/a\u003e\u003c/sup\u003e regulator is also updating the regulatory framework for medical devices to protect patients and secure the UK’s global reputation for responsible innovation in medical device software.\u003c/p\u003e\n\n  \u003cp\u003eAs part of this work, the \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e will develop guidance on the transparency and interpretability of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as a medical device.\u003csup id=\"fnref:67\"\u003e\u003ca href=\"#fn:67\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 67]\u003c/a\u003e\u003c/sup\u003e The \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e will consider the specific challenges posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in this context, drawing on the applicable \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation cross-sectoral principles and ethical principles for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in health and social care to issue practical guidance on how to meet legal product safety requirements. The \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e will work with other regulators such as the Information Commissioner’s Office (\u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e) and the National Data Guardian to consider patients’ data protection and trust in medical devices.\u003c/p\u003e\n\n  \u003cp\u003eThis work will provide manufacturers with clear requirements and guidance to attract responsible innovation to the UK.\u003c/p\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 3: An innovative and iterative approach",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-3-an-innovative-and-iterative-approach",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-3-an-innovative-and-iterative-approach",
        "text": "\u003ch3 id=\"aims-of-the-regulatory-framework\"\u003e3.1 Aims of the regulatory framework\u003c/h3\u003e\u003cp\u003e31. Regulation can increase innovation by giving businesses the incentive to solve important problems while addressing the risk of harm to citizens. For example, product safety legislation has increased innovation towards safer products and services.\u003csup id=\"fnref:68\"\u003e\u003ca href=\"#fn:68\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 68]\u003c/a\u003e\u003c/sup\u003e In the case of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, a context-based, proportionate approach to regulation will help strengthen public trust and increase \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption.\u003csup id=\"fnref:69\"\u003e\u003ca href=\"#fn:69\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 69]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e32. The National \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Strategy set out our aim to regulate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e effectively and support innovation.\u003csup id=\"fnref:70\"\u003e\u003ca href=\"#fn:70\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 70]\u003c/a\u003e\u003c/sup\u003e In line with the principles set out in the Plan for Digital Regulation,\u003csup id=\"fnref:71\"\u003e\u003ca href=\"#fn:71\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 71]\u003c/a\u003e\u003c/sup\u003e our approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation will be proportionate; balancing real risks against the opportunities and benefits that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can generate. We will maintain an effective balance as we implement the framework by focusing on the context and outcomes of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e33. Our policy paper proposed a pro-innovation framework designed to give consumers the confidence to use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e products and services, and provide businesses the clarity they need to invest in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and innovate responsibly.\u003csup id=\"fnref:72\"\u003e\u003ca href=\"#fn:72\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 72]\u003c/a\u003e\u003c/sup\u003e This approach was broadly welcomed – particularly by industry. Based on feedback, we have distilled our aims into 3 objectives that our framework is designed to achieve:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\n\u003cstrong\u003eDrive growth and prosperity\u003c/strong\u003e by making responsible innovation easier and reducing regulatory uncertainty. This will encourage investment in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and support its adoption throughout the economy, creating jobs and helping us to do them more efficiently.\u003cbr\u003e\u003cbr\u003eTo achieve this objective we must act quickly to remove existing barriers to innovation and prevent the emergence of new ones. This will allow \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies to capitalise on early development successes and achieve long term market advantage.\u003csup id=\"fnref:73\"\u003e\u003ca href=\"#fn:73\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 73]\u003c/a\u003e\u003c/sup\u003e By acting now, we can give UK innovators a headstart in the global race to convert the potential of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e into long term advantages for the UK, maximising the economic and social value of these technologies and strengthening our current position as a world leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003csup id=\"fnref:74\"\u003e\u003ca href=\"#fn:74\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 74]\u003c/a\u003e\u003c/sup\u003e\n\u003cbr\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eIncrease public trust in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/strong\u003e by addressing risks and protecting our fundamental values.\u003cbr\u003e\u003cbr\u003eTrust is a critical driver for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption.\u003csup id=\"fnref:75\"\u003e\u003ca href=\"#fn:75\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 75]\u003c/a\u003e\u003c/sup\u003e If people do not trust \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, they will be reluctant to use it. Such reluctance can reduce demand for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e products and hinder innovation. Therefore we must demonstrate that our regulatory framework (described in section \u003ca href=\"#32\"\u003e3.2\u003c/a\u003e) effectively addresses \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks.\n\u003cbr\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eStrengthen the UK’s position as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/strong\u003e The development of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies can address some of the most pressing global challenges, from climate change to future pandemics. There is also growing international recognition that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e requires new regulatory responses to guide responsible innovation.\u003cbr\u003e\u003cbr\u003eThe UK can play a central role in the global conversation by shaping international governance and regulation to maximise opportunities and build trust in the technology, while mitigating potential cross-border risks and protecting our democratic values. There is also an important leadership role for the UK in the development of the global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance industry,\u003csup id=\"fnref:76\"\u003e\u003ca href=\"#fn:76\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 76]\u003c/a\u003e\u003c/sup\u003e including in auditing and safety.\u003cbr\u003e\u003cbr\u003eWe will ensure that the UK remains attractive to innovators and investors by promoting interoperability with other regulatory approaches and minimising cross-border frictions. We will work closely with global partners through multilateral and bilateral engagements to learn from, influence and adapt as international and domestic approaches to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation continue to emerge (see \u003ca href=\"#partsix\"\u003epart 6\u003c/a\u003e).\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e34. The proposed regulatory framework does not seek to address all of the wider societal and global challenges that may relate to the development or use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. This includes issues relating to access to data, compute capability, and sustainability, as well as the balancing of the rights of content producers and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e developers. These are important issues to consider – especially in the context of the UK’s ability to maintain its place as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e – but they are outside of the scope of our proposals for a new overarching framework for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation.\u003c/p\u003e\u003cp\u003e35. Government is taking wider action to ensure the UK retains its status as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, for example by taking forward Sir Patrick Vallance’s recommendation relating to intellectual property law and generative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003csup id=\"fnref:77\"\u003e\u003ca href=\"#fn:77\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 77]\u003c/a\u003e\u003c/sup\u003e This will ensure we keep the right balance between protecting rights holders and our thriving creative industries, while supporting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e developers to access the data they need.\u003c/p\u003e\u003ch3 id=\"the-proposed-regulatory-framework\"\u003e3.2 The proposed regulatory framework\u003c/h3\u003e\u003cp id=\"paragraph37\"\u003e36. Our innovative approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation uses a principles-based framework for regulators to interpret and apply to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e within their remits. This collaborative and iterative approach can keep pace with a fast moving technology that requires proportionate action to balance risk and opportunity and to strengthen the UK’s position as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Our agile approach aligns with Sir Patrick Vallance’s Regulation for Innovation report,\u003csup id=\"fnref:78\"\u003e\u003ca href=\"#fn:78\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 78]\u003c/a\u003e\u003c/sup\u003e which highlights that flexible regulatory approaches can better strike the balance between providing clarity, building trust and enabling experimentation. Our framework will provide more clarity to innovators by encouraging collaboration between government, regulators, industry and civil society.\u003c/p\u003e\u003cp\u003e37. We have identified the essential characteristics of our regulatory regime. Our framework will be \u003cstrong\u003epro-innovation\u003c/strong\u003e, \u003cstrong\u003eproportionate\u003c/strong\u003e, \u003cstrong\u003etrustworthy\u003c/strong\u003e, \u003cstrong\u003eadaptable\u003c/strong\u003e, \u003cstrong\u003eclear\u003c/strong\u003e and \u003cstrong\u003ecollaborative\u003c/strong\u003e.\u003csup id=\"fnref:79\"\u003e\u003ca href=\"#fn:79\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 79]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003e\n\u003cstrong\u003ePro-innovation\u003c/strong\u003e: enabling rather than stifling responsible innovation.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eProportionate\u003c/strong\u003e: avoiding unnecessary or disproportionate burdens for businesses and regulators.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eTrustworthy\u003c/strong\u003e: addressing real risks and fostering public trust in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in order to promote and encourage its uptake.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eAdaptable\u003c/strong\u003e: enabling us to adapt quickly and effectively to keep pace with emergent opportunities and risks as \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies evolve.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eClear\u003c/strong\u003e: making it easy for actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle, including businesses using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, to know what the rules are, who they apply to, who enforces them, and how to comply with them.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003e\n\u003cstrong\u003eCollaborative\u003c/strong\u003e: encouraging government, regulators, and industry to work together to facilitate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation, build trust and ensure that the voice of the public is heard and considered.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e38. The framework, built around the 4 key elements below, is designed to empower our existing regulators and promote coherence across the regulatory landscape. The 4 key elements are:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eDefining \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e based on its unique characteristics to support regulator coordination (\u003ca href=\"#section321\"\u003esection 3.2.1\u003c/a\u003e).\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eAdopting a context-specific approach (\u003ca href=\"#section322\"\u003esection 3.2.2\u003c/a\u003e).\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eProviding a set of cross-sectoral principles to guide regulator responses to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks and opportunities(\u003ca href=\"#section323\"\u003esection 3.2.3\u003c/a\u003e).\u003cbr\u003e\n    \u003cul\u003e\n      \u003cli\u003eThe principles clarify government’s expectations for responsible \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and describe good governance at all stages of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eThe application of the principles will initially be at the discretion of the regulators, allowing prioritisation according to the needs of their sectors.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eFollowing this initial non-statutory period of implementation, and when parliamentary time allows, we anticipate introducing a statutory duty requiring regulators to have due regard to the principles.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eDelivering new central functions to support regulators to deliver the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework, maximising the benefits of an iterative approach and ensuring that the framework is coherent (\u003ca href=\"#section324\"\u003esection 3.2.4\u003c/a\u003e).\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003ch3 id=\"section321\"\u003e3.2.1 Defining Artificial Intelligence\u003c/h3\u003e\u003cp\u003e39. To regulate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e effectively, and to support the clarity of our proposed framework, we need a common understanding of what is meant by ‘artificial intelligence’. There is no general definition of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e that enjoys widespread consensus.\u003csup id=\"fnref:80\"\u003e\u003ca href=\"#fn:80\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 80]\u003c/a\u003e\u003c/sup\u003e That is why we have defined \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e by reference to the 2 characteristics that generate the need for a bespoke regulatory response.\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eThe ‘adaptivity’ of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can make it difficult to explain the intent or logic of the system’s outcomes:\u003cbr\u003e\n    \u003cul\u003e\n      \u003cli\u003e\n\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems are ‘trained’ – once or continually – and operate by inferring patterns and connections in data which are often not easily discernible to humans.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eThrough such training, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems often develop the ability to perform new forms of inference not directly envisioned by their human programmers.\u003cbr\u003e\n\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n  \u003cli\u003eThe ‘autonomy’ of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can make it difficult to assign responsibility for outcomes:\u003cbr\u003e\n    \u003cul\u003e\n      \u003cli\u003eSome \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems can make decisions without the express intent or ongoing control of a human.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e40. The combination of adaptivity and autonomy can make it difficult to explain, predict, or control the outputs of an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system, or the underlying logic by which they are generated. It can also be challenging to allocate responsibility for the system’s operation and outputs. For regulatory purposes, this has potentially serious implications, particularly when decisions are made relating to significant matters, like an individual’s health, or where there is an expectation that a decision should be justifiable in easily understood terms, like a legal ruling.\u003c/p\u003e\u003cp\u003e41. By defining \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e with reference to these functional capabilities and designing our approach to address the challenges created by these characteristics, we future-proof our framework against unanticipated new technologies that are autonomous and adaptive. Because we are not creating blanket new rules for specific technologies or applications of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, like facial recognition or \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e, we do not need to use rigid legal definitions. Our use of these defining characteristics was widely supported in responses to our policy paper,\u003csup id=\"fnref:81\"\u003e\u003ca href=\"#fn:81\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 81]\u003c/a\u003e\u003c/sup\u003e as rigid definitions can quickly become outdated and restrictive with the rapid evolution of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003csup id=\"fnref:82\"\u003e\u003ca href=\"#fn:82\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 82]\u003c/a\u003e\u003c/sup\u003e We will, however, retain the ability to adapt our approach to defining \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e if necessary, alongside the ongoing monitoring and iteration of the wider regulatory framework.\u003c/p\u003e\u003cp\u003e42. Below, we provide some illustrative examples of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems to demonstrate their autonomous and adaptive characteristics. While many aspects of the technologies described in these case studies will be covered by existing law, they illustrate how \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-specific characteristics introduce novel risks and regulatory implications.\u003c/p\u003e\u003ch4 id=\"figure-1-illustration-of-our-strategy-for-regulating-ai\"\u003eFigure 1: Illustration of our strategy for regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\n\u003c/h4\u003e\u003cfigure class=\"image embedded\"\u003e\u003cdiv class=\"img\"\u003e\u003cimg src=\"https://assets.publishing.service.gov.uk/media/6423094d3d885d000fdadd30/Diagram01__1_.svg\" alt=\"\"\u003e\u003c/div\u003e\n\u003cfigcaption\u003e\u003cp\u003eIllustration of our strategy for regulating AI\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-31-natural-language-processing-in-customer-service-chatbots\"\u003eCase study 3.1: Natural language processing in customer service chatbots\u003c/h4\u003e\n\n  \u003cp\u003e\u003cstrong\u003eAdaptivity:\u003c/strong\u003e Provides responses to real-time customer messages, having been trained on huge datasets to identify statistical patterns in ordinary human speech, potentially increasing personalisation over time as the system learns from each new experience.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eAutonomy:\u003c/strong\u003e Generates a human-like output based on the customer’s text input, to answer queries, help customers find products and services, or send targeted updates. Operates with little need for human oversight or intervention.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eIllustrative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related regulatory implication:\u003c/strong\u003e Unintentional inclusion of inaccurate or misleading information in training data, producing harmful instructions or convincingly spreading misinformation.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-32-automated-healthcare-triage-systems\"\u003eCase study 3.2: Automated healthcare triage systems\u003c/h4\u003e\n\n  \u003cp\u003e\u003cstrong\u003eAdaptivity:\u003c/strong\u003e Predicts patient conditions based on the pathology, treatment and risk factors associated with health conditions from the analysis of medical datasets, patient records and real-time health data.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eAutonomy:\u003c/strong\u003e Generates information about the likely causes of a patient’s symptoms and recommends potential interventions and treatments, either to a medical professional or straight to a patient.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eIllustrative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related regulatory implication:\u003c/strong\u003e Unclear liability for an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e triage system that provides incorrect medical advice, leading to negative health outcomes for a patient and affecting the patient’s ability to obtain redress.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-33-text-to-image-generators\"\u003eCase study 3.3: Text-to-image generators\u003c/h4\u003e\n\n  \u003cp\u003e\u003cstrong\u003eAdaptivity:\u003c/strong\u003e Uses large amounts of online content to learn how to create rich, highly specific images on the basis of a short text prompt.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eAutonomy:\u003c/strong\u003e Based on text input, these systems generate images that mimic the qualities of human-created art, with no ongoing oversight from the user.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eIllustrative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related regulatory implication:\u003c/strong\u003e Reproduction of biases or stereotyping in training data, leading to offensive language or content.\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e43. Industry, regulators, and civil society responded positively to our proposed definition, recognising that it supports our context-based and flexible approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation. We will monitor how regulators interpret and apply adaptivity and autonomy when formulating domain-specific definitions of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Government will support coordination between regulators when we see potential for better alignment between their interpretations and use of our defining characteristics.\u003c/p\u003e\u003cp\u003e44. Active and collaborative horizon scanning will ensure that we can identify developments and emerging trends, and adapt our framework accordingly. We will convene industry, academia and other key stakeholders to inform economy-wide horizon scanning activity. This work will build on the activity of individual regulators.\u003c/p\u003e\u003ch3 id=\"section322\"\u003e3.2.2 Regulating the use – not the technology\u003c/h3\u003e\u003cp\u003e45. Our framework is context-specific.\u003csup id=\"fnref:83\"\u003e\u003ca href=\"#fn:83\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 83]\u003c/a\u003e\u003c/sup\u003e We will not assign rules or risk levels to entire sectors or technologies. Instead, we will regulate based on the outcomes \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is likely to generate in particular applications. For example, it would not be proportionate or effective to classify all applications of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in critical infrastructure as high risk. Some uses of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in critical infrastructure, like the identification of superficial scratches on machinery, can be relatively low risk. Similarly, an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-powered chatbot used to triage customer service requests for an online clothing retailer should not be regulated in the same way as a similar application used as part of a medical diagnostic process.\u003c/p\u003e\u003cp\u003e46. A context-specific approach allows regulators to weigh the risks of using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e against the costs of missing opportunities to do so.\u003csup id=\"fnref:84\"\u003e\u003ca href=\"#fn:84\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 84]\u003c/a\u003e\u003c/sup\u003e Regulators told us that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk assessments should include the failure to exploit \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e capabilities. For example, there can be a significant opportunity cost related to not having access to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in safety-critical operations, from heavy industry,\u003csup id=\"fnref:85\"\u003e\u003ca href=\"#fn:85\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 85]\u003c/a\u003e\u003c/sup\u003e to personal healthcare (see \u003ca href=\"#box11\"\u003ebox 1.1\u003c/a\u003e). Sensitivity to context will allow the framework to respond to the level of risk in a proportionate manner and avoid stifling innovation or missing opportunities to capitalise on the social benefits made available by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e47. To best achieve this context-specificity we will empower existing UK regulators to apply the cross-cutting principles. Regulators are best placed to conduct detailed risk analysis and enforcement activities within their areas of expertise. Creating a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-specific, cross-sector regulator would introduce complexity and confusion, undermining and likely conflicting with the work of our existing expert regulators.\u003c/p\u003e\u003ch3 id=\"section323\"\u003e3.2.3 A principles-based approach\u003c/h3\u003e\u003cp\u003e48. Existing regulators will be expected to implement the framework underpinned by 5 values-focused cross-sectoral principles:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eSafety, security and robustness\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eAppropriate transparency and explainability\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eFairness\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eAccountability and governance\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eContestability and redress\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eThese build on, and reflect our commitment to, the Organisation for Economic Co-operation and Development (\u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e) values-based \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e principles, which promote the ethical use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e49. The principles set out the key elements of responsible \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e design, development and use, and will help guide businesses. Regulators will lead the implementation of the framework, for example by issuing guidance on best practice for adherence to these principles.\u003c/p\u003e\u003cp\u003e50. Regulators will be expected to apply the principles proportionately to address the risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e within their remits, in accordance with existing laws and regulations. In this way, the principles will complement existing regulation, increase clarity, and reduce friction for businesses operating across regulatory remits.\u003c/p\u003e\u003cp\u003e51. A principles-based approach allows the framework to be agile and proportionate. It is in line with the Plan for Digital Regulation,\u003csup id=\"fnref:86\"\u003e\u003ca href=\"#fn:86\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 86]\u003c/a\u003e\u003c/sup\u003e the findings from the independent Taskforce on Innovation, Growth and Regulatory Reform,\u003csup id=\"fnref:87\"\u003e\u003ca href=\"#fn:87\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 87]\u003c/a\u003e\u003c/sup\u003e the Regulatory Horizons Council’s Closing the Gap report on implementing innovation-friendly regulation,\u003csup id=\"fnref:88\"\u003e\u003ca href=\"#fn:88\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 88]\u003c/a\u003e\u003c/sup\u003e and Sir Patrick Vallance’s Regulation for Innovation report.\u003csup id=\"fnref:89\"\u003e\u003ca href=\"#fn:89\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 89]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e52. Since publishing the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation policy paper,\u003csup id=\"fnref:90\"\u003e\u003ca href=\"#fn:90\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 90]\u003c/a\u003e\u003c/sup\u003e we have updated and strengthened the principles. We have:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eReflected stakeholder feedback by expanding on concepts such as robustness and governance. We have also considered the results of public engagement research that highlighted an expectation for principles such as transparency, fairness and accountability to be included within an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance framework.\u003csup id=\"fnref:91\"\u003e\u003ca href=\"#fn:91\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 91]\u003c/a\u003e\u003c/sup\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eMerged the safety principle with security and robustness, given the significant overlap between these concepts.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eBetter reflected concepts of accountability and responsibility.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eRefined each principle’s definition and rationale.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"principle--safety-security-robustness\"\u003ePrinciple:  Safety, security, robustness\u003c/h4\u003e\n  \u003cp\u003e\u003cbr\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eDefinition and explanation\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should function in a robust, secure and safe way throughout the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle, and risks should be continually identified, assessed and managed.\u003c/p\u003e\n\n  \u003cp\u003eRegulators may need to introduce measures for regulated entities to ensure that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems are technically secure and function reliably as intended throughout their entire life cycle.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRationale for the principle\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe breadth of possible uses for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and its capacity to autonomously develop new capabilities and functions mean that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can have a significant impact on safety and security. Safety-related risks are more apparent in certain domains, such as health or critical infrastructure, but they can materialise in many areas. Safety will be a core consideration for some regulators and more marginal for others. However, it will be important for all regulators to assess the likelihood that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could pose a risk to safety in their sector or domain, and take a proportionate approach to managing it.\u003c/p\u003e\n\n  \u003cp\u003eAdditionally, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should be technically secure and should reliably function as intended and described. System developers should be aware of the specific security threats that could apply at different stages of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle and embed resilience to these threats into their systems. Other actors should remain vigilant of security issues when they interact with an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system. We anticipate that regulators may wish to consider the National Cyber Security Centre (\u003cabbr title=\"National Cyber Security Centre\"\u003eNCSC\u003c/abbr\u003e) principles for securing machine learning models when assessing whether \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e actors are adequately prioritising security.\u003csup id=\"fnref:92\"\u003e\u003ca href=\"#fn:92\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 92]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eWhen applying this principle, regulators will need to consider providing guidance in a way that is coordinated and coherent with the activities of other regulators. Regulators’ implementation of this principle may require the corresponding \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors to regularly test or carry out due diligence on the functioning, resilience and security of a system.\u003csup id=\"fnref:93\"\u003e\u003ca href=\"#fn:93\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 93]\u003c/a\u003e\u003c/sup\u003e Regulators may also need to consider technical standards addressing safety, robustness and security to benchmark the safe and robust performance of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems and to provide \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors with guidance for implementing this principle in their remit.\u003c/p\u003e\n\n  \u003ch4 id=\"principle--appropriate-transparency-and-explainability\"\u003ePrinciple:  Appropriate transparency and explainability\u003c/h4\u003e\n  \u003cp\u003e\u003cbr\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eDefinition and explanation\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should be appropriately transparent and explainable.\u003c/p\u003e\n\n  \u003cp\u003eTransparency refers to the communication of appropriate information about an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system to relevant people (for example, information on how, when, and for which purposes an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system is being used). Explainability refers to the extent to which it is possible for relevant parties to access, interpret and understand the decision-making processes of an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system.\u003csup id=\"fnref:94\"\u003e\u003ca href=\"#fn:94\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 94]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eAn appropriate level of transparency and explainability will mean that regulators have sufficient information about \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems and their associated inputs and outputs to give meaningful effect to the other principles (for example, to identify accountability). An appropriate degree of transparency and explainability should be proportionate to the risk(s) presented by an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system.\u003c/p\u003e\n\n  \u003cp\u003eRegulators may need to look for ways to support and encourage relevant life cycle actors to implement appropriate transparency measures, for example through regulatory guidance. Parties directly affected by the use of an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system should also be able to access sufficient information about \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems to be able to enforce their rights. In applying the principle to their business processes, relevant life cycle actors may be asked to provide this information in the form and manner required by regulators, including through product labelling. Technical standards could also provide useful guidance on available methods to assess, design, and improve transparency and explainability within \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems – recognising that consumers, users and regulators will require different information.\u003csup id=\"fnref:95\"\u003e\u003ca href=\"#fn:95\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 95]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRationale for the principle\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eTransparency can increase public trust,\u003csup id=\"fnref:96\"\u003e\u003ca href=\"#fn:96\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 96]\u003c/a\u003e\u003c/sup\u003e which has been shown to be a significant driver of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption.\u003csup id=\"fnref:97\"\u003e\u003ca href=\"#fn:97\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 97]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eWhen \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems are not sufficiently explainable, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e suppliers and users risk inadvertently breaking laws, infringing rights, causing harm and compromising the security of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems.\u003c/p\u003e\n\n  \u003cp\u003eAt a technical level, the explainability of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems remains an important research and development challenge. The logic and decision-making in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems cannot always be meaningfully explained in a way that is intelligible to humans, although in many settings this poses no substantial risk. It is also true that in some cases, a decision made by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e may perform no worse on explainability than a comparable decision made by a human.\u003csup id=\"fnref:98\"\u003e\u003ca href=\"#fn:98\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 98]\u003c/a\u003e\u003c/sup\u003e Future developments of the technology may pose additional challenges to achieving explainability. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should display levels of explainability that are appropriate to their context, including the level of risk and consideration of what is achievable given the state of the art.\u003c/p\u003e\n\n  \u003ch4 id=\"principle-fairness\"\u003ePrinciple: Fairness\u003c/h4\u003e\n  \u003cp\u003e\u003cbr\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eDefinition and explanation\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should not undermine the legal rights of individuals or organisations, discriminate unfairly against individuals or create unfair market outcomes. Actors involved in all stages of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle should consider definitions of fairness that are appropriate to a system’s use, outcomes and the application of relevant law.\u003c/p\u003e\n\n  \u003cp\u003eFairness is a concept embedded across many areas of law and regulation, including equality and human rights, data protection, consumer and competition law, public and common law, and rules protecting vulnerable people.\u003c/p\u003e\n\n  \u003cp\u003eRegulators may need to develop and publish descriptions and illustrations of fairness that apply to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems within their regulatory domain, and develop guidance that takes into account relevant law, regulation, technical standards,\u003csup id=\"fnref:99\"\u003e\u003ca href=\"#fn:99\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 99]\u003c/a\u003e\u003c/sup\u003e and assurance techniques.\u003c/p\u003e\n\n  \u003cp\u003eRegulators will need to ensure that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems in their domain are designed, deployed and used considering such descriptions of fairness. Where concepts of fairness are relevant in a broad range of intersecting regulatory domains, we anticipate that developing joint guidance will be a priority for regulators.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRationale for the principle\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eIn certain circumstances, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can have a significant impact on people’s lives, including insurance offers, credit scores, and recruitment outcomes. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-enabled decisions with high impact outcomes should not be arbitrary and should be justifiable.\u003c/p\u003e\n\n  \u003cp\u003eIn order to ensure a proportionate and context-specific approach regulators should be able to describe and illustrate what fairness means within their sectors and domains, and consult with other regulators where multiple remits are engaged by a specific use case. We expect that regulators’ interpretations of fairness will include consideration of compliance with relevant law and regulation, including:\u003c/p\u003e\n\n  \u003col\u003e\n    \u003cli\u003e\n      \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should not produce discriminatory outcomes, such as those which contravene the Equality Act 2010 or the Human Rights Act 1998. Use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e by public authorities should comply with the additional duties placed on them by legislation (such as the Public Sector Equality Duty).\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n      \u003cp\u003eProcessing of personal data involved in the design, training, and use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should be compliant with requirements under the UK General Data Protection Regulation (\u003cabbr title=\"UK General Data Protection Regulation\"\u003eGDPR\u003c/abbr\u003e), the Data Protection Act 2018,\u003csup id=\"fnref:100\"\u003e\u003ca href=\"#fn:100\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 100]\u003c/a\u003e\u003c/sup\u003e particularly around fair processing and solely automated decision-making.\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n      \u003cp\u003eConsumer and competition law, including rules protecting vulnerable consumers and individuals.\u003csup id=\"fnref:101\"\u003e\u003ca href=\"#fn:101\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 101]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n      \u003cp\u003eRelevant sector-specific fairness requirements, such as the Financial Conduct Authority (\u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e) Handbook.\u003c/p\u003e\n    \u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003ch4 id=\"principle-accountability-and-governance\"\u003ePrinciple: Accountability and governance\u003c/h4\u003e\n  \u003cp\u003e\u003cbr\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eDefinition and explanation\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eGovernance measures should be in place to ensure effective oversight of the supply and use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems, with clear lines of accountability established across the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors should take steps to consider, incorporate and adhere to the principles and introduce measures necessary for the effective implementation of the principles at all stages of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003c/p\u003e\n\n  \u003cp\u003eRegulators will need to look for ways to ensure that clear expectations for regulatory compliance and good practice are placed on appropriate actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chain, and may need to encourage the use of governance procedures that reliably ensure these expectations are met.\u003c/p\u003e\n\n  \u003cp\u003eRegulator guidance on this principle should reflect that ‘accountability’ refers to the expectation that organisations or individuals will adopt appropriate measures to ensure the proper functioning, throughout their life cycle, of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems that they research, design, develop, train, operate, deploy, or otherwise use.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRationale for the principle\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems can operate with a high level of autonomy, making decisions about how to achieve a certain goal or outcome in a way that has not been explicitly programmed or foreseen.\u003csup id=\"fnref:102\"\u003e\u003ca href=\"#fn:102\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 102]\u003c/a\u003e\u003c/sup\u003e Establishing clear, appropriate lines of ownership and accountability is essential for creating business certainty while ensuring regulatory compliance.\u003c/p\u003e\n\n  \u003cp\u003eDoing so for actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle is difficult, given the complexity of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chains, as well as the adaptivity, autonomy and opacity of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems. In some cases, technical standards can provide useful guidance on good practices for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance.\u003csup id=\"fnref:103\"\u003e\u003ca href=\"#fn:103\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 103]\u003c/a\u003e\u003c/sup\u003e Assurance techniques like impact assessments can help to identify potential risks early in the development life cycle, enabling their mitigation through appropriate safeguards and governance mechanisms.\u003c/p\u003e\n\n  \u003cp\u003eRegulatory guidance should also reflect the responsibilities such life cycle actors have for demonstrating proper accountability and governance (for example, by providing documentation on key decisions throughout the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system life cycle, conducting impact assessments or allowing audits where appropriate).\u003c/p\u003e\n\n  \u003ch4 id=\"principle-contestability-and-redress\"\u003ePrinciple: Contestability and redress\u003c/h4\u003e\n  \u003cp\u003e\u003cbr\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eDefinition and explanation\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eWhere appropriate, users, impacted third parties and actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle should be able to contest an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e decision or outcome that is harmful or creates material risk of harm.\u003c/p\u003e\n\n  \u003cp\u003eRegulators will be expected to clarify existing routes to contestability and redress, and implement proportionate measures to ensure that the outcomes of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use are contestable where appropriate.\u003c/p\u003e\n\n  \u003cp\u003eWe would also expect regulators to encourage and guide regulated entities to make clear routes (including informal channels) easily available and accessible, so affected parties can contest harmful \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e outcomes or decisions as needed.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eRationale for the principle\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies can result in different types of harm and can have a material impact on people’s lives. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems’ outcomes may introduce risks such as the reproduction of biases or safety concerns.\u003c/p\u003e\n\n  \u003cp\u003ePeople and organisations should be able to contest outcomes where existing rights have been violated or they have been harmed.\u003c/p\u003e\n\n  \u003cp\u003eIt will be important for regulators to provide clear guidance on this principle so that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors can implement it in practice. This should include clarifying that appropriate transparency and explainability are relevant to good implementation of this contestability and redress principle.\u003c/p\u003e\n\n  \u003cp\u003eThe UK’s initial non-statutory approach will not create new rights or new routes to redress at this stage.\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e53. We anticipate that regulators will need to issue guidance on the principles or update existing guidance to provide clarity to business. Regulators may also publish joint guidance on one or more of the principles, focused on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases that cross multiple regulatory remits. We are keen to work with regulators and industry to understand the best approach to providing guidance. We expect that practical guidance will support actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle to adhere to the principles and embed them into their technical and operational business processes. Regulators may also use alternative measures and introduce other tools or resources, in addition to issuing guidance, within their existing remits and powers to implement the principles.\u003c/p\u003e\u003cp\u003e54. Government will monitor the overall effectiveness of the principles and the wider impact of the framework.\u003csup id=\"fnref:104\"\u003e\u003ca href=\"#fn:104\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 104]\u003c/a\u003e\u003c/sup\u003e This will include working with regulators to understand how the principles are being applied and whether the framework is adequately supporting innovation.\u003c/p\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e1. Do you agree that requiring organisations to make it clear when they are using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would improve transparency?\u003c/p\u003e\n\n  \u003cp\u003e2. Are there other measures we could require of organisations to improve \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e transparency?\u003c/p\u003e\n\n  \u003cp\u003e3. Do you agree that current routes to contest or get redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms are adequate?\u003c/p\u003e\n\n  \u003cp\u003e4. How could current routes to contest or seek redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms be improved, if at all?\u003c/p\u003e\n\n  \u003cp\u003e5. Do you agree that, when implemented effectively, the revised cross-sectoral principles will cover the risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies?\u003c/p\u003e\n\n  \u003cp\u003e6. What, if anything, is missing from the revised principles?\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-34-explainable-ai-in-practice\"\u003eCase study 3.4: Explainable \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in practice\u003c/h4\u003e\n\n  \u003cp\u003eThe level of explainability needed from an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system is highly specific to its context, including the extent to which an application is safety-critical. The level and type of explainability required will likely vary depending on whether the intended audience of the explanation is a regulator, technical expert, or lay person.\u003c/p\u003e\n\n  \u003cp\u003eFor example, a technical expert designing self-driving vehicles would need to understand the system’s decision-making capabilities to test, assess and refine them. In the same context, a lay person may need to understand the decision-making process only in order to use the vehicle safely. If the vehicle malfunctioned and caused a harmful outcome,\u003csup id=\"fnref:105\"\u003e\u003ca href=\"#fn:105\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 105]\u003c/a\u003e\u003c/sup\u003e a regulator may need information about how the system operates in order to allocate responsibility – similar to the level of explainability currently needed to hold human drivers accountable.\u003c/p\u003e\n\n  \u003cp\u003eWhile \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e explainability remains a technical challenge and an area of active research, regulators are already conducting work to address it. In 2021, the \u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e and the Alan Turing Institute issued co-developed guidance on explaining decisions made with \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e,\u003csup id=\"fnref:106\"\u003e\u003ca href=\"#fn:106\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 106]\u003c/a\u003e\u003c/sup\u003e giving organisations practical advice to help explain the processes, services and decisions delivered or assisted by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e to the individuals affected by them.\u003c/p\u003e\n\n  \u003cp\u003eThe audience for an explanation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s outcomes will often be a regulator, who may require a higher standard of explainability depending on the risks represented by an application. The \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e’s Project Glass Box work is addressing the challenge of setting medical device requirements that take into account adequate consideration of human interpretability and its consequences for the safety and effectiveness for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e used in medical devices.\u003csup id=\"fnref:107\"\u003e\u003ca href=\"#fn:107\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 107]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-35-what-the-principles-mean-for-businesses-in-practice\"\u003eCase study 3.5: What the principles mean for businesses in practice\u003c/h4\u003e\n\n  \u003cp\u003eA fictional company, ‘Good \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Recruitment Limited’, provides recruitment services that use a range of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems to accelerate the recruitment process, including a service that automatically shortlists candidates based on application forms. While potentially useful, such systems may discriminate against certain groups that have historically not been selected for certain positions.\u003c/p\u003e\n\n  \u003cp\u003eAfter the implementation of the UK’s new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework, the Equality and Human Rights Commission (\u003cabbr title=\"Equality and Human Rights Commission\"\u003eEHRC\u003c/abbr\u003e) and the Information Commissioner Office (\u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e) will be supported and encouraged to work with the Employment Agency Standards Inspectorate (\u003cabbr title=\"Employment Agency Standards Inspectorate\"\u003eEASI\u003c/abbr\u003e) and other regulators and organisations in the employment sector to issue joint guidance. The joint guidance could address the cross-cutting principles relating to fairness, appropriate transparency and explainability, and contestability and redress in the context of the use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems in recruitment or employment. Such joint guidance could, for example, make things clearer and easier for Good \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Recruitment Limited by:\u003c/p\u003e\n\n  \u003col\u003e\n    \u003cli\u003eClarifying the type of information businesses should provide when implementing such systems\u003c/li\u003e\n    \u003cli\u003eIdentifying appropriate supply chain management processes such as due diligence or \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e impact assessments\u003c/li\u003e\n    \u003cli\u003eSuggesting proportionate measures for bias detection, mitigation and monitoring\u003c/li\u003e\n    \u003cli\u003eProviding suggestions for the provision of contestability and redress routes.\u003c/li\u003e\n  \u003c/ol\u003e\n\n  \u003cp\u003eGood \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Recruitment Limited would also be able to apply a variety of tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, such as technical standards, that would supplement regulatory guidance and other measures promoted by regulators. In their published guidance regulators could, where appropriate, refer businesses to existing technical standards on transparency (such as \u003ca rel=\"external\" href=\"https://standards.ieee.org/ieee/7001/6929/\"\u003eIEEE 7001-2021\u003c/a\u003e), as well as standards on bias mitigation (such as \u003ca rel=\"external\" href=\"https://www.iso.org/standard/77607.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 24027:2021\u003c/a\u003e).\u003c/p\u003e\n\n  \u003cp\u003eBy following this guidance Good \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Recruitment Limited would be able to develop and deploy their services responsibly.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"section324\"\u003e3.2.4 Our preferred model for applying the principles\u003c/h3\u003e\u003cp\u003e55. Initially, the principles will be issued by government on a non-statutory basis and applied by regulators within their remits. We will support regulators to apply the principles using the powers and resources available to them. This initial period of implementation will provide a valuable opportunity to ensure that the principles are effective and that the wider framework is supporting innovation while addressing risks appropriately.\u003c/p\u003e\u003cp\u003e56. While industry has strongly supported non-statutory measures in the first instance, favouring flexibility and fewer burdens, some businesses and regulators have suggested that government should go beyond a non-statutory approach to ensure the principles have the desired impact.\u003csup id=\"fnref:108\"\u003e\u003ca href=\"#fn:108\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 108]\u003c/a\u003e\u003c/sup\u003e Some regulators have also expressed concerns that they lack the statutory basis to consider the application of the principles. We are committed to an approach that leverages collaboration with our expert regulators but we agree that we may need to intervene further to ensure that our framework is effective.\u003c/p\u003e\u003cp\u003e57. Following a period of non-statutory implementation, and when parliamentary time allows, we anticipate that we will want to strengthen and clarify regulators’ mandates by introducing a new duty requiring them to have due regard to the principles. Such a duty would give a clear signal that we expect regulators to act and support coherence across the regulatory landscape, ensuring that the framework displays the characteristics that we have identified.\u003csup id=\"fnref:109\"\u003e\u003ca href=\"#fn:109\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 109]\u003c/a\u003e\u003c/sup\u003e One of the strengths of this approach is that regulators would still be able to exercise discretion and expert judgement regarding the relevance of each principle to their individual domains.\u003c/p\u003e\u003cp\u003e58. A duty would ensure that regulators retain the ability to exercise judgement when applying the principles in particular contexts – benefiting from some of the flexibility expected through non-statutory implementation. For example, while the duty to have due regard would require regulators to demonstrate that they had taken account of the principles, it may be the case that not every regulator will need to introduce measures to implement every principle. In having due regard to a particular principle, a regulator may exercise their expert judgement and determine that their sector or domain does not require action to be taken. The introduction of the duty will, however, give regulators a clear mandate and incentive to apply the principles where relevant to their sectors or domains.\u003c/p\u003e\u003cp\u003e59. If our monitoring of the effectiveness of the initial, non-statutory framework suggests that a statutory duty is unnecessary, we would not introduce it. Similarly, we will monitor whether particular principles cannot be, or are not being, applied in certain circumstances or by specific regulators because of the interpretation of existing legal requirements or because of technical constraints. Such circumstances may require broader legislative changes. Should we decide there is a need for statutory measures, we will work with regulators to review the interaction of our principles with their existing duties and powers.\u003c/p\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e7. Do you agree that introducing a statutory duty on regulators to have due regard to the principles would clarify and strengthen regulators’ mandates to implement our principles while retaining a flexible approach to implementation?\u003c/p\u003e\n\n  \u003cp\u003e8. Is there an alternative statutory intervention that would be more effective?\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"the-role-of-individual-regulators-in-applying-the-principles\"\u003e3.2.5 The role of individual regulators in applying the principles\u003c/h3\u003e\u003cp\u003e60. In some sectors, principles for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance will already exist and may even go further than the cross-cutting principles we propose. Our framework gives sectors the ability to develop and apply more specific principles to suit their own domains, where government or regulators identify these are needed.\u003c/p\u003e\u003cp\u003e61. The Ministry of Defence published its own \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ethical principles and policy in June 2022, which determines HM Government’s approach regarding \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-enabled military capabilities. We will ensure appropriate coherence and alignment in the application of this policy through a context specific approach and thereby promote UK leadership in the employment of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e for defence purposes. Ahead of introducing any statutory duty to have due regard to our principles, and in advance of introducing other material iterations of the framework, we will consider whether exemptions are needed to allow existing regulators (such as those working in areas like national security) to continue their domain-level approach\u003c/p\u003e\u003cp\u003e62. Not all principles will be equally relevant in all contexts and sometimes two or more principles may come into conflict. For example, it may be difficult to assess the fairness of an algorithm’s outputs without access to sensitive personal data about the subjects of the processing. Regulators will need to use their expertise and judgement to prioritise and apply the principles in such cases, sharing information where possible with government and other regulators about how they are assessing the relevance of each principle. This collaboration between regulators and government will allow the framework to be adapted to ensure it is practical, coherent and supporting innovation.\u003c/p\u003e\u003cp\u003e63. In implementing the new regulatory framework we expect that regulators will:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eAssess the cross-cutting principles and apply them to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases that fall within their remit.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eIssue relevant guidance on how the principles interact with existing legislation to support industry to apply the principles. Such guidance should also explain and illustrate what compliance looks like.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eSupport businesses operating within the remits of multiple regulators by collaborating and producing clear and consistent guidance, including joint guidance where appropriate.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e64. Regulators will need to monitor and evaluate their own implementation of the framework and their own effectiveness at regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e within their remits. We understand that there may be \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks that do not clearly fall within the remits of the UK’s existing regulators.\u003csup id=\"fnref:110\"\u003e\u003ca href=\"#fn:110\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 110]\u003c/a\u003e\u003c/sup\u003e Not every new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risk will require a regulatory response and there is a growing ecosystem of tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e that can support the application of the cross-cutting principles. These are described further in \u003ca href=\"#partfour\"\u003epart 4\u003c/a\u003e.\u003c/p\u003e\u003cp\u003e65. Where prioritised risks fall within a gap in the legal landscape, regulators will need to collaborate with government to identify potential actions. This may include identifying iterations to the framework such as changes to regulators’ remits, updates to the Regulators’ Code,\u003csup id=\"fnref:111\"\u003e\u003ca href=\"#fn:111\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 111]\u003c/a\u003e\u003c/sup\u003e or additional legislative interventions. Our approach benefits from our strong sovereign parliamentary system, which reliably allows for the introduction of targeted and proportionate measures in response to emerging issues, including by adapting existing legislation if necessary.\u003csup id=\"fnref:112\"\u003e\u003ca href=\"#fn:112\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 112]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e66. The Sir Patrick Vallance review has highlighted that rushed attempts to regulate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e too early would risk stifling innovation.\u003csup id=\"fnref:113\"\u003e\u003ca href=\"#fn:113\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 113]\u003c/a\u003e\u003c/sup\u003e Our approach aligns with this perspective. We recognise the need to build a stronger evidence base before making decisions on statutory interventions. In doing so, we will ensure that we strike the right balance between retaining flexibility in our iterative approach and providing clarity to businesses. As detailed in \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e, we will deliver a range of central functions, including horizon scanning and risk monitoring, to identify and respond to situations where prioritised risks are not adequately covered by the framework, or where gaps between regulators’ remits are negatively impacting innovation.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-36-responding-to-regulatory-policy-challenges--self-driving-vehicles\"\u003eCase study 3.6: Responding to regulatory policy challenges – self-driving vehicles\u003c/h4\u003e\n\n  \u003cp\u003eSome aspects of a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use case may sit outside regulators’ existing remits, meaning they do not have a mandate to address specific harms or support a new product to enter the market.\u003c/p\u003e\n\n  \u003cp\u003eThe advent of self-driving vehicles highlighted such a regulatory and policy challenge. Where sophisticated \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-enabled software is capable of performing the designated driving task, existing regulatory structures – where responsibility for road safety is achieved by licensing human drivers – are not fit for purpose. This creates uncertainty regarding the development and deployment of self-driving vehicles that cannot be addressed by regulators alone.\u003c/p\u003e\n\n  \u003cp\u003eTo achieve the government’s ambition to ‘make the UK one of the best places in the world to develop and deploy self-driving vehicles technology’,\u003csup id=\"fnref:114\"\u003e\u003ca href=\"#fn:114\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 114]\u003c/a\u003e\u003c/sup\u003e manufacturers need clarity about the regulatory landscape they are operating in and the general public needs to have confidence in the safety, fairness and trustworthiness of these vehicles.\u003c/p\u003e\n\n  \u003cp\u003eThe government published its Connected \u0026amp; Automated Mobility 2025 report\u003csup id=\"fnref:115\"\u003e\u003ca href=\"#fn:115\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 115]\u003c/a\u003e\u003c/sup\u003e to address this challenge, describing how the ecosystem could be adapted to spur innovation and secure the economic and social benefits of this technology.\u003c/p\u003e\n\n  \u003cp\u003eThe work of the UK’s Centre for Connected and Autonomous Vehicles is an example of government acting to identify regulatory gaps, develop policy and build UK capabilities. A central monitoring and evaluation function, described below, will identify and assess gaps in the regulatory ecosystem that could stifle \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation so that government can take action to address them.\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"guidance-to-regulators-on-applying-the-principles\"\u003e3.2.6 Guidance to regulators on applying the principles\u003c/h3\u003e\u003cp\u003e67. The proposed regulatory framework is dependent upon the implementation of the principles by our expert regulators. This regulator-led approach has received broad support from across industry, with stakeholders acknowledging the importance of the sector-specific expertise held by individual regulators. We expect regulators to collaborate proactively to achieve the best outcomes for the economy and society. We will work with regulators to monitor the wider framework and ensure that this collaborative approach to implementation is effective. If improvements are needed, including interventions to drive stronger collaboration across regulators, we will take further action.\u003c/p\u003e\u003cp\u003e68. Our engagement with regulators and industry highlighted the need for central government to support regulators. We will work with regulators to develop guidance that helps them implement the principles in a way that aligns with our expectations for how the framework should operate. Existing legal frameworks already mandate and guide regulators’ actions. For example, nearly all regulators are bound by the Regulators’ Code\u003csup id=\"fnref:116\"\u003e\u003ca href=\"#fn:116\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 116]\u003c/a\u003e\u003c/sup\u003e and all regulators – as public bodies – are required to comply with the Human Rights Act.\u003csup id=\"fnref:117\"\u003e\u003ca href=\"#fn:117\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 117]\u003c/a\u003e\u003c/sup\u003e Our proposed guidance to regulators will seek to ensure that when applying the principles, regulators are supported and encouraged to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eAdopt a proportionate approach that promotes growth and innovation by focusing on the risks that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e poses in a particular context.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eConsider proportionate measures to address prioritised risks, taking into account cross-cutting risk assessments undertaken by, or on behalf of, government.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eDesign, implement and enforce appropriate regulatory requirements and, where possible, integrate delivery of the principles into existing monitoring, investigation and enforcement processes.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eDevelop joint guidance, where appropriate, to support industry compliance with the principles and relevant regulatory requirements.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eConsider how tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e like assurance techniques and technical standards can support regulatory compliance.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eEngage proactively and collaboratively with government’s monitoring and evaluation of the framework.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-37-what-this-means-for-businesses\"\u003eCase study 3.7: What this means for businesses\u003c/h4\u003e\n\n  \u003cp\u003eA fictional company, ‘\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited’, has delayed the deployment of a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e application as – under the current patchwork of relevant regulatory requirements – it has been challenging to identify appropriate compliance actions for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-driven insurance products.\u003c/p\u003e\n\n  \u003cp\u003eFollowing implementation of the UK’s new pro-innovation framework to regulate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, we could expect to see joint guidance produced collaboratively by the Information Commissioner’s Office (\u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e), Equality and Human Rights Commission (\u003cabbr title=\"Equality and Human Rights Commission\"\u003eEHRC\u003c/abbr\u003e), Financial Conduct Authority (\u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e) and other relevant regulatory authorities. This would provide greater clarity on the regulatory requirements relevant to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as well as guidance on how to satisfy those requirements in the context of insurance and consumer-facing financial services.\u003c/p\u003e\n\n  \u003cp\u003eUnder the proposed regulatory framework, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited could be supported by new or updated guidance issued by regulators to address the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory principles. The company may also be able to follow joint regulatory guidance, issued as a result of collaboration between regulators, and use a set of tools provided by regulators, such as template risk assessments and transparency measures, and relevant technical standards (for example, international standards for transparency and bias mitigation). The collaboration between regulators and focus on practical implementation measures will guide the responsible deployment of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Fairness Insurance Limited’s \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e product by making it easier for the company to navigate the regulatory landscape and address specific risks such as discrimination.\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e69. Further details about the implementation of the regulatory framework will be provided through an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation roadmap which will be published alongside the government response to the consultation on this white paper.\u003c/p\u003e\u003ch3 id=\"section331\"\u003e3.3.1 New central functions to support the framework\u003c/h3\u003e\u003cp\u003e70. Government has a responsibility to make sure the regulatory framework operates proportionately and supports innovation. Feedback to our proposals from businesses has been clear that the current patchwork of regulation, with relatively little in the way of central coordination or oversight, will create a growing barrier to innovation if left unaddressed. Responses from over 130 organisations and individuals to our 2022 policy paper highlighted the need for a greater level of monitoring and coordination to achieve the coherence and improved clarity we need to support innovation. Businesses, particularly small to medium sized enterprises, noted that regulatory coordination could improve business certainty and investment, resulting in more and better jobs in the sector.\u003c/p\u003e\u003cp\u003e71. Government therefore intends to put mechanisms in place to coordinate, monitor and adapt the framework as a whole. Further detail on these functions is set out below. Enhanced monitoring activity will allow us to take a structured approach to gathering feedback from industry on the impact of our regime as it is introduced. These mechanisms will supplement and support the work of regulators, without undermining their independence. Equally, such mechanisms are not intended to duplicate existing activities.\u003c/p\u003e\u003cp\u003e72. Delivering some functions centrally provides government with an overarching view of how the framework is working, where it is effective and where it may need improving. A central suite of functions will also facilitate collaboration by bringing together a wide range of interested parties, including regulators, international partners, industry, civil society organisations such as trade unions and advocacy groups, academia and the general public. Our engagement with these groups has highlighted the need for our proposed central functions. We will continue to convene a wide range of stakeholders to ensure that we hear the full spectrum of viewpoints. This breadth of engagement and collaboration will be integral to government’s ability to monitor and improve the framework. The functions will identify and support opportunities for further coordination between regulators, resulting in greater clarity for businesses and stronger consumer trust.\u003c/p\u003e\u003cp\u003e73. We have identified a set of functions that will drive regulatory coherence and support regulators to implement the pro-innovation approach that we have outlined. These functions have been informed by our discussions with industry, research organisations, and regulators following the publication of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e policy paper.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box31\"\u003eBox 3.1: Functions required to support implementation of the framework\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n\u003cstrong\u003eMonitoring, assessment and feedback\u003c/strong\u003e\u003csup id=\"fnref:118\"\u003e\u003ca href=\"#fn:118\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 118]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eDevelop and maintain a central monitoring and evaluation (\u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e) framework to assess cross-economy and sector-specific impacts of the new regime.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eEnsure appropriate data is gathered from relevant sources – for example, from industry, regulators, government and civil society – and considered as part of the overall assessment of the effectiveness of the framework.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eSupport and equip regulators to undertake internal \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e and find ways to support regulators’ contributions to the central \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eMonitor the regime’s overall effectiveness including the extent to which it is proportionate and supporting innovation.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eProvide advice to ministers on issues that may need to be addressed to improve the regime, including where additional intervention may be required to ensure that the framework remains effective as the capability of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and the state of the art develops.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eThis function is at the heart of our iterative approach. We need to know whether the framework is working – for example, whether it is able to respond to and mitigate prioritised risks and whether the framework is actively supporting innovation – and we need the ability to spot issues quickly so we can adapt the framework in response.\u003c/p\u003e\n\n    \u003cp\u003e\u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e needs to be undertaken centrally to determine whether the regime as a whole is delivering against our objectives. \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e will assess whether our regime is operating in a way that is pro-innovation, clear, proportionate, adaptable, trustworthy and collaborative.\u003c/p\u003e\n\n    \u003cp\u003eOur engagement with industry, regulators, and civil society has shown us the importance of establishing a feedback loop to measure the effectiveness of the framework. We will ensure mechanisms are in place to gather evidence and insights to inform policy design.\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cp\u003e\u003cstrong\u003eSupport coherent implementation of the principles\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eDevelop and maintain central regulatory guidance to support regulators in the implementation of the principles.\u003c/li\u003e\n      \u003cli\u003eIdentify barriers that prevent regulators from effectively implementing the principles, such as:\n        \u003cul\u003e\n          \u003cli\u003eScope of regulatory remit.\u003c/li\u003e\n          \u003cli\u003eInsufficient regulatory powers.\u003c/li\u003e\n          \u003cli\u003eInsufficient regulatory capabilities.\u003c/li\u003e\n        \u003c/ul\u003e\n      \u003c/li\u003e\n      \u003cli\u003eIdentify conflicts or inconsistencies in the way the principles are interpreted across regulatory remits, and assess the impact this is having on innovation. Some variation across regulators’ approaches to implementation is to be expected and encouraged, given the context-based approach that we are taking.\u003c/li\u003e\n      \u003cli\u003eWork with regulators to resolve discrepancies that are having a significant impact on innovation, and share learning and best practice.\u003c/li\u003e\n      \u003cli\u003eMonitor and assess the ongoing relevance of the principles themselves.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eBusinesses have noted that, within a context-based regulatory framework, an appropriate degree of central leadership is needed to ensure coherence. To be effective, this function must be performed centrally, as the whole regulatory landscape needs to be considered to:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eEnsure that, as far as necessary to support innovation, regulators interpret and implement the principles in a coherent way.\u003c/li\u003e\n      \u003cli\u003eEffectively monitor how well the principles are being implemented, as well as their ongoing relevance.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003eThis function will play a central part in delivering a regulatory regime that is:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eClear: by making it easier for businesses working across regulatory remits (for example, by supporting the development of joint regulatory guidance where appropriate).\u003c/li\u003e\n      \u003cli\u003eProportionate and pro-innovation: as it allows government to find and prevent any application of the principles that has a disproportionate or harmful impact on innovation.\u003c/li\u003e\n      \u003cli\u003eAdaptable and trustworthy: as it forms part of the feedback loop established by the \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function to understand how well the regime operates and whether it should be changed.\u003c/li\u003e\n      \u003cli\u003eCollaborative: by encouraging cross-government cooperation aligned to the principles.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/div\u003e\n\n  \u003cp\u003e\u003cstrong\u003eCross-sectoral risk assessment\u003c/strong\u003e\u003csup id=\"fnref:119\"\u003e\u003ca href=\"#fn:119\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 119]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eDevelop and maintain a cross-economy, society-wide \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk register to support regulators’ internal risks assessments.\u003c/li\u003e\n      \u003cli\u003eMonitor, review and re-prioritise known risks.\u003c/li\u003e\n      \u003cli\u003eIdentify and prioritise new and emerging risks (working with the horizon scanning function).\u003c/li\u003e\n      \u003cli\u003eWork with regulators to clarify responsibilities in relation to new risks or areas of contested responsibility.\u003c/li\u003e\n      \u003cli\u003eSupport join-up between regulators on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks that cut across remits and facilitate issuing of joint guidance where appropriate.\u003c/li\u003e\n      \u003cli\u003eIdentify where risks are not adequately covered.\u003c/li\u003e\n      \u003cli\u003eShare risk enforcement best practices.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eStakeholders have emphasised that a cross-sectoral assessment of risk is required to ensure that any new risks can be addressed and do not fall in any gaps between regulator remits. To be effective, this function must be performed centrally, as risks need to be considered across the whole economy to:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eEncourage regulators to take a coherent approach to assessing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks.\u003c/li\u003e\n      \u003cli\u003eEnsure risks do not fall between regulatory gaps and that appropriate action is taken where cross-sector risks do not have an obvious ‘home’ within a single regulatory remit.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003eA centrally delivered risk function will ensure that the framework’s approach to risk is informed by a cross-sector, holistic viewpoint. A cross-cutting approach to risk allows a proportionate but effective response.\u003c/p\u003e\n\n    \u003cp\u003eThis function will play a central part in delivering a regulatory regime that is:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eClear: by making it easier for businesses working across sectors.\u003c/li\u003e\n      \u003cli\u003eProportionate: by ensuring an appropriate response to cross-sector risks.\u003c/li\u003e\n      \u003cli\u003eTrustworthy: by making sure priority \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks are being addressed.\u003c/li\u003e\n      \u003cli\u003eCollaborative: by allowing important actors – such as frontier researchers, civil society, international partners and regulators – to be convened to engage in focused, prioritised discussions on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/div\u003e\n\n  \u003cp\u003e\u003cstrong\u003eSupport for innovators (including testbeds and sandboxes as detailed in \u003ca href=\"#section334\"\u003esection 3.3.4\u003c/a\u003e)\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eRemove barriers to innovation by assisting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators to navigate regulatory complexity and get their product to market while minimising legal and compliance risk (drawing on the expertise of all relevant regulators).\u003c/li\u003e\n      \u003cli\u003eIdentify cross-cutting regulatory issues that are having real-world impacts and stifling innovation, and identify opportunities for improvement to our regulatory framework.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eWe want to make it easy for innovators to navigate the regulatory landscape. Businesses have noted that tools such as regulatory sandboxes can help innovators to navigate the regulatory landscape. Central commissioning or delivery of the sandbox or testbed will also enable information and insights generated from this work to directly inform our implementation of the overall regulatory framework.\u003c/p\u003e\n\n    \u003cp\u003eThis function will play a central part in delivering a regulatory regime that is:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eClear: by making it easier for businesses working across sectors.\u003c/li\u003e\n      \u003cli\u003eAdaptable and trustworthy: as it forms an important part of the feedback loop to understand how well the regime is functioning and how it should iterate.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003eTo support innovators, we will take forward Sir Patrick Vallance’s recommendation for a multi-regulator \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandbox to be established\u003csup id=\"fnref:120\"\u003e\u003ca href=\"#fn:120\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 120]\u003c/a\u003e\u003c/sup\u003e (see \u003ca href=\"#section334\"\u003esection 3.3.4\u003c/a\u003e for more detail).\u003c/p\u003e\n  \u003c/div\u003e\n\n  \u003cp\u003e\u003cstrong\u003eEducation and awareness\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eProvide guidance to businesses seeking to navigate the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory landscape.\u003c/li\u003e\n      \u003cli\u003eRaise awareness and provide guidance to consumers and the general public to ensure that these groups are empowered and encouraged to engage with the ongoing monitoring and iteration of the framework.\u003c/li\u003e\n      \u003cli\u003eEncourage regulators to promote awareness campaigns to educate consumers and users on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation and risks.\u003csup id=\"fnref:121\"\u003e\u003ca href=\"#fn:121\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 121]\u003c/a\u003e\u003c/sup\u003e\n\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eTo be effective, this function must be performed centrally, as the whole regulatory landscape needs to be considered to provide useful guidance to businesses and consumers on navigating it. This will ensure that businesses and consumers are able to contribute to the monitoring and evaluation of the framework and its ongoing iteration.\u003c/p\u003e\n\n    \u003cp\u003eThis function will help deliver a regulatory regime that is:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eClear: by helping businesses working across sectors to navigate the regulatory landscape.\u003c/li\u003e\n      \u003cli\u003eTrustworthy: by increasing awareness of the framework and its requirements among consumers and businesses.\u003c/li\u003e\n      \u003cli\u003eCollaborative: by educating and raising awareness to empower businesses and consumers to participate in the ongoing evaluation and iteration of the framework.\u003c/li\u003e\n      \u003cli\u003ePro-innovation: by enhancing trust, which is shown to increase \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/div\u003e\n\n  \u003cp\u003e\u003cstrong\u003eHorizon scanning\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eMonitor emerging trends and opportunities in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e development to ensure that the framework can respond to them effectively.\u003c/li\u003e\n      \u003cli\u003eProactively convene industry, frontier researchers, academia and other key stakeholders to establish how the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework could support the UK’s \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem to maximise the benefits of emerging opportunities whilst continuing to take a proportionate approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk.\u003c/li\u003e\n      \u003cli\u003eSupport the risk assessment function to identify and prioritise new and emerging \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks, working collaboratively with industry, academia, global partners, and regulators.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eThis function will support horizon-scanning activities in individual regulators but a central function is also necessary. As stakeholders have highlighted, an economy-wide view is required to anticipate opportunities that emerge across the landscape, particularly those that cut across regulatory remits or fall in the gaps between them.\u003c/p\u003e\n\n    \u003cp\u003eThis function will help deliver a regulatory regime that is:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eAdaptable: by identifying emerging trends to enable intelligent, coordinated adaptation of the regulatory framework.\u003c/li\u003e\n      \u003cli\u003eCollaborative: by convening partners including frontier researchers, industry, civil society, international partners and regulators, to identify emerging trends.\u003c/li\u003e\n      \u003cli\u003eTrustworthy: by ensuring that our regulatory framework is able to adapt in the face of emerging trends\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/div\u003e\n\n  \u003cp\u003e\u003cstrong\u003eEnsure interoperability with international regulatory frameworks\u003c/strong\u003e\u003c/p\u003e\n  \u003cdiv class=\"example\"\u003e\n    \u003cp\u003e\u003cstrong\u003eActivities\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003eMonitor alignment between UK principles and international approaches to regulation, assurance and/or risk management, and technical standards.\u003c/li\u003e\n      \u003cli\u003eSupport cross-border coordination and collaboration by identifying opportunities for regulatory interoperability.\u003c/li\u003e\n    \u003c/ul\u003e\n\n    \u003cp\u003e\u003cstrong\u003eRationale\u003c/strong\u003e\u003c/p\u003e\n\n    \u003cp\u003eTo be effective, this function must be performed centrally. The whole regulatory landscape needs to be considered to understand how well the UK framework aligns with international jurisdictions. The impact of international alignment on innovation and adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in the UK is a key concern for businesses. The central oversight and monitoring of the global alignment of the framework will support UK engagement with like-minded international partners on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation, building our influence in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\n\n    \u003cp\u003eThis function will play a central part in delivering a regulatory regime that is:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003ePro-innovation: by ensuring that UK innovators can trade internationally and UK companies can attract overseas investment.\u003c/li\u003e\n      \u003cli\u003eCollaborative: by fostering close cooperation with international partners.\u003c/li\u003e\n      \u003cli\u003eProportionate: by making sure the framework is sufficiently aligned with international approaches to maximise market access and business opportunities without imposing unnecessary burdens that could stifle innovation or otherwise negatively impact on international trade and/or investment in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in the UK.\u003c/li\u003e\n      \u003cli\u003eAdaptable: as it forms an important part of the feedback loop to understand how well the regime is functioning and how it should iterate.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/div\u003e\n\u003c/div\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e9. Do you agree that the functions outlined in \u003ca href=\"#box31\"\u003eBox 3.1\u003c/a\u003e would benefit our \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation framework if delivered centrally?\u003c/p\u003e\n\n  \u003cp\u003e10. What, if anything, is missing from the central functions?\u003c/p\u003e\n\n  \u003cp\u003e11. Do you know of any existing organisations who should deliver one or more of our proposed central functions?\u003c/p\u003e\n\n  \u003cp\u003e12. Are there additional activities that would help businesses confidently innovate and use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies?\u003c/p\u003e\n\n  \u003cp\u003e12.1. If so, should these activities be delivered by government, regulators or a different organisation?\u003c/p\u003e\n\n  \u003cp\u003e13. Are there additional activities that would help individuals and consumers confidently use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies?\u003c/p\u003e\n\n  \u003cp\u003e13.1. If so, should these activities be delivered by government, regulators or a different organisation?\u003c/p\u003e\n\n  \u003cp\u003e14. How can we avoid overlapping, duplicative or contradictory guidance on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e issued by different regulators?\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box-32-supporting-coherence-in-risk-assessment\"\u003eBox 3.2: Supporting coherence in risk assessment\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n\u003cstrong\u003eWhy?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eMany \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks do not fall neatly into the remit of one individual regulator and they could go unaddressed if not monitored at a cross-sector level. A central, cross-economy risk function will also enable government to monitor future risks in a rigorous, coherent and balanced way. This will include ‘high impact but low probability’ risks such as existential risks posed by artificial general intelligence or \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e biosecurity risks.\u003c/p\u003e\n\n  \u003cp\u003eA pro-innovation approach to regulation involves tolerating a certain degree of risk rather than intervening in all cases. Government needs the ability to assess and prioritise \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks, ensuring that any intervention is proportionate and consistent with levels of risk mitigation activity elsewhere across the economy or \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003c/p\u003e\n\n  \u003cp\u003eEstablishing a central risk function will bring coherence to the way regulators and industry think about \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk. It will also foster collaboration between government, regulators, industry and civil society to provide clarity for businesses managing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk across sectors.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eWhat?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe central risk function will identify, assess, prioritise and monitor cross-cutting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks that may require government intervention.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eHow?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe central risk function will bring together cutting-edge knowledge from industry, regulators, academia and civil society – including skilled computer scientists with a deep technical understanding of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\n\n  \u003cp\u003eGiven the importance of risk management expertise, we will seek inspiration and learning from sectors where operational risk management is highly developed. This will include looking for examples of how failures and near misses can be recorded and used to inform good practice.\u003c/p\u003e\n\n  \u003cp\u003eRegulators will have a key role in designing the central risk framework and ensuring alignment with their existing practices. Where a risk that has been prioritised for intervention falls outside of any existing regulator’s remit, the central risk function will identify measures that could be taken to address the gap (for example, updates to regulatory remits). The central risk function will also support smaller regulators that lack technical \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e expertise to better understand \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks.\u003c/p\u003e\n\u003c/div\u003e\u003ch4 id=\"figure-2-central-risks-function-activities\"\u003eFigure 2: Central risks function activities\u003c/h4\u003e\u003cfigure class=\"image embedded\"\u003e\u003cdiv class=\"img\"\u003e\u003cimg src=\"https://assets.publishing.service.gov.uk/media/6422fec960a35e000c0cafb0/Diagram03-blackText__1_.svg\" alt=\"\"\u003e\u003c/div\u003e\n\u003cfigcaption\u003e\u003cp\u003eCentral risk function activities\u003c/p\u003e\u003c/figcaption\u003e\u003c/figure\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box3-3\"\u003eBox 3.3: How a central monitoring and evaluation (\u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e) function enables a proportionate, adaptable approach\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n\u003cstrong\u003eWhy?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eWe will need to monitor the implementation of the framework closely to make sure that it is working as designed. We will monitor the regime to ensure it is pro-innovation, proportionate, adaptable, trustworthy, clear and collaborative – our desired characteristics.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eWhat?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe central \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function will gather evidence and feedback from a range of sources and actors in the ecosystem. For example, effective \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e of the whole framework is likely to require input and data from industry, regulators, civil society, academia, international partners and the general public. Insights from regulatory sandboxes and testbeds as well as wider monitoring of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem as a whole will also be valuable.\u003csup id=\"fnref:122\"\u003e\u003ca href=\"#fn:122\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 122]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eGovernment’s ability to access reliable, comprehensive data and insights for the purposes of monitoring the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework will be closely related to our work raising awareness and educating businesses and consumers on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related issues. It is important for our \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e data to be drawn from a wide range of sources, reflecting the full spectrum of views and including seldom heard voices from the general public. Raising awareness and educating stakeholder groups will help to ensure that the broader conversation is inclusive, informed and rigorous.\u003c/p\u003e\n\n  \u003cp\u003eWe will develop and monitor metrics that demonstrate whether the framework is working as intended. For example, the central \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function will look at the effectiveness of the framework in mitigating unacceptable risks and assess whether the implementation of the principles by regulators is disproportionate or negatively affecting innovation.\u003c/p\u003e\n\n  \u003cp\u003eInsights from the \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function will contribute to the adaptability of our framework by enabling government to identify opportunities for improvement so we can benefit fully from the flexibility we have built into our approach. Such iteration could include removing or amending existing regulation as well as updating the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework itself.\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eHow?\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eThe range, sources and quality of the data that informs our monitoring and evaluation of the framework will be critical.\u003c/p\u003e\n\n  \u003cp\u003eThe \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function will identify the metrics and data sources to help us measure how well the regime is working, both in terms of the framework’s ability to mitigate risk but also to ensure that it is supporting innovation. It will bring together a wide range of views including industry, civil society groups and academia.\u003c/p\u003e\n\n  \u003cp\u003eCrucially, we will work with regulators to identify how their work – including data collected from their own regulatory activities – can support our central \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e function in order to ensure the best outcomes for the whole economy.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e15. Do you agree with our overall approach to monitoring and evaluation?\u003c/p\u003e\n\n  \u003cp\u003e16. What is the best way to measure the impact of our framework?\u003c/p\u003e\n\n  \u003cp\u003e17. Do you agree that our approach strikes the right balance between supporting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation; addressing known, prioritised risks; and future-proofing the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation framework?\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e74. It is important to have the right architecture in place to oversee the delivery of the central functions described above. The \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem already benefits from a range of organisations with extensive expertise in regulatory issues. Ground-breaking coordination initiatives like the Digital Regulation Cooperation Forum (\u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e) play a valuable role in enhancing regulatory alignment and fostering dialogue on digital issues across regulators. However, the \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e was not created to support the delivery of all the functions we have identified or the implementation of our proposed regulatory framework for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e75. Government will initially be responsible for delivering the central functions described above, working in partnership with regulators and other key actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem to leverage existing activities where possible. This is aligned with our overall iterative approach and enables system-wide review of the framework. We recognise that there may be value in a more independent delivery of the central functions in the longer term.\u003c/p\u003e\u003cp\u003e76. Where relevant activities are already undertaken by organisations either within or outside of government, the primary role of the central functions will be to leverage these activities and assess their effectiveness. Where this is not the case – for example, where new bespoke capabilities are needed to monitor and evaluate the operation of the framework as a whole – these functions will initially be established in government.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"case-study-38-building-on-a-strong-foundation-of-regulatory-coordination\"\u003eCase study 3.8: Building on a strong foundation of regulatory coordination\u003c/h4\u003e\n\n  \u003cp\u003eThe growth of digital technologies requires regulators to coordinate and act cohesively. The Digital Regulation Cooperation Forum (\u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e) has published its vision for a joined-up approach to digital regulation. It conducts cross-regulator horizon scanning for future technology and has issued detailed discussion papers on the benefits, harms and auditing of algorithms.\u003c/p\u003e\n\n  \u003cp\u003eRegulators are also exploring ways to provide simpler ‘shop fronts’ for those they regulate, with the \u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and Digital Regulations Service offer already robustly tested with end users and now widely available.\u003csup id=\"fnref:123\"\u003e\u003ca href=\"#fn:123\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 123]\u003c/a\u003e\u003c/sup\u003e \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e regulators have a multi-agency advice service for digital innovators pilot underway, supported by government’s Regulators’ Pioneer Fund,\u003csup id=\"fnref:124\"\u003e\u003ca href=\"#fn:124\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 124]\u003c/a\u003e\u003c/sup\u003e which aims to make it easier for firms operating across digital regulatory boundaries to do business.\u003c/p\u003e\n\n  \u003cp\u003eExisting regulatory forums may need to be supplemented or adapted to successfully implement the cross-cutting principles. We will work in partnership with existing bodies as well as industry to improve and enhance regulatory coordination.\u003c/p\u003e\n\u003c/div\u003e\u003cp\u003e77. We are deliberately taking an iterative approach to the delivery of the regulatory framework and we anticipate that the model for providing the central functions will develop over time. We will identify where existing structures may need to be supplemented or adapted. In particular, we are focused on understanding:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eWhether existing regulatory forums could be expanded to include the full range of regulators involved in the regulation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e or whether additional mechanisms are needed.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eWhat additional expertise government may need to support the implementation and monitoring of the principles, including the potential role that could be played by established advisory bodies.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eThe most effective way to convene input from across industry and consumers to ensure a broad range of opinions.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e78. Government, in fulfilling the regulatory central functions and overseeing the framework, will benefit from engaging external expertise to gather insights and advice from experts in industry, academia and civil society. The \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Council has been an important source of expertise over the last 3 years, advising government on the development of the National \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Strategy as well as our approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance. As we enter a new phase we will review the role of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Council and consider how best to engage expertise to support the implementation of the regulatory framework.\u003c/p\u003e\u003cp\u003e79. As the regulatory framework evolves and we develop a clearer understanding of the system-level functions that are needed, we will review the operational model outlined above. In particular, we will consider if a government unit is the most appropriate mechanism for delivering the central functions in the longer term or if an independent body would be more effective.\u003c/p\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e18. Do you agree that regulators are best placed to apply the principles and government is best placed to provide oversight and deliver central functions?\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"section332\"\u003e3.3.2 Government’s role in addressing accountability across the life cycle\u003c/h3\u003e\u003cp\u003e80. The clear allocation of accountability and legal responsibility is important for effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance. Legal responsibility for compliance with the principles should be allocated to the actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle best able to identify, assess and mitigate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks effectively. Incoherent or misplaced allocation of legal responsibility could hinder innovation or adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e81. However, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chains can be complex and opaque, making effective governance of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and supply chain risk management difficult. Inappropriate allocation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk, liability, and responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance throughout the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle and within \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chains could impact negatively on innovation. For example, inappropriate allocation of liability to a business using, but not developing, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e could stifle \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption. Similarly, allocating too much responsibility to businesses developing foundation models, on the grounds that these models could be used by third parties in a range of contexts, would hamper innovation.\u003c/p\u003e\u003cp\u003e82. We recognise the need to consider which actors should be responsible and liable for complying with the principles, which may not be the same actors who bear the burden under current legal frameworks. For example, data protection law differentiates between data controllers and data processors. Similarly, product safety laws include the concepts of producers and distributors. In the context of those specific legal frameworks, liability for compliance with various existing legal obligations is allocated by law to those identified supply chain actors. It is not yet clear how responsibility and liability for demonstrating compliance with the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory principles will be or should ideally be, allocated to existing supply chain actors within the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003c/p\u003e\u003cp\u003e83. We are not proposing to intervene and make changes to life cycle accountability at this stage. It is too soon to make decisions about liability as it is a complex, rapidly evolving issue which must be handled properly to ensure the success of our wider \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem. However, to further our understanding of this topic we will engage a range of experts, including technicians and lawyers. It may become apparent that current legal frameworks, when combined with implementation of our \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e principles by regulators, will allocate legal responsibility and liability across the supply chain in a way that is not fair or effective. We would consider proportionate interventions to address such issues which could otherwise undermine our pro-innovation approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation. Our agile approach benefits our sovereign parliamentary system’s reliable ability to introduce targeted measures – for example by amending existing legislation if necessary – in response to new evidence.\u003csup id=\"fnref:125\"\u003e\u003ca href=\"#fn:125\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 125]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e84. Tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e like assurance techniques and technical standards can support supply chain risk management. These tools can also drive the uptake and adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e by building justified trust in these systems, giving users confidence that key \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks have been identified, addressed and mitigated across the supply chain. For example, by describing measures that manufacturers should take to ensure the safety of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems, technical standards can provide reassurance to purchasers and users of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems that appropriate safety-focused measures have been adopted, ultimately encouraging adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e85. Our evaluation of the framework will assess whether the legal responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is effectively and fairly distributed. As we implement the framework, we will continue our extensive engagement to gather evidence from regulators, industry, academia, and civil society on its impact on different actors across the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle. This will allow us to monitor the effects of our framework on actors across the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chain on an ongoing basis. We will need a particular focus on foundation models given the potential challenges they pose to life cycle accountability, especially when available as open-source. By centrally evaluating whether there are adequate measures for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e accountability, we can assess the need for further interventions into \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e liability across the whole economy and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003c/p\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eL1. What challenges might arise when regulators apply the principles across different \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e applications and systems? How could we address these challenges through our proposed \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework?\u003c/p\u003e\n\n  \u003cp\u003eL2.1 Do you agree that the implementation of our principles through existing legal frameworks will fairly and effectively allocate legal responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e across the life cycle?\u003c/p\u003e\n\n  \u003cp\u003eL.2.2. How could it be improved, if at all?\u003c/p\u003e\n\n  \u003cp\u003eL3. If you work for a business that develops, uses, or sells \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, how do you currently manage \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk including through the wider supply chain? How could government support effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risk management?\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"section333\"\u003e3.3.3 Foundation models and the regulatory framework\u003c/h3\u003e\u003cp\u003e86. Foundation models are an emerging type of general purpose \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e that are trained on vast quantities of data and can be adapted to a wide range of tasks. The fast-paced development of foundation models brings novel challenges for governments seeking to regulate \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Despite high levels of interest in the topic, the research community has not found a consensus on how foundation models work, the risks they pose or even the extent of their capabilities.\u003csup id=\"fnref:126\"\u003e\u003ca href=\"#fn:126\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 126]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e87. Foundation models have been described as paradigm shifting and could have significant impacts on society and the economy.\u003csup id=\"fnref:127\"\u003e\u003ca href=\"#fn:127\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 127]\u003c/a\u003e\u003c/sup\u003e They can be used for a wide variety of purposes and deployed in many already complex ecosystems. Given the widely acknowledged transformative potential of foundation models, we must give careful attention to how they might interact with our proposed regulatory framework. Our commitment to an adaptable, proportionate approach presents a clear opportunity for the UK to lead the global conversation and set global norms for the future-proof regulation of foundation models.\u003c/p\u003e\u003cp\u003e88. There is a relatively small number of organisations developing foundation models. Some organisations exercise close control over the development and distribution of their foundation models. Other organisations take an open-source approach to the development and distribution of the technology. Open-source models can improve access to the transformational power of foundation models, but can cause harm without adequate guardrails.\u003csup id=\"fnref:128\"\u003e\u003ca href=\"#fn:128\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 128]\u003c/a\u003e\u003c/sup\u003e The variation in organisational approaches to developing and supplying foundation models introduces a wide range of complexities for the regulation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. The potential opacity of foundation models means that it can also be challenging to identify and allocate accountability for outcomes generated by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems that rely on or integrate them.\u003c/p\u003e\u003cp\u003e89. Our proposed framework considers the issues raised by foundation models in light of our life cycle accountability analysis, outlined in \u003ca href=\"#section332\"\u003esection 3.3.2\u003c/a\u003e above. Given the small number of organisations supplying foundation models and a proportionately larger number of businesses integrating or otherwise deploying foundation models elsewhere in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem, we recognise the important role of tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, including assurance techniques and technical standards.\u003c/p\u003e\u003cp\u003e90. The proposed central functions described in \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e will play an important role in informing our approach to regulating foundation models. The central risk function’s proactive, rigorous monitoring of risks associated with foundation models and the horizon scanning function’s identification of related opportunities will be critical to ensuring that we strike the balance needed as part of our proportionate, pro-innovation regulatory approach. It will be crucial to ensure that the proposed monitoring and evaluation function has access to the technical skills and capabilities needed to assess the impact that our framework has on the opportunities and risks presented by foundation models.\u003c/p\u003e\u003cp\u003e91. We recognise that industry, academia, research organisations and global partners are looking for ways to address the challenges related to the regulation of foundation models.\u003csup id=\"fnref:129\"\u003e\u003ca href=\"#fn:129\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 129]\u003c/a\u003e\u003c/sup\u003e For example, we know that developers of foundation models are exploring ways to embed alignment theory into their models. This is an important area of research, and government will need to work closely with the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e research community to leverage insights and inform our iteration of the regulatory framework. Our collaborative, adaptable framework will draw on the expertise of those researchers and other stakeholders as we continue to develop policy in this evolving area.\u003c/p\u003e\u003cp\u003e92. The UK is committed to building its capabilities in foundation models. Our Foundation Model Taskforce announced in the Integrated Review Refresh 2023\u003csup id=\"fnref:130\"\u003e\u003ca href=\"#fn:130\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 130]\u003c/a\u003e\u003c/sup\u003e will support government to build UK capability and ensure the UK harnesses the benefits presented by this emerging technology. Our proposed framework will ensure we create the right regulatory environment as we move to maximise the transformative potential of foundation models.\u003c/p\u003e\u003ch4 id=\"case-study-39-life-cycle-accountability-for-large-language-models\"\u003eCase-study 3.9: Life cycle accountability for large language models\u003c/h4\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003cp\u003eLarge language models (\u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e) are a type of foundation model.\u003csup id=\"fnref:131\"\u003e\u003ca href=\"#fn:131\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 131]\u003c/a\u003e\u003c/sup\u003e The potential of \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e goes beyond reproducing or translating natural language: \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e also have the power to write software,\u003csup id=\"fnref:132\"\u003e\u003ca href=\"#fn:132\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 132]\u003c/a\u003e\u003c/sup\u003e generate stories\u003csup id=\"fnref:133\"\u003e\u003ca href=\"#fn:133\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 133]\u003c/a\u003e\u003c/sup\u003e through films and virtual reality,\u003csup id=\"fnref:134\"\u003e\u003ca href=\"#fn:134\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 134]\u003c/a\u003e\u003c/sup\u003e and more.\u003csup id=\"fnref:135\"\u003e\u003ca href=\"#fn:135\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 135]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003e\u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e fall within the scope of our regulatory framework as they are autonomous and adaptable.\u003c/p\u003e\n\n  \u003cp\u003eWe are mindful of the rapid technological change in the development of foundation models such as \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e and the new opportunities that they bring to applications including search engines, medical devices, and financial and legal services. However, \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e also have limitations, for example, the models are not trained on a sense of truth,\u003csup id=\"fnref:136\"\u003e\u003ca href=\"#fn:136\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 136]\u003c/a\u003e\u003c/sup\u003e so they can reproduce inconsistent or false outputs that seem highly credible.\u003csup id=\"fnref:137\"\u003e\u003ca href=\"#fn:137\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 137]\u003c/a\u003e\u003c/sup\u003e Because they can be adapted to a wide variety of tasks downstream within an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chain, any improvements or defects in a foundation model could quickly affect all adapted products.\u003c/p\u003e\n\n  \u003cp\u003eUnder the UK’s pro-innovation \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework, regulators may decide to issue specific guidance and requirements for \u003cabbr title=\"large language model\"\u003eLLM\u003c/abbr\u003e developers and deployers to address risks and implement the cross-cutting principles. This could include guidance on appropriate transparency measures to inform users when \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is being used and the data used to train the model.\u003c/p\u003e\n\n  \u003cp\u003eThe wide-reaching impact of \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e through the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chain – together with their general purpose and potential wide ranging application – means they are unlikely to be directly ‘caught’ within the remit of any single regulator. This makes effective governance and supply chain risk-management challenging where \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e are involved. The \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework’s monitoring and evaluation function will therefore need to assess the impacts of \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e. The cross-cutting accountability and governance principle will encourage regulators and businesses to find ways to demonstrate accountability and good governance in responsible \u003cabbr title=\"large language model\"\u003eLLM\u003c/abbr\u003e development and use.\u003c/p\u003e\n\n  \u003cp\u003eAt this point it would be premature to take specific regulatory action in response to foundation models including \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e. To do so would risk stifling innovation, preventing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e adoption, and distorting the UK’s thriving \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem.\u003c/p\u003e\n\n  \u003cp\u003eHowever, we are mindful of the rapid rate of advances in the power and application of \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e, and the potential creation of new or previously unforeseen risks. As such, \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e will be a core focus of our monitoring and risk assessment functions and we will work with the wider \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e community to ensure our adaptive framework is capable of identifying and responding to developments relating to \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e.\u003c/p\u003e\n\n  \u003cp\u003eFor example, one way of monitoring the potential impact of \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e could be by monitoring the amount of compute used to train them, which is much easier to assess and govern than other inputs such as data, or talent. This could involve statutory reporting requirements for models over a certain size. This metric could become less useful as a way of establishing who has access to powerful models if machine learning development becomes increasingly open-source.\u003csup id=\"fnref:138\"\u003e\u003ca href=\"#fn:138\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 138]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\n  \u003cp\u003eLife cycle accountability – including the allocation of responsibility and liability for risks arising from the use of foundation models including \u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e – is a priority area for ongoing research and policy development. We will explore the ways in which technical standards and other tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can support good practices for responsible innovation across the life cycle and supply chain. We will also work with regulators to ensure they are appropriately equipped to engage with actors across the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chain and allocate legal liability appropriately.\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eF1. What specific challenges will foundation models such as large language models (\u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e) or open-source models pose for regulators trying to determine legal responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e outcomes?\u003c/p\u003e\n\n  \u003cp\u003eF2. Do you agree that measuring compute provides a potential tool that could be considered as part of the governance of foundation models?\u003c/p\u003e\n\n  \u003cp\u003eF3. Are there other approaches to governing foundation models that would be more effective?\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"section334\"\u003e3.3.4 Artificial intelligence sandboxes and testbeds\u003c/h3\u003e\u003cp\u003e93. Government is committed to supporting innovators by addressing regulatory challenges that prevent new, cutting-edge products from getting to market. Barriers can be particularly high when a path to market requires interaction with multiple regulators or regulatory guidance is nascent. Sir Patrick Vallance’s Digital Report recommends that government works with regulators to develop an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandbox to support innovators. At the Budget, government confirmed our commitment to taking forward this recommendation.\u003csup id=\"fnref:139\"\u003e\u003ca href=\"#fn:139\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 139]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e94. The Information Commissioner’s Office (\u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e) and the Financial Conduct Authority (\u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e) have already successfully piloted digital sandboxes in their sectors.\u003csup id=\"fnref:140\"\u003e\u003ca href=\"#fn:140\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 140]\u003c/a\u003e\u003c/sup\u003e The \u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e sandbox has worked with over 800 businesses and accelerated their speed to market by an estimated 40% on average.\u003csup id=\"fnref:141\"\u003e\u003ca href=\"#fn:141\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 141]\u003c/a\u003e\u003c/sup\u003e Sandbox participation has also been found to have significant financial benefits, particularly for smaller organisations.\u003csup id=\"fnref:142\"\u003e\u003ca href=\"#fn:142\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 142]\u003c/a\u003e\u003c/sup\u003e We have heard from regulators, including those with less experience of taking part in previous initiatives, that they are keen to participate in new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandboxes to support their regulated sectors.\u003c/p\u003e\u003cp\u003e95. Regulatory sandboxes and testbeds will play an important role in our proposed regulatory regime. Such initiatives enable government and regulators to:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eSupport innovators to get novel products and services to market faster, so they can start generating economic and social benefits.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eTest how the regulatory framework is operating in practice and illuminate unnecessary barriers to innovation that need to be addressed.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eIdentify emerging technology and market trends to which our regulatory framework may need to adapt.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e96. To deliver an effective sandbox, we would like to understand more deeply what service focus would be most useful to industry. We are considering 4 options:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eSingle sector, single regulator: support innovators to bring \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e products to the market in collaboration with a single regulator, focusing on only one chosen industry sector.\u003csup id=\"fnref:143\"\u003e\u003ca href=\"#fn:143\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 143]\u003c/a\u003e\u003c/sup\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eMultiple sectors, single regulator: support \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators in collaboration with a single regulator that is capable of working across multiple industry sectors.\u003csup id=\"fnref:144\"\u003e\u003ca href=\"#fn:144\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 144]\u003c/a\u003e\u003c/sup\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eSingle sector, multiple regulator: establish a sandbox that only operates in one industry sector but is capable of supporting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators whose path to market requires interaction with one or more regulators operating in that sector.\u003csup id=\"fnref:145\"\u003e\u003ca href=\"#fn:145\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 145]\u003c/a\u003e\u003c/sup\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eMultiple sectors, multiple regulators: a sandbox capable of operating with one or more regulators in one or more industry sectors to help \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators reach their target market. The \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e is piloting a version of this model.\u003csup id=\"fnref:146\"\u003e\u003ca href=\"#fn:146\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 146]\u003c/a\u003e\u003c/sup\u003e\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e97. We intend to focus an initial pilot on a single sector, multiple regulator sandbox. Recognising the importance of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovations that have implications in multiple sectors (like generative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e models), we would look to expand this capability to cover multiple industry sectors over time.\u003c/p\u003e\u003cp\u003e98. Initially, we envisage focusing the sandbox on a sector where there is a high degree of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e investment, industry demand for a sandbox, and appetite for improved collaboration between regulators to help \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators take their products to market. We invite consultation feedback on this proposal as well as suggestions for industry sectors that meet these criteria.\u003c/p\u003e\u003cp\u003e99. We would also like to build a deeper understanding of what service offering would be most helpful to industry. Some sandboxes offer supervised real-life or simulated test environments where innovators can trial new products, often under relaxed regulatory requirements.\u003csup id=\"fnref:147\"\u003e\u003ca href=\"#fn:147\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 147]\u003c/a\u003e\u003c/sup\u003e In other scenarios, a team of technologists and regulation experts give customised advice and support to participating innovators over a number of months to help them understand and overcome regulatory barriers so they can reach their target market.\u003csup id=\"fnref:148\"\u003e\u003ca href=\"#fn:148\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 148]\u003c/a\u003e\u003c/sup\u003e Our current preference is for the customised advice and support model, as we think this is where we can deliver benefits most effectively in the short term. We will explore options for developing a safe test environment capability at a later date, informed by our initial pilot work.\u003c/p\u003e\u003cp\u003e100. The implementation of an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory sandbox will also be closely informed by Sir Patrick Vallance’s review into digital regulation and his recommendation to establish a multi-regulator sandbox.\u003csup id=\"fnref:149\"\u003e\u003ca href=\"#fn:149\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 149]\u003c/a\u003e\u003c/sup\u003e The review sets out a number of design principles, which we will build into our pilot approach. This includes targeting such initiatives at start-ups and small to medium-sized businesses. As a matter of priority, we will engage with businesses to understand how such an approach should be designed and delivered to best support their needs.\u003c/p\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003eS1. To what extent would the sandbox models described in \u003ca href=\"#section334\"\u003esection 3.3.4\u003c/a\u003e support innovation?\u003c/p\u003e\n\n  \u003cp\u003eS2. What could government do to maximise the benefit of sandboxes to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators?\u003c/p\u003e\n\n  \u003cp\u003eS3. What could government do to facilitate participation in an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory sandbox?\u003c/p\u003e\n\n  \u003cp\u003eS4. Which industry sectors or classes of product would most benefit from an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandbox?\u003c/p\u003e\n\u003c/div\u003e\u003ch3 id=\"regulator-capabilities\"\u003e3.3.5 Regulator capabilities\u003c/h3\u003e\u003cp\u003e101. Government has prioritised the ongoing assessment of the different capability needs across the regulatory landscape. We will keep this under close review as part of our ongoing monitoring and evaluation activity.\u003c/p\u003e\u003cp\u003e102. While our approach does not currently involve or anticipate extending any regulator’s remit,\u003csup id=\"fnref:150\"\u003e\u003ca href=\"#fn:150\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 150]\u003c/a\u003e\u003c/sup\u003e regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e uses effectively will require many of our regulators to acquire new skills and expertise. Our research\u003csup id=\"fnref:151\"\u003e\u003ca href=\"#fn:151\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 151]\u003c/a\u003e\u003c/sup\u003e has highlighted different levels of capability among regulators when it comes to understanding \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and addressing its unique characteristics. Our engagement has also elicited a wide range of views on the capabilities regulators require to address \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks and on the best way for regulators to acquire these.\u003c/p\u003e\u003cp\u003e103. We identified potential capability gaps among many, but not all, regulators, primarily in relation to:\u003c/p\u003e\u003cp\u003e\u003cstrong\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e expertise. Particularly:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eTechnical expertise in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technology.\u003csup id=\"fnref:152\"\u003e\u003ca href=\"#fn:152\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 152]\u003c/a\u003e\u003c/sup\u003e For example, on how \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is being used to deliver products and services and on the development, use and applicability of technical standards.\u003csup id=\"fnref:153\"\u003e\u003ca href=\"#fn:153\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 153]\u003c/a\u003e\u003c/sup\u003e\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eExpertise on how \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases interact across multiple regulatory regimes.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eMarket intelligence on how \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies are being used to disrupt existing business models, both in terms of the potential opportunities and risks that can impact regulatory objectives.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e\u003cstrong\u003eOrganisational capacity. A regulator’s ability to:\u003c/strong\u003e\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eEffectively adapt to the emergence of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases and applications, and assimilate and share this knowledge throughout the organisation.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eWork with organisations that provide assurance techniques (such as assurance service providers) and develop technical standards (such as standards development organisations), to identify relevant tools and embed them into the regulatory framework and best practice.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eWork across regulators to share knowledge and cooperate in the regulation of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases that interact across multiple regulatory regimes.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eEstablish relationships and communicate effectively with organisations and groups not normally within their remit.\u003cbr\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e104. In the initial phases of implementation, government will work collaboratively with key partners to leverage existing work on this topic. For example, the Digital Regulation Cooperation Forum (\u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e) is already exploring ways of addressing capability gaps within its members.\u003c/p\u003e\u003cp\u003e105. There are options for addressing capability gaps within individual regulators and across the wider regulatory landscape, which we will continue to explore. It may, for example, be appropriate to establish a common pool of expertise that could establish best practice for supporting innovation through regulatory approaches and make it easier for regulators to work with each other on common issues. An alternative approach would be to explore and facilitate collaborative initiatives between regulators – including, where appropriate, further supporting existing initiatives such as the \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e – to share skills and expertise.\u003c/p\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e19. As a regulator, what support would you need in order to apply the principles in a proportionate and pro-innovation way?\u003c/p\u003e\n\n  \u003cp\u003e20. Do you agree that a pooled team of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e experts would be the most effective way to address capability gaps and help regulators apply the principles?\u003c/p\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 4: Tools for trustworthy AI to support implementation",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-4-tools-for-trustworthy-ai-to-support-implementation",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-4-tools-for-trustworthy-ai-to-support-implementation",
        "text": "\u003ch3 id=\"partfour\"\u003e4.1 \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance techniques\u003c/h3\u003e\u003cp\u003e106. Tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e including assurance techniques and technical standards will play a critical role in enabling the responsible adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and supporting the proposed regulatory framework. Industry and civil society were keen to see a range of practical tools to aid compliance. Government is already supporting the development of these tools by publishing a Roadmap to an effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance ecosystem in the UK\u003csup id=\"fnref:154\"\u003e\u003ca href=\"#fn:154\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 154]\u003c/a\u003e\u003c/sup\u003e and establishing the UK \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Standards Hub\u003csup id=\"fnref:155\"\u003e\u003ca href=\"#fn:155\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 155]\u003c/a\u003e\u003c/sup\u003e to champion the use of technical standards.\u003csup id=\"fnref:156\"\u003e\u003ca href=\"#fn:156\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 156]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e107. To assure \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems effectively, we need a toolbox of assurance techniques to measure, evaluate and communicate the trustworthiness of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems across the development and deployment life cycle. These techniques include impact assessment, audit, and performance testing along with formal verification methods.\u003c/p\u003e\u003cp\u003e108. It is unlikely that demand for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance can be entirely met through organisations building in-house capability. The emerging market for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance services and expertise will have an important role to play in providing a range of assurance techniques to actors within the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chain. There is an opportunity for the UK to become a global leader in this market as the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance industry develops. This will enable organisations to determine whether \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies are aligned with relevant regulatory requirements.\u003c/p\u003e\u003cp\u003e109. To help innovators understand how \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance techniques can support wider \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance, the government will launch a Portfolio of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance techniques in Spring 2023. The Portfolio is a collaboration with industry to showcase how these tools are already being applied by businesses to real-world use cases and how they align with the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory principles.\u003c/p\u003e\u003ch3 id=\"ai-technical-standards\"\u003e4.2 \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technical standards\u003c/h3\u003e\u003cp\u003e110. Assurance techniques need to be underpinned by available technical standards, which provide common understanding across assurance providers. Technical standards and assurance techniques will also enable organisations to demonstrate that their systems are in line with the UK’s \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory principles.\u003c/p\u003e\u003cp\u003e111. Multiple international and regional standards development organisations are developing, or have already released, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-specific technical standards, addressing topics such as risk management, transparency, bias, safety and robustness. Accordingly, technical standards can be used by regulators to complement sector-specific approaches to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation by providing common benchmarks and practical guidance to organisations.\u003csup id=\"fnref:157\"\u003e\u003ca href=\"#fn:157\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 157]\u003c/a\u003e\u003c/sup\u003e Overall, technical standards can embed flexibility\u003csup id=\"fnref:158\"\u003e\u003ca href=\"#fn:158\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 158]\u003c/a\u003e\u003c/sup\u003e into regulatory regimes and drive responsible innovation by helping organisations to address \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks.\u003csup id=\"fnref:159\"\u003e\u003ca href=\"#fn:159\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 159]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\u003cp\u003e112. The UK plays a leading role in the development of international technical standards, working with industry, international and UK partners.\u003csup id=\"fnref:160\"\u003e\u003ca href=\"#fn:160\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 160]\u003c/a\u003e\u003c/sup\u003e The government will continue to support the role of technical standards in complementing our approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation, including through the UK \u003ca rel=\"external\" href=\"https://aistandardshub.org/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Standards Hub\u003c/a\u003e.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box-41-supporting-a-layered-approach-to-ai-technical-standards\"\u003eBox 4.1: Supporting a layered approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technical standards\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nThe government will complement its context-specific approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation by proposing a proportionate ‘layered approach’ to applying available \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technical standards. This involves regulators identifying relevant technical standards and encouraging their adoption by actors in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle to support the integration of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation principles into technical and operational business processes:\u003c/p\u003e\n\n  \u003cp\u003eLayer 1: To provide consistency and common foundations across regulatory remits, in the first instance regulators could seek to encourage adoption of sector-agnostic standards which can be applied across \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e use cases to support the implementation of cross-sectoral principles. For example, management systems, risk management, and quality standards\u003csup id=\"fnref:161\"\u003e\u003ca href=\"#fn:161\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 161]\u003c/a\u003e\u003c/sup\u003e can provide industry with good practices for the responsible development of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems. The adoption of these standards should be encouraged by multiple regulators as tools for regulated entities to establish common good practices for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance.\u003c/p\u003e\n\n  \u003cp\u003eLayer 2: To adapt these governance practices to the specific risks raised by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in a particular context, regulators could look at encouraging adoption of additional standards addressing specific issues such as bias and transparency.\u003csup id=\"fnref:162\"\u003e\u003ca href=\"#fn:162\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 162]\u003c/a\u003e\u003c/sup\u003e Such standards would act as tools for industry to operationalise compliance with specific \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation principles. As these standards will provide good practices for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance applicable to multiple sectors, regulators could complement these with sector-specific guidance.\u003c/p\u003e\n\n  \u003cp\u003eFor example, standards for bias mitigation could be promoted by the Financial Conduct Authority (\u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e) and the Equality and Human Rights Commission (\u003cabbr title=\"Equality and Human Rights Commission\"\u003eEHRC\u003c/abbr\u003e) as practical tools for providers of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e scoring models to identify and mitigate relevant sources of bias to ensure the fairness of the outcomes when the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e model is applied to financial services (credit scoring) and \u003cabbr title=\"Human Resources\"\u003eHR\u003c/abbr\u003e practices (candidate scoring) respectively.\u003c/p\u003e\n\n  \u003cp\u003eLayer 3: Where relevant, regulators could encourage adoption of sector-specific technical standards to support compliance with specific regulatory requirements and performance measures.\u003csup id=\"fnref:163\"\u003e\u003ca href=\"#fn:163\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 163]\u003c/a\u003e\u003c/sup\u003e\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e21. Which non-regulatory tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would most help organisations to embed the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation principles into existing business processes?\u003c/p\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 5: Territorial application",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-5-territorial-application",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-5-territorial-application",
        "text": "\u003ch3 id=\"territorial-application-of-the-regulatory-framework\"\u003e5.1 Territorial application of the regulatory framework\u003c/h3\u003e\u003cp\u003e113. Our \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation framework applies to the whole of the UK. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is used in various sectors and impacts on a wide range of policy areas, some of which are reserved and some of which are devolved. We will continue to consider any devolution impacts of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation as the policy develops and in advance of any legislative action. Some regulators share remits with their counterparts in the devolved administrations. Our framework, to be initially set out on a non-statutory basis, will not alter the current territorial arrangement of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e policy. We will rely on the interactions with existing legislation on reserved matters, such as the Data Protection Act 2018 and the Equality Act 2010, to implement our framework.\u003c/p\u003e\u003cp\u003e114. We will continue to engage devolved administrations, businesses, and members of the public from across the UK to ensure that every part of the country benefits from our pro-innovation approach. We will, for example, convene the devolved administrations for views on the functions we expect the government to perform and on the potential implications of introducing a statutory duty on regulators to have due regard to the principles.\u003c/p\u003e\u003ch3 id=\"extraterritorial-application-of-the-regulatory-framework\"\u003e5.2 Extraterritorial application of the regulatory framework\u003c/h3\u003e\u003cp\u003e115. While we expect our principles-based approach to influence the global conversation on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance, we are not currently proposing the introduction of new legal requirements. Our framework will not therefore change the territorial applicability of existing legislation relevant to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e (including, for example, data protection legislation).\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 6: Global interoperability and international engagement",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#partsix",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#partsix",
        "text": "\u003ch3 id=\"our-regulatory-framework-on-the-world-stage\"\u003e6.1 Our regulatory framework on the world stage\u003c/h3\u003e\u003cp\u003e116. Countries and jurisdictions around the world are moving quickly to set the rules that govern \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. The UK is a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e with a strategic advantage that places us at the forefront of these developments. The UK is ranked third in the world for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e publications and also has the third highest number of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies.\u003csup id=\"fnref:164\"\u003e\u003ca href=\"#fn:164\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 164]\u003c/a\u003e\u003c/sup\u003e We want to build on this position, making the UK the best place to research \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and to create and build innovative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies. At the same time, we recognise the importance of working closely with international partners. As such, the UK’s approach to both our domestic regulation and international discussions will continue to be guided by our ambition to develop \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e frameworks that champion our democratic values and economic priorities.\u003c/p\u003e\u003cp\u003e117. In line with our domestic approach, we will focus on supporting the positive global opportunities \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can bring while protecting citizens against the potential harms and risks that can emanate across borders. We will work closely with international partners to both learn from, and influence, regulatory and non-regulatory developments (see examples in \u003ca href=\"#box61\"\u003ebox 6.1\u003c/a\u003e). Given the complex and cross-border nature of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e supply chains, with many \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e businesses operating across multiple jurisdictions, close international cooperation will strengthen the impact of our proposed framework.\u003c/p\u003e\u003cp\u003e118. We will promote interoperability and coherence between different approaches, challenging barriers which may stand in the way of businesses operating internationally. We will ensure that the UK’s regulatory framework encourages the development of a responsive and compatible system of global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance. We will build our international influence, allowing the UK to engage meaningfully with like-minded partners on issues such as cross-border \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks and opportunities.\u003c/p\u003e\u003cp\u003e119. The UK will continue to pursue an inclusive, multi-stakeholder approach, from negotiating new global norms to helping partner countries build their awareness and capacity in relation to the benefits and risks of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technology. We will, for example, support other nations to implement regulation and technical standards that support inclusive, responsible and sustainable artificial intelligence. More widely, the International Tech Strategy will reiterate how we will shape global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e activities in line with UK values and priorities, protecting against efforts to adopt and apply \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies in the service of authoritarianism and repression. We will work with UK industry leaders to ensure that we stay at the forefront of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and share our best practice with like-minded nations. Similarly, we will learn from our international partners, encouraging them to share lessons we can integrate into our framework.\u003c/p\u003e\u003cp\u003e120. Our international approach will include ensuring that proven, effective, and agreed upon assurance techniques and international technical standards play a role in the wider regulatory ecosystem. Such measures will also support cross-border trade by setting out risk management and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance practices that are globally recognised by trading partners, reducing technical barriers to trade and increasing market access. We will also use our world-leading innovation provisions in Free Trade Agreements to address the challenges innovators in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e may face and ensure that businesses are able to take advantage of the opportunities it presents.\u003c/p\u003e\u003cp\u003e121. In multilateral engagements, we will work to leverage each forum’s strengths, expertise and membership to ensure they are adding maximum value to global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance discussions and are relevant to our democratic values and economic priorities.\u003c/p\u003e\u003cdiv class=\"call-to-action\"\u003e\n  \u003ch4 id=\"box61\"\u003eBox 6.1: Examples of international engagement and collaboration\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nThe UK has played an active and leading role on the international \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e stage and will continue to do so. Some (non-exhaustive) examples of activities are:\u003c/p\u003e\n\n  \u003cp\u003e\u003cstrong\u003eMultilateral \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e engagement\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cul\u003e\n    \u003cli\u003e\n\u003cstrong\u003e\u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Governance Working Party (\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-GO):\u003c/strong\u003e The UK is an active member of the \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e’s Working Party on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Governance (AIGO), which supports the implementation of the \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e’s \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e principles and enables the exchange of experience and best practice to advance the responsible stewardship of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003csup id=\"fnref:165\"\u003e\u003ca href=\"#fn:165\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 165]\u003c/a\u003e\u003c/sup\u003e\n\u003c/li\u003e\n    \u003cli\u003e\n\u003cstrong\u003eGlobal Partnership on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e (\u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e):\u003c/strong\u003e The UK is a key contributor to – and founding member of – the Global Partnership on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e (\u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e), which is an independent organisation consisting of 29 countries and a range of international experts. \u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e was launched in 2020 as the first international multilateral forum to focus solely on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and the UK has played a significant role in shaping its development and influencing its agenda.\u003csup id=\"fnref:166\"\u003e\u003ca href=\"#fn:166\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 166]\u003c/a\u003e\u003c/sup\u003e\n      \u003cul\u003e\n        \u003cli\u003eAt the 2022 \u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e Ministerial Summit in Japan, we demonstrated the scale of the UK’s \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ambitions by announcing £1.2m of funding to develop a Net Zero Data Space for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Applications (which will also support our Net Zero policy objectives).\u003csup id=\"fnref:167\"\u003e\u003ca href=\"#fn:167\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 167]\u003c/a\u003e\u003c/sup\u003e This is in addition to the previous £1m investment to advance \u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e research on data justice (collaborating with The Alan Turing Institute and 12 pilot partners in low and medium income countries).\u003c/li\u003e\n      \u003c/ul\u003e\n    \u003c/li\u003e\n    \u003cli\u003e\n\u003cstrong\u003eG7:\u003c/strong\u003e The UK is actively engaged in the G7’s work on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and we are working closely with Japan – which holds the G7 Presidency for 2023 – to encourage greater international collaboration, support the development of consistent, proportionate and interoperable regulatory interventions, and champion the role of tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e where appropriate.\u003c/li\u003e\n    \u003cli\u003e\n\u003cstrong\u003eCouncil of Europe Committee on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e (\u003cabbr title=\"Council of Europe Committee on AI\"\u003eCAI\u003c/abbr\u003e):\u003c/strong\u003e The UK holds a Bureau position and we are working closely with like-minded nations on the proposed Convention on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, to help protect human rights, democracy and rule of law.\u003csup id=\"fnref:168\"\u003e\u003ca href=\"#fn:168\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 168]\u003c/a\u003e\u003c/sup\u003e\n\u003c/li\u003e\n    \u003cli\u003e\n\u003cstrong\u003eUNESCO:\u003c/strong\u003e The UK was actively involved in the development of the UNESCO Ethics of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Recommendations and UK organisations have been supporting the development of implementation tools.\u003csup id=\"fnref:169\"\u003e\u003ca href=\"#fn:169\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 169]\u003c/a\u003e\u003c/sup\u003e\n\u003c/li\u003e\n    \u003cli\u003e\n\u003cstrong\u003eGlobal standards development organisations:\u003c/strong\u003e The UK will continue to work with international partners and global standards development organisations to develop and promote global technical standards for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, including through the UK \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Standards Hub.\u003csup id=\"fnref:170\"\u003e\u003ca href=\"#fn:170\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 170]\u003c/a\u003e\u003c/sup\u003e For example, the UK is playing a leading role in the International Organisation for Standardisation and International Electrotechnical Commission\u003csup id=\"fnref:171\"\u003e\u003ca href=\"#fn:171\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 171]\u003c/a\u003e\u003c/sup\u003e (\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e) on 4 active \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e projects.\u003csup id=\"fnref:172\"\u003e\u003ca href=\"#fn:172\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 172]\u003c/a\u003e\u003c/sup\u003e Through the British Standards Institution (BSI),\u003csup id=\"fnref:173\"\u003e\u003ca href=\"#fn:173\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 173]\u003c/a\u003e\u003c/sup\u003e we are also a member of the Open Community for Ethics in Autonomous and Intelligent Systems (\u003cabbr title=\"Open Community for Ethics in Autonomous and Intelligent Systems\"\u003eOCEANIS\u003c/abbr\u003e).\u003csup id=\"fnref:174\"\u003e\u003ca href=\"#fn:174\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 174]\u003c/a\u003e\u003c/sup\u003e\n\u003c/li\u003e\n  \u003c/ul\u003e\n\n  \u003cp\u003e\u003cstrong\u003eBilateral \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e engagement\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cul\u003e\n    \u003cli\u003eThe UK is engaging with individual nations and jurisdictions as they develop regulatory and governance approaches to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. These include the European Union (and its Member States), US, Canada, Singapore, Japan, Australia, Israel, Norway, and Switzerland, amongst many others. We will continue to maintain close dialogues to share information and knowledge, learn from and adapt our approach in collaboration with others, and work together to shape the international landscape.\u003c/li\u003e\n  \u003c/ul\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Part 7: Conclusion and next steps",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-7-conclusion-and-next-steps",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#part-7-conclusion-and-next-steps",
        "text": "\u003ch3 id=\"conclusion-and-next-steps\"\u003e7.1 Conclusion and next steps\u003c/h3\u003e\u003cp\u003e122. Our proportionate approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is designed to strengthen the UK’s position as a global leader in artificial intelligence, harness \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s ability to drive growth and prosperity,\u003csup id=\"fnref:175\"\u003e\u003ca href=\"#fn:175\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 175]\u003c/a\u003e\u003c/sup\u003e and increase public trust in these technologies. The approach we set out is proportionate, adaptable, and context-sensitive to strike the right balance between responding to risks and maximising opportunities.\u003c/p\u003e\u003cp\u003e123. The proposals set out in this document have been informed by the feedback we received from over 130 respondents as part of our call for views on our 2022 policy paper. We will continue to work closely with businesses and regulators as we start to establish the central functions we have identified. Ongoing engagement with industry will be key to our monitoring and evaluation. Feedback will ensure the framework can adapt to new evidence, future-proofing the UK’s role as a leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation and ensuring that we can take a leading role in shaping the global narrative on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation.\u003c/p\u003e\u003cp\u003e124. Given the pace at which \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies and risks emerge, and the scale of the opportunities at stake, we know that there is no time to waste if we are to strengthen the UK’s position as one of the best places in the world to start an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e company. In collaboration with regulators, we are already exploring approaches to implementing the framework and will scale up this activity over the coming months. We are committed to an adaptable, iterative approach that allows us to learn and improve the framework. Our sovereign parliamentary system enables us to deliver targeted and proportionate measures – including by adapting existing legislation if necessary – based on emerging evidence.\u003csup id=\"fnref:176\"\u003e\u003ca href=\"#fn:176\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 176]\u003c/a\u003e\u003c/sup\u003e There are therefore aspects of our implementation work that will be delivered in parallel with the wider consultation set out in this white paper.\u003c/p\u003e\u003cp\u003e125. In the first 6 months following publication we will:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eEngage with industry, the public sector, regulators, academia and civil society through the consultation period.\u003c/li\u003e\n  \u003cli\u003ePublish the government’s response to this consultation.\u003c/li\u003e\n  \u003cli\u003eIssue the cross-sectoral principles to regulators, together with initial guidance to regulators for their implementation. We will work with regulators to understand how the description of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s characteristics can be applied within different regulatory remits and the impact this will have on the application of the cross-sectoral principles.\u003c/li\u003e\n  \u003cli\u003eDesign and publish an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Regulation Roadmap with plans for establishing the central functions (detailed in \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e), including monitoring and coordinating implementation of the principles. This roadmap will set out key partner organisations and identify existing initiatives that will be scaled-up or leveraged to deliver the central functions. It will also include plans to pilot a new \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandbox or testbed.\u003c/li\u003e\n  \u003cli\u003e\n    \u003cp\u003eAnalyse findings from commissioned research projects and improve our understanding of:\u003c/p\u003e\n\n    \u003cul\u003e\n      \u003cli\u003ePotential barriers faced by businesses seeking to comply with our framework and ways to overcome these.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eHow accountability for regulatory compliance is currently assigned throughout the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle in real-world scenarios.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eThe ability of key regulators to implement our regulatory framework, and how we can best support them.\u003cbr\u003e\n\u003c/li\u003e\n      \u003cli\u003eBest practice in measuring and reporting on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risks across regulatory frameworks.\u003c/li\u003e\n    \u003c/ul\u003e\n  \u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e126. In the 6 to 12 months after publication we will:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eAgree partnership arrangements with leading organisations and existing initiatives to deliver the first central functions.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eEncourage key regulators to publish guidance on how the cross-sectoral principles apply within their remit.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003ePublish proposals for the design of a central \u003cabbr title=\"monitoring and evaluation\"\u003eM\u0026amp;E\u003c/abbr\u003e framework including identified metrics, data sources, and any identified thresholds or triggers for further intervention or iteration of the framework. This will be published for consultation.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eContinue to develop a regulatory sandbox or testbed with innovators and regulators.\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003e127. In the longer-term, 12 months or more after publication, we will:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eDeliver a first iteration of all the central functions required to ensure the framework is effective.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eWork with key regulators that have not published guidance on how the cross-sectoral principles apply within their remit to encourage and support them to do so.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003ePublish a draft central, cross-economy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk register for consultation.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003eDevelop the regulatory sandbox or testbed drawing on insights from the pilot.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003ePublish the first monitoring and evaluation report. This will evaluate how well the cross-sectoral principles are functioning and the delivery of the central functions. Performance will be measured against our framework characteristics: pro-innovation, proportionate, trustworthy, adaptable, clear and collaborative. The report will also consider existing regulatory activity and the role of government in supporting this, including whether appropriate guidance (including joint guidance) has been issued. In the report, we will include considerations on the need for any iteration of the framework, including the need for statutory interventions.\u003cbr\u003e\n\u003c/li\u003e\n  \u003cli\u003ePublish an updated \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Regulation Roadmap which will set out plans for the future delivery of the central functions. In particular, it will assess whether a central government team is the most appropriate mechanism for overseeing the central functions in the longer term or if a more independent body would be more effective.\u003c/li\u003e\n\u003c/ul\u003e\u003cdiv class=\"example\"\u003e\n  \u003cp\u003e\u003cstrong\u003eConsultation questions:\u003c/strong\u003e\u003c/p\u003e\n\n  \u003cp\u003e22. Do you have any other thoughts on our overall approach? Please include any missed opportunities, flaws, and gaps in our framework.\u003c/p\u003e\n\u003c/div\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Annex A: Implementation of the principles by regulators",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a-implementation-of-the-principles-by-regulators",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-a-implementation-of-the-principles-by-regulators",
        "text": "\u003ch3 id=\"a1-factors-that-government-believes-regulators-may-wish-to-consider-when-providing-guidanceimplementing-each-principle\"\u003eA.1 Factors that government believes regulators may wish to consider when providing guidance/implementing each principle\u003c/h3\u003e\u003ctable\u003e\n  \u003cthead\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"col\"\u003e Principle\u003c/th\u003e\n      \u003cth scope=\"col\"\u003eImplementation considerations\u003c/th\u003e\n    \u003c/tr\u003e\n  \u003c/thead\u003e\n  \u003ctbody\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"row\"\u003e Safety, security and robustness\u003c/th\u003e\n      \u003ctd\u003eWe anticipate that regulators will need to:\u003cbr\u003e\u003cbr\u003e 1. Provide guidance about this principle including:\u003cbr\u003e\u003cbr\u003e (i) considerations of good cybersecurity practices, such as the \u003cabbr title=\"National Cyber Security Centre\"\u003eNCSC\u003c/abbr\u003e principles for the security of machine learning,\u003csup id=\"fnref:177\"\u003e\u003ca href=\"#fn:177\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 177]\u003c/a\u003e\u003c/sup\u003e as a secured system should be capable of maintaining the integrity of information.\u003cbr\u003e\u003cbr\u003e (ii) considerations of privacy practices such as accessibility only to authorised users and safeguards against bad actors.\u003cbr\u003e\u003cbr\u003e 2. Refer to a risk management framework that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors should apply. Models should be regularly reviewed over time as a mitigation strategy.\u003cbr\u003e\u003cbr\u003e 3. Consider the role of available technical standards, for example addressing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e safety, security, testing, data quality, and robustness (including, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/79804.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 24029-2\u003c/a\u003e\u003cem\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81088.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 5259-1\u003c/a\u003e\u003c/em\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81092.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 5259-3\u003c/a\u003e* , \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81093.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 5259-4\u003c/a\u003e* , and \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81283.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 5469\u003c/a\u003e*) to clarify regulatory guidance and support the implementation of risk treatment measures.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"row\"\u003e Appropriate transparency and explainability\u003c/th\u003e\n      \u003ctd\u003eWe anticipate that regulators will need to:\u003cbr\u003e\u003cbr\u003e1. Set expectations for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors to proactively or retrospectively provide information relating to:\u003cbr\u003e\u003cbr\u003e (i) the nature and purpose of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in question including information relating to any specific outcome,\u003cbr\u003e\u003cbr\u003e (ii) the data being used and information relating to training data,\u003cbr\u003e\u003cbr\u003e (iii) the logic and process used and where relevant information to support explainability of decision-making and outcomes,\u003cbr\u003e\u003cbr\u003e (iiii) accountability for the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and any specific outcomes.\u003cbr\u003e\u003cbr\u003e2. Set explainability requirements, particularly of higher risk systems, to ensure appropriate balance between information needs for regulatory enforcement (for example, around safety) and technical tradeoffs with system robustness.\u003cbr\u003e\u003cbr\u003e3. Consider the role of available technical standards addressing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e transparency and explainability (such as \u003ca rel=\"external\" href=\"https://standards.ieee.org/ieee/7001/6929/\"\u003eIEEE 7001\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/82148.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TS 6254\u003c/a\u003e\u003cem\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/84111.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 12792\u003c/a\u003e\u003c/em\u003e)\u003csup id=\"fnref:178\"\u003e\u003ca href=\"#fn:178\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 178]\u003c/a\u003e\u003c/sup\u003e to clarify regulatory guidance and support the implementation of risk treatment measures.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"row\"\u003e Fairness\u003c/th\u003e\n      \u003ctd\u003eWe anticipate that regulators will need to:\u003cbr\u003e\u003cbr\u003e1. Interpret and articulate ‘fairness’ as relevant to their sector or domain,\u003cbr\u003e\u003cbr\u003e2. Decide in which contexts and specific instances fairness is important and relevant (which it may not always be).\u003cbr\u003e\u003cbr\u003e3. Design, implement and enforce appropriate governance requirements for ‘fairness’ as applicable to the entities that they regulate.\u003cbr\u003e\u003cbr\u003e4. Where a decision involving use of an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system has a legal or similarly significant effect on an individual, regulators will need to consider the suitability of requiring \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system operators to provide an appropriate justification for that decision to affected parties.\u003cbr\u003e\u003cbr\u003e5. \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems should comply with regulatory requirements relating to vulnerability of individuals within specific regulatory domains. Regulators will need to consider how use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems may alter individuals’ vulnerability, pursuant to their existing powers and remits.\u003cbr\u003e\u003cbr\u003e 6. Consider the role of available technical standards addressing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e fairness, bias mitigation and ethical considerations (for example, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/77607.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 24027:2021\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/84110.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 12791\u003c/a\u003e*, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/78507.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 24368:2022\u003c/a\u003e) to clarify regulatory guidance and support the implementation of risk treatment measures.\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"row\"\u003e Accountability and governance\u003c/th\u003e\n      \u003ctd\u003eWe anticipate that regulators will need to:\u003cbr\u003e\u003cbr\u003e1. Determine who is accountable for compliance with existing regulation and the principles. In the initial stages of implementation, regulators might provide guidance on how to demonstrate accountability. In the medium to long term, government may issue additional guidance on how accountability applies to specific actors within the ecosystem.\u003cbr\u003e\u003cbr\u003e2. Provide guidance on governance mechanisms including, potentially, activities in scope of appropriate risk management and governance processes (including reporting duties).\u003cbr\u003e\u003cbr\u003e3. Consider how available technical standards addressing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance, risk management, transparency and other issues can support responsible behaviour and maintain accountability within an organisation (for example, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/77304.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 23894\u003c/a\u003e\u003cem\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81230.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 42001\u003c/a\u003e\u003c/em\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/82148.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TS 6254\u003c/a\u003e\u003cem\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81283.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 5469\u003c/a\u003e\u003c/em\u003e , \u003ca rel=\"external\" href=\"https://www.iso.org/standard/80655.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 25059\u003c/a\u003e*).\u003c/td\u003e\n    \u003c/tr\u003e\n    \u003ctr\u003e\n      \u003cth scope=\"row\"\u003e Contestability and redress\u003c/th\u003e\n      \u003ctd\u003eWe anticipate that regulators will need to:\u003cbr\u003e\u003cbr\u003e1. Create or update guidance with relevant information on where to direct a complaint or dispute for those affected by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e harms. Guidance should clarify existing ‘formal’ routes of redress offered by regulators in certain scenarios.\u003cbr\u003e\u003cbr\u003e2. Clarify interactions with requirements of appropriate transparency and explainability, acting as pre-conditions of effective redress and contestability.\u003c/td\u003e\n    \u003c/tr\u003e\n  \u003c/tbody\u003e\n\u003c/table\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Annex B: Stakeholder engagement",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-b-stakeholder-engagement",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annex-b-stakeholder-engagement",
        "text": "\u003ch3 id=\"b1-summary\"\u003eB.1 Summary\u003c/h3\u003e\u003cp\u003eIn July 2022, we published a policy paper outlining our proposals for \u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement#executive-summary\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/a\u003e\u003csup id=\"fnref:179\"\u003e\u003ca href=\"#fn:179\" class=\"footnote\" rel=\"footnote\" role=\"doc-noteref\"\u003e[footnote 179]\u003c/a\u003e\u003c/sup\u003e We proposed a non-statutory framework underpinned by a set of cross-sectoral principles including transparency, safety, and security. The principles were intended to guide how regulators approach \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risks. We outlined our intention for the framework to be coherent, proportionate and adaptable, with regulatory coordination to reduce burdens on business and agility to keep pace with rapid technological advancements. Our proposals were designed to strengthen the UK’s position as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e by ensuring the UK is the best place to develop and use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies.\u003c/p\u003e\u003cp\u003eWe launched a call for views on the proposals outlined in our policy paper to capture feedback from stakeholders between July and September 2022. We received responses from over 130 different stakeholders. There were some clear themes amongst the responses, with stakeholders noting the importance of regulatory coordination and asking for further details on how this will be achieved.\u003c/p\u003e\u003cp\u003eThe 2023 \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation white paper sets out our latest position based on the feedback we received. In particular, we have considered the need for new central functions to undertake activities such as system-wide risk monitoring and evaluation of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation framework.\u003c/p\u003e\u003cp\u003eWe welcome feedback on our latest proposals and will actively engage stakeholders as part of a consultation running to 21st June. See \u003ca href=\"#annexc\"\u003eAnnex C\u003c/a\u003e for more details on how to contribute to this consultation.\u003c/p\u003e\u003ch3 id=\"b2-background\"\u003eB.2 Background\u003c/h3\u003e\u003cp\u003eIn July 2022, we opened a public call for views on our policy paper: \u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement#executive-summary\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e. We invited stakeholder views on how the UK can best set the rules for regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in a way that drives innovation and growth while also protecting our fundamental values. Feedback was collected to inform the development of the white paper.\u003c/p\u003e\u003cp\u003eWe welcomed reflections on our proposed approach and specifically invited views and supporting evidence on the following questions:\u003cbr\u003e\u003cbr\u003e1. What are the most important challenges with our existing approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e? Do you have views on the most important gaps, overlaps or contradictions?\u003cbr\u003e\u003cbr\u003e2. Do you agree with the context-driven approach delivered through the UK’s established regulators set out in this paper? What do you see as the benefits of this approach? What are the disadvantages?\u003cbr\u003e\u003cbr\u003e3. Do you agree that we should establish a set of cross-sectoral principles to guide our overall approach? Do the proposed cross-sectoral principles cover the common issues and risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies? What, if anything, is missing?\u003cbr\u003e\u003cbr\u003e4. Do you have any early views on how we best implement our approach? In your view, what are some of the key practical considerations? What will the regulatory system need to deliver on our approach? How can we best streamline and coordinate guidance on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e from regulators?\u003cbr\u003e\u003cbr\u003e5. Do you anticipate any challenges for businesses operating across multiple jurisdictions? Do you have any early views on how our approach could help support cross-border trade and international cooperation in the most effective way?\u003cbr\u003e\u003cbr\u003e6. Are you aware of any robust data sources to support monitoring the effectiveness of our approach, both at an individual regulator and system level?\u003c/p\u003e\u003cp\u003eThe call for views and evidence was open for 10 weeks, closing on 26 September 2022. In this period we met with 39 stakeholders to capture detailed feedback on our proposals. In total, we received responses from over 130 stakeholders. Stakeholders represented a range of perspectives, from start-ups to Big Tech, and included developers, deployers, and funders from across the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle. We also heard from researchers, regulators, lawyers, trade bodies and unions as well as representatives from the devolved administrations, local government, and wider public sector.\u003c/p\u003e\u003cp\u003eWe have carefully analysed all the views and evidence submitted. We are grateful for the time and effort our stakeholders committed during this process, which has informed and strengthened our policy position as outlined in the white paper.\u003c/p\u003e\u003ch3 id=\"b3-responses\"\u003eB.3 Responses\u003c/h3\u003e\u003cp\u003eOverall, there was strong support for context specific regulation implemented by existing regulators and many noted that this approach would drive innovation. Stakeholders felt our proposals were a proportionate way to establish regulatory best practice in a fast-changing landscape. However, responses also asked for more practical detail, particularly around risk tolerance, compliance measures, and the overall coherence of the framework.\u003c/p\u003e\u003cp\u003eOur analysis found 6 overarching themes raised by stakeholders:\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003e1. Articulating the intended societal benefits of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is key to a future-proofed regulatory vision that works for citizens as well as businesses.\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStakeholders were keen to see a long-term vision that set out our ambition to unlock societal benefits alongside economic opportunities. Stakeholders broadly agreed that the principles addressed the key risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. A number of stakeholders commented that our approach should explicitly reference human rights. While stakeholders welcomed our alignment with the \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e framework, many felt further use of international approaches by organisations such as the \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e or UNESCO would add more human focused benefits and aid companies working across jurisdictions. A small number of stakeholders noted that environmental sustainability was missing from our principles. Some suggested that it should be included as a core principle, while others recommended that environmental outcomes should be measured through impact assessments.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGovernment response:\u003c/strong\u003e We have analysed our principles in consideration of both stakeholder feedback and our risk assessment work. The white paper clarifies the substance of the principles in \u003ca href=\"#section323\"\u003esection 3.2.3\u003c/a\u003e. Human rights and environmental sustainability are not explicitly named in the revised principles as we expect regulators to adhere to existing law when implementing the principles. We have emphasised the social benefits alongside the economic opportunities we intend to unlock with our pro-innovation approach to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003e2. Offering greater central clarity around the scope of the regime is critical to ensuring business confidence\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eA number of stakeholders praised our description of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e for capturing the distinct regulatory challenges that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e poses and our proposed characteristics were largely considered to be fit for purpose. There were some concerns that the definition was not ‘user-friendly’ on its own. While many felt that creating a more specific definition of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would be difficult and some noted it could be unhelpful, there was clear appetite for further detail on how regulators will maintain a coherent definition of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e within and across sectors. Use cases were suggested as a means of illustrating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies within scope.\u003c/p\u003e\u003cp\u003eMany stakeholders, especially from industry, were keen to see a clear and transparent risk management framework with assessment criteria. In particular, multiple stakeholders felt that it would be beneficial for central government or a central body to provide a clear description of what constitutes ‘unacceptable risk’. Some suggested this could complement more detailed risk analysis by regulators to ensure a coordinated and coherent approach – as well as effectively identifying any gaps. Stakeholders indicated that greater clarity on risk would support business development and could also promote high standards, public trust, and the adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGovernment response:\u003c/strong\u003e We stress-tested our proposed characteristics of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e against stakeholder feedback and found that concerns centred on how we would ensure coherence across sectors and regulators. We recognise a trade-off between the certainty provided by a blanket approach, such as a singular definition and central risk framework, and the agility enabled by sector-specific expertise, including regulator-refined definitions. Given the fast pace of technological development and stakeholder praise for a future-proofed approach, we have retained our core, defining characteristics for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, see \u003ca href=\"#section321\"\u003esection 3.2.1\u003c/a\u003e. We have considered how regulators can be given the technical capability necessary to create clear definitions for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in and across their sectors, see \u003ca href=\"#section321\"\u003esection 3.2.1\u003c/a\u003e. In \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e of the white paper, we outline how new central functions will help identify conflicts or gaps in regulator definitions of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Acknowledging feedback that a central steer on ‘acceptable’ risk would provide business confidence and investment, we have proposed that centralised risk monitoring and horizon scanning would be key central functions.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003e3. A principles-based approach will enable regulation to keep pace with a fast-evolving technology\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStakeholders generally agreed that a principles-based approach implemented by regulators would offer a proportionate way to build best practice. Stakeholders felt the principles address the key risks that \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e poses while allowing regulators to tailor approaches to their sectors. Stakeholders welcomed our use of the \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e principles as a means of promoting international alignment and interoperability.\u003c/p\u003e\u003cp\u003eWhile stakeholders recognised the benefits that a flexible non-statutory approach offers, some stakeholders were concerned that a non-statutory approach would be unenforceable. A few stakeholders suggested clarifying how \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation dovetails with existing legislation and defining thresholds for when our regime may shift to statutory implementation.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGovernment response:\u003c/strong\u003e We appreciate the praise of our adaptation of the multilaterally agreed \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e principles. We further outline our international approach in the white paper, recognising that interoperability will help ensure that UK businesses can continue to innovate. While we continue with a non-statutory approach for initial implementation, reflecting on stakeholder concerns around enforceability, we anticipate that introducing a statutory duty to have due regard on regulators might be needed to strengthen the framework. A duty to have due regard to our cross-sector principles will provide a legislative incentive while maintaining flexibility for the framework to adapt to technological changes. We will monitor the implementation of the framework to assess whether it is effective without the need to implement a statutory duty and will also review responses to the white paper consultation.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003e4. Providing centralised coordination and oversight will be essential to regulatory coherence and horizon scanning\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStakeholders voiced concerns that regulators did not have the capability to ensure a coherent compliance process, especially for businesses operating across or between industry sectors or regulatory remits. Stakeholders reported expensive, time-consuming confusion when there was not clear regulatory ownership of a technology or issue. Some criticised communication and knowledge-sharing between regulators. One stakeholder explained that joint guidance had previously been very useful. Others suggested that regulators should have more stringent duties to collaborate to ensure consistency and shared best practice.\u003c/p\u003e\u003cp\u003eA number of stakeholders were supportive of a central coordination function for existing regulators, as opposed to a new regulator for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Many stressed the importance of a coordination function to aid navigation of trade-offs and conflicts (such as between the need to collect data to minimise bias and the need to refrain from collecting data in the interest of privacy). While many stakeholders stated the need for central coordination, many were solution-agnostic. Proposals included:\u003c/p\u003e\u003cul\u003e\n  \u003cli\u003eAn expanded role for the \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e. Some stakeholders suggested the \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e was well-positioned to take on a coordination function but others questioned the \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e’s suitability. In particular, it was felt that the \u003cabbr title=\"Digital Regulation Cooperation Forum\"\u003eDRCF\u003c/abbr\u003e would require more capacity to fulfil a coordination role.\u003c/li\u003e\n  \u003cli\u003eA new central body to undertake coordination. Stakeholders suggested establishing a new body, such as a ‘Centre for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Governance’, to undertake functions such as: conducting cross-sector risk-mapping; conducting regulatory gap analyses and horizon scanning; monitoring the applicability of emerging \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e standards; supplying training; and monitoring international approaches.\u003c/li\u003e\n  \u003cli\u003eAppointing an existing regulator as ‘lead regulator’ for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. Some stakeholders felt that regulators should have more incentives to work together and the entire regulatory landscape could learn from more advanced regulators.\u003c/li\u003e\n\u003c/ul\u003e\u003cp\u003eStakeholders stated the importance of clarifying regulator remits and addressing gaps, noting the fast pace of change for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies. Some suggested that a coordination body should be responsible for a horizon scanning function that monitors and evaluates risks.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGovernment response:\u003c/strong\u003e Building on reflections from stakeholders, we identified a small range of regulators with remits that are likely to be significantly affected by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and conducted analysis of their capability to implement our policy paper proposals. We found varied readiness, with some regulators already demonstrating world-leading approaches to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and others asking for further support. Similarly, knowledge and information sharing mechanisms were not uniform across regulators and we identified a need for coordination mechanisms to streamline compliance processes for business and ensure regulation provides system-wide coverage of current and future opportunities. We considered multiple options for coordination functions, in line with stakeholder suggestions, and incorporated feedback into our analysis. We outline our proposals for central functions in \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e in the white paper.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003e5. Streamlining liability and tailoring reporting obligations will be key to enabling responsible innovation\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eWhile stakeholders were strongly supportive of compliance and assurance as a means of facilitating public trust and the wider adoption of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies, many were keen to limit the burden of reporting obligations, particularly for startups and \u003cabbr title=\"small and medium-sized enterprises\"\u003eSMEs\u003c/abbr\u003e. Industry stakeholders noted that the costs of reporting burdens would be passed onto consumers. Some stakeholders emphasised that the government should have a role in providing education and support for small businesses.\u003c/p\u003e\u003cp\u003eThere was interest in regulatory sandboxes as a way to enable investment and establish best practice. Generally there was a strong appetite for industry-led solutions and a less burdensome or ‘tick box’ approach to compliance. Stakeholders were strongly supportive of standards as a way to drive accountability, adoption, and good consumer outcomes. Stakeholders suggested sector compliance templates and voluntary industry forums as ways to share knowledge and reduce the burden of establishing best practice.\u003c/p\u003e\u003cp\u003eSome stakeholders felt the paper lacked a position on liability and argued a clear allocation of legal responsibility would enable effective enforcement and unlock investment. More specifically, some stakeholders suggested that, when appropriate, targeting foundation models (often developed by larger organisations) would increase innovation and competition by reducing liability burdens on smaller companies. Stakeholders often suggested impact assessments could be used to help address liability issues at all stages of the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGovernment response:\u003c/strong\u003e We welcomed the thoughtful suggestions from respondents regarding innovative compliance measures. We noted the significant appetite for regulatory sandboxes and have outlined our proposals in the white paper, see \u003ca href=\"#section334\"\u003esection 3.3.4\u003c/a\u003e. We agree that reporting burdens should be proportionate and give detail on how we will continue to work with regulators to ensure compliance measures are streamlined. We acknowledge that regulation measures can affect competition and innovation by creating undue burdens on start-ups and \u003cabbr title=\"small and medium-sized enterprises\"\u003eSMEs\u003c/abbr\u003e. We are confident that regulators will oversee proportionate and innovation friendly measures in their remits, with a central function undertaking activity to streamline and ensure coherence. We recognise that liability is complicated by a complex \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e value-chain that can incorporate many different actors in different roles. As such, we believe that regulators are best positioned to begin allocating liability in their sectors, adopting a context-based approach that builds on best practice. Our proposal setting out activities to be undertaken centrally will ensure that regulators’ approaches to liability are proportionate, coherent across sectors, and supportive of innovation.\u003cbr\u003e\u003cbr\u003e\u003cstrong\u003e6. Establishing interoperability will be critical to ensuring an internationally competitive approach\u003c/strong\u003e\u003c/p\u003e\u003cp\u003eStakeholders welcomed the UK’s relatively flexible approach but many were concerned that the need for interoperability across jurisdictions would result in businesses conforming to the strictest regulation. Stakeholders warned that international divergence could create more burdens than advantages for businesses. Many stakeholders wanted friction minimised to ensure export prospects for British businesses, with support for an international agreement on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation equivalence, where \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems authorised on key international markets would be permitted for trade in the UK. Many stakeholders also wanted to see the UK maintain its position as a global leader in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e discussions. Stakeholders emphasised the importance of alignment with international partners such as the EU and US to ensure global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance supports our common democratic values.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eGovernment response:\u003c/strong\u003e In the white paper, we set out our vision for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation to ensure that the UK is the best place to start and grow an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e business. We share stakeholder concerns on interoperability and plan to continue using our leading role in international forums such as the \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e, G7, and Council of Europe to promote pro-innovation approaches to regulation that capitalise on the potential social and economic benefits of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e while addressing the new risks the technology can pose. Our plan for international engagement, detailed in \u003ca href=\"#partsix\"\u003epart 6\u003c/a\u003e, clarifies our approach with an emphasis on interoperability.\u003c/p\u003e"
      }
    },
    {
      "@type": "Question",
      "name": "Annex C: How to respond to this consultation",
      "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annexc",
      "acceptedAnswer": {
        "@type": "Answer",
        "url": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach/white-paper#annexc",
        "text": "\u003cp\u003eWe are inviting individuals and organisations to provide their views by responding to the questions set out in this consultation. The questions are listed below.\u003c/p\u003e\u003cp\u003eThe consultation will be open for 12 weeks, until 21 June.\u003c/p\u003e\u003cp\u003eYou can respond online via the \u003ca rel=\"external\" href=\"https://dcms.eu.qualtrics.com/jfe/form/SV_cBDeiMplOHExtYO\"\u003econsultation survey link here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eOur \u003ca href=\"https://www.gov.uk/government/publications/office-for-artificial-intelligence-information-collection-and-analysis-privacy-notice\"\u003eprivacy statement is set out here\u003c/a\u003e.\u003c/p\u003e\u003cp\u003eIf for exceptional reasons, you are unable to use the online system, for example because you use specialist accessibility software that is not compatible with the system, you may request and complete a word document version of the form.\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBy email\u003c/strong\u003e\u003cbr\u003e\n\u003ca href=\"mailto:evidence@officeforai.gov.uk\"\u003eevidence@officeforai.gov.uk\u003c/a\u003e\u003c/p\u003e\u003cp\u003e\u003cstrong\u003eBy post\u003c/strong\u003e\u003c/p\u003e\u003cdiv class=\"address\"\u003e\u003cdiv class=\"adr org fn\"\u003e\u003cp\u003e\nOffice for Artificial Intelligence\u003cbr\u003eDepartment for Science, Innovation and Technology\u003cbr\u003e100 Parliament Street\u003cbr\u003eLondon\u003cbr\u003eSW1A 2BQ\n\u003c/p\u003e\u003c/div\u003e\u003c/div\u003e\u003cdiv class=\"example\"\u003e\n  \u003ch3 id=\"questions\"\u003eQuestions:\u003c/h3\u003e\n\n  \u003ch4 id=\"the-revised-cross-sectoral-ai-principles\"\u003eThe revised cross-sectoral \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e principles\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n1. Do you agree that requiring organisations to make it clear when they are using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would adequately ensure transparency?\u003c/p\u003e\n\n  \u003cp\u003e2. What other transparency measures would be appropriate, if any?\u003c/p\u003e\n\n  \u003cp\u003e3. Do you agree that current routes to contestability or redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms are adequate?\u003c/p\u003e\n\n  \u003cp\u003e4. How could routes to contestability or redress for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related harms be improved, if at all?\u003c/p\u003e\n\n  \u003cp\u003e5. Do you agree that, when implemented effectively, the revised cross-sectoral principles will cover the risks posed by \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies?\u003c/p\u003e\n\n  \u003cp\u003e6. What, if anything, is missing from the revised principles?\u003c/p\u003e\n\n  \u003ch4 id=\"a-statutory-duty-to-regard\"\u003eA statutory duty to regard\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n7. Do you agree that introducing a statutory duty on regulators to have due regard to the principles would clarify and strengthen regulators’ mandates to implement our principles, while retaining a flexible approach to implementation?\u003c/p\u003e\n\n  \u003cp\u003e8. Is there an alternative statutory intervention that would be more effective?\u003c/p\u003e\n\n  \u003ch4 id=\"new-central-functions-to-support-the-framework\"\u003eNew central functions to support the framework\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n9. Do you agree that the functions outlined in \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e would benefit our \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation framework if delivered centrally?\u003c/p\u003e\n\n  \u003cp\u003e10. What, if anything, is missing from the central functions?\u003c/p\u003e\n\n  \u003cp\u003e11. Do you know of any existing organisations who should deliver one or more of our proposed central functions?\u003c/p\u003e\n\n  \u003cp\u003e12. Are there additional activities that would help businesses confidently innovate and use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies?\u003c/p\u003e\n\n  \u003cp\u003e12.1. If so, should these activities be delivered by government, regulators or a different organisation?\u003c/p\u003e\n\n  \u003cp\u003e13. Are there additional activities that would help individuals and consumers confidently use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e technologies?\u003c/p\u003e\n\n  \u003cp\u003e13.1. If so, should these activities be delivered by government, regulators or a different organisation?\u003c/p\u003e\n\n  \u003cp\u003e14. How can we avoid overlapping, duplicative or contradictory guidance on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e issued by different regulators?\u003c/p\u003e\n\n  \u003ch4 id=\"monitoring-and-evaluation-of-the-framework\"\u003eMonitoring and evaluation of the framework\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n15. Do you agree with our overall approach to monitoring and evaluation?\u003c/p\u003e\n\n  \u003cp\u003e16. What is the best way to measure the impact of our framework?\u003c/p\u003e\n\n  \u003cp\u003e17. Do you agree that our approach strikes the right balance between supporting \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovation; addressing known, prioritised risks; and future-proofing the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation framework?\u003c/p\u003e\n\n  \u003cp\u003e18. Do you agree that regulators are best placed to apply the principles and government is best placed to provide oversight and deliver central functions?\u003c/p\u003e\n\n  \u003ch4 id=\"regulator-capabilities-1\"\u003eRegulator capabilities\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n19. As a regulator, what support would you need in order to apply the principles in a proportionate and pro-innovation way?\u003c/p\u003e\n\n  \u003cp\u003e20. Do you agree that a pooled team of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e experts would be the most effective way to address capability gaps and help regulators apply the principles?\u003c/p\u003e\n\n  \u003ch4 id=\"tools-for-trustworthy-ai\"\u003eTools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\n\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n21. Which non-regulatory tools for trustworthy \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e would most help organisations to embed the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulation principles into existing business processes?\u003c/p\u003e\n\n  \u003ch4 id=\"final-thoughts\"\u003eFinal thoughts\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\n22. Do you have any other thoughts on our overall approach? Please include any missed opportunities, flaws, and gaps in our framework.\u003c/p\u003e\n\n  \u003ch4 id=\"legal-responsibility-for-ai\"\u003eLegal responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\n\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nL1. What challenges might arise when regulators apply the principles across different \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e applications and systems? How could we address these challenges through our proposed \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory framework?\u003c/p\u003e\n\n  \u003cp\u003eL2.i. Do you agree that the implementation of our principles through existing legal frameworks will fairly and effectively allocate legal responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e across the life cycle?\u003c/p\u003e\n\n  \u003cp\u003eL.2.ii. How could it be improved, if at all?\u003c/p\u003e\n\n  \u003cp\u003eL3. If you are a business that develops, uses, or sells \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, how do you currently manage \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk including through the wider supply chain? How could government support effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-related risk management?\u003c/p\u003e\n\n  \u003ch4 id=\"foundation-models-and-the-regulatory-framework\"\u003eFoundation models and the regulatory framework\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nF1. What specific challenges will foundation models such as large language models (\u003cabbr title=\"large language models\"\u003eLLMs\u003c/abbr\u003e) or open-source models pose for regulators trying to determine legal responsibility for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e outcomes?\u003c/p\u003e\n\n  \u003cp\u003eF2. Do you agree that measuring compute provides a potential tool that could be considered as part of the governance of foundation models?\u003c/p\u003e\n\n  \u003cp\u003eF3. Are there other approaches to governing foundation models that would be more effective?\u003c/p\u003e\n\n  \u003ch4 id=\"ai-sandboxes-and-testbeds\"\u003e\n\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandboxes and testbeds\u003c/h4\u003e\n\n  \u003cp\u003e\u003cbr\u003e\nS1. Which of the sandbox models described in \u003ca href=\"#section334\"\u003esection 3.3.4\u003c/a\u003e would be most likely to support innovation?\u003c/p\u003e\n\n  \u003cp\u003eS2. What could government do to maximise the benefit of sandboxes to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e innovators\u003c/p\u003e\n\n  \u003cp\u003eS3. What could government do to facilitate participation in an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e regulatory sandbox?\u003c/p\u003e\n\n  \u003cp\u003eS4. Which industry sectors or classes of product would most benefit from an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sandbox?\u003c/p\u003e\n\u003c/div\u003e\u003cdiv class=\"footnotes\" role=\"doc-endnotes\"\u003e\n  \u003col\u003e\n    \u003cli id=\"fn:1\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.insiderintelligence.com/insights/artificial-intelligence-healthcare/\"\u003eThe use of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in healthcare and medicine is booming\u003c/a\u003e, Insider Intelligence, 2023. \u003ca href=\"#fnref:1\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:2\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.forbes.com/sites/markminevich/2022/07/08/how-to-fight-climate-change-using-ai/?sh=5f274222a838\"\u003eHow to fight climate change using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Forbes, 2022; \u003ca rel=\"external\" href=\"https://arxiv.org/abs/1906.05433\"\u003eTackling Climate Change with Machine Learning\u003c/a\u003e, Rolnick et al., 2019. \u003ca href=\"#fnref:2\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:3\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.newscientist.com/article/2330866-deepminds-protein-folding-ai-cracks-biologys-biggest-problem/\"\u003eDeepMind’s protein-folding \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e cracks biology’s biggest problem, New Scientist,\u003c/a\u003e 2022; \u003ca rel=\"external\" href=\"https://www.nature.com/articles/s41586-019-1923-7\"\u003eImproved protein structure prediction using potentials from deep learning\u003c/a\u003e, Senior et al., 2020. \u003ca href=\"#fnref:3\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:4\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/uk-science-and-technology-framework/the-uk-science-and-technology-framework#regulation-and-standards\"\u003eThe UK Science and Technology Framework\u003c/a\u003e, Department for Science, Innovation and Technology, 2023. \u003ca href=\"#fnref:4\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:5\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://technologymagazine.com/articles/six-of-the-best-future-uses-for-artificial-intelligence?utm_campaign=Artificial%2BIntelligence%2BWeekly\u0026amp;utm_medium=email\u0026amp;utm_source=Artificial_Intelligence_Weekly_316\"\u003eSix of the best future uses of Artificial Intelligence\u003c/a\u003e, Technology Magazine, 2023; \u003ca rel=\"external\" href=\"https://www.sciencedirect.com/science/article/abs/pii/S026840121930917X?via%3Dihub\"\u003eMultidisciplinary perspectives on emerging challenges, opportunities, and agenda for research, practice and policy\u003c/a\u003e, Dwivedi et al., 2021. \u003ca href=\"#fnref:5\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:6\"\u003e\n      \u003cp\u003eLarge dedicated \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies make a major contribution to the UK economy, with GVA (gross value added) per employee estimated to be £400k, more than double that of comparable estimates of large dedicated firms in other sectors. See \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study 2022, DSIT, 2023\u003c/a\u003e. \u003ca href=\"#fnref:6\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:7\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:7\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:8\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/ai-barometer-2021/ai-barometer-part-4-transport-and-logistics#risks\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Barometer Part 4 –Transport and logistics\u003c/a\u003e, Centre for Data Ethics and Innovation, 2021. \u003ca href=\"#fnref:8\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:9\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.nytimes.com/2021/12/05/business/media/tiktok-algorithm.html\"\u003eHow TikTok Reads Your Mind\u003c/a\u003e, New York Times, 2021. \u003ca href=\"#fnref:9\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:10\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://ai.googleblog.com/2020/12/privacy-considerations-in-large.html\"\u003ePrivacy Considerations in Large Language Models\u003c/a\u003e, Google Research, 2020. \u003ca href=\"#fnref:10\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:11\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.turing.ac.uk/sites/default/files/2021-03/cahai_feasibility_study_primer_final.pdf\"\u003eArtificial Intelligence, Human Rights, Democracy, and the Rule of Law,\u003c/a\u003e Alan Turing Institute and Council of Europe, 2021. \u003ca href=\"#fnref:11\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:12\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.oecd-ilibrary.org/science-and-technology/demand-for-ai-skills-in-jobs_3ed32d94-en\"\u003eDemand for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e skills in jobs\u003c/a\u003e, \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e iLibrary, 2021. \u003ca href=\"#fnref:12\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:13\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance\"\u003ePublic expectations for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance (transparency, fairness and accountability)\u003c/a\u003e, Centre for Data Ethics and Innovation, 2023. \u003ca href=\"#fnref:13\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:14\"\u003e\n      \u003cp\u003eThe \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sector is estimated to contribute £3.7bn in GVA (Gross Value Added) to the UK economy. \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study 2022\u003c/a\u003e, DSIT, 2023. \u003ca href=\"#fnref:14\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:15\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Office for Artificial Intelligence, 2022. \u003ca href=\"#fnref:15\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:16\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.tortoisemedia.com/intelligence/global-ai/\"\u003eGlobal \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Index,\u003c/a\u003e Tortoise Media, 2022. \u003ca href=\"#fnref:16\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:17\"\u003e\n      \u003cp\u003eTransport apps like \u003ca rel=\"external\" href=\"http://maps.google.com\"\u003eGoogle Maps\u003c/a\u003e, and \u003ca rel=\"external\" href=\"http://citymapper.com\"\u003eCityMapper\u003c/a\u003e, use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. \u003ca href=\"#fnref:17\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:18\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://researchberg.com/index.php/rrst/article/view/37\"\u003eArtificial Intelligence in Banking Industry: A Review on Fraud Detection, Credit Management, and Document Processing\u003c/a\u003e, ResearchBerg Review of Science and Technology, 2018. \u003ca href=\"#fnref:18\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:19\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.deepmind.com/blog/accelerating-fusion-science-through-learned-plasma-control\"\u003eAccelerating fusion science through learned plasma control\u003c/a\u003e, Deepmind, 2022; \u003ca rel=\"external\" href=\"https://www.nature.com/articles/s41586-021-04301-9\"\u003eMagnetic control of tokamak plasmas through deep reinforcement learning\u003c/a\u003e, Degrave et al., 2022. \u003ca href=\"#fnref:19\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:20\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.morganstanley.com/ideas/ai-drug-discovery#:~:text=Biotechs%20are%20applying%20AI%20and,new%20drugs%20is%20costly%20guesswork.\"\u003eWhy Artificial Intelligence Could Speed Drug Discovery,\u003c/a\u003e Morgan Stanley, 2022. \u003ca href=\"#fnref:20\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:21\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.bcg.com/publications/2022/how-ai-can-help-climate-change\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Is Essential for Solving the Climate Crisis\u003c/a\u003e, BCG, 2022. \u003ca href=\"#fnref:21\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:22\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.nber.org/papers/w11093\"\u003eGeneral Purpose Technologies – Handbook of Economic Growth\u003c/a\u003e, National Bureau of Economic Research, 2005. \u003ca href=\"#fnref:22\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:23\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/uk-science-and-technology-framework\"\u003eThe UK Science and Technology Framework\u003c/a\u003e, Department for Science, Innovation and Technology, 2023. \u003ca href=\"#fnref:23\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:24\"\u003e\n      \u003cp\u003eIn 2022 annual revenues generated by UK \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies totalled an estimated £10.6 billion. \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study 2022\u003c/a\u003e, DSIT, 2023. \u003ca href=\"#fnref:24\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:25\"\u003e\n      \u003cp\u003eDSIT analysis estimates over 50,000 full time workers are employed in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e roles in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e companies. \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study 2022\u003c/a\u003e, DSIT, 2023. \u003ca href=\"#fnref:25\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:26\"\u003e\n      \u003cp\u003eFor example, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e can potentially improve health and safety in mining while also improving efficiency. See \u003ca rel=\"external\" href=\"https://www.axora.com/insights/how-ai-is-being-used-to-improve-health-and-safety-in-mining/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e on-side: how artificial intelligence is being used to improve health and safety in mining\u003c/a\u003e, Axora, 2023. \u003ca href=\"#box11\"\u003eBox 1.1\u003c/a\u003e gives further examples of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e driving efficiency improvements. \u003ca href=\"#fnref:26\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:27\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.forbes.com/sites/garydrenik/2023/01/11/large-language-models-will-define-artificial-intelligence/\"\u003eLarge Language Models Will Define Artificial Intelligence\u003c/a\u003e, Forbes, 2023; \u003ca rel=\"external\" href=\"https://arxiv.org/abs/2112.04426\"\u003eScaling Language Models: Methods, Analysis \u0026amp; Insights from Training Gopher\u003c/a\u003e, Borgeaud et al., 2022. \u003ca href=\"#fnref:27\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:28\"\u003e\n      \u003cp\u003eSee, for example, \u003ca rel=\"external\" href=\"https://blogs.nvidia.com/blog/2023/01/26/what-are-large-language-models-used-for/\"\u003eWhat are Large Language Models used for?\u003c/a\u003e NVIDIA, 2023. \u003ca href=\"#fnref:28\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:29\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.nature.com/articles/d41586-019-01155-0\"\u003eBlack hole pictured for first time – in spectacular detail\u003c/a\u003e, Nature, 2019. \u003ca href=\"#fnref:29\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:30\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://unfolded.deepmind.com/stories/unlocking-a-decade-of-data-to-fight-antibiotic-resistance\"\u003eAccelerating the race against antibiotic resistance\u003c/a\u003e, Deepmind, 2022. \u003ca href=\"#fnref:30\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:31\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://unfolded.deepmind.com/stories/matthew-higgins-is-unlocking-a-new-path-to-stop-malaria-in-its-tracks\"\u003eStopping malaria in its tracks\u003c/a\u003e, Deepmind, 2022. \u003ca href=\"#fnref:31\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:32\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://unfolded.deepmind.com/stories/accelerating-the-fight-against-plastic-pollution\"\u003eCreating plastic-eating enzymes that could save us from pollution\u003c/a\u003e, Deepmind, 2022. \u003ca href=\"#fnref:32\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:33\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://transform.england.nhs.uk/ai-lab/explore-all-resources/understand-ai/mia-mammography-intelligent-assessment/\"\u003eMia mammography intelligent assessment\u003c/a\u003e, \u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e England, 2021. \u003ca href=\"#fnref:33\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:34\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://link.springer.com/article/10.1007/s43154-022-00077-6\"\u003eRobotics and Autonomous Systems for Net Zero Agriculture\u003c/a\u003e, Pearson et al., 2022. \u003ca href=\"#fnref:34\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:35\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ingentaconnect.com/content/ben/cdt/2021/00000022/00000006/art00005\"\u003eArtificial intelligence, big data and machine learning approaches to precision medicine and drug discovery\u003c/a\u003e, Current Drug Targets, 2021. \u003ca href=\"#fnref:35\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:36\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.biorxiv.org/content/10.1101/2023.01.08.523187v1.full.pdf\"\u003eUnlocking de novo antibody design with generative artificial intelligence\u003c/a\u003e, Shanehsazzadeh et al., 2023. \u003ca href=\"#fnref:36\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:37\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/news/pioneering-new-tools-to-be-rolled-out-in-fight-against-child-abusers\"\u003ePioneering new tools to be rolled out in fight against child abusers\u003c/a\u003e, Home Office, 2019. \u003ca href=\"#fnref:37\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:38\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ncsc.gov.uk/collection/intelligent-security-tools\"\u003eIntelligent security tools\u003c/a\u003e, National Cyber Security Centre, 2019. \u003ca href=\"#fnref:38\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:39\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.vox.com/recode/2023/1/5/23539055/generative-ai-chatgpt-stable-diffusion-lensa-dall-e\"\u003eWhat is generative \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, and why is it suddenly everywhere?\u003c/a\u003e, Vox, 2023. \u003ca href=\"#fnref:39\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:40\"\u003e\n      \u003cp\u003eSee, for example, \u003ca href=\"https://www.gov.uk/government/publications/findings-from-the-drcf-algorithmic-processing-workstream-spring-2022/the-benefits-and-harms-of-algorithms-a-shared-perspective-from-the-four-digital-regulators\"\u003eThe Benefits and Harms of Algorithms\u003c/a\u003e, The Digital Regulation Cooperation Forum, 2022; \u003ca rel=\"external\" href=\"https://www.nber.org/system/files/working_papers/w29247/w29247.pdf\"\u003eHarms of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Acemoglu, 2021. \u003ca href=\"#fnref:40\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:41\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://cset.georgetown.edu/publication/ai-accidents-an-emerging-threat/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Accidents: An Emerging Threat\u003c/a\u003e, Center for Security and Emerging Technology, 2021. \u003ca href=\"#fnref:41\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:42\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.nature.com/articles/s42256-021-00338-7\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e for radiographic COVID-19 detection selects shortcuts over signal\u003c/a\u003e, DeGrave, Janizek and Lee, 2021; \u003ca rel=\"external\" href=\"https://5rightsfoundation.com/uploads/Pathways-how-digital-design-puts-children-at-risk.pdf\"\u003ePathways: How digital design puts children at risk\u003c/a\u003e, 5Rights Foundation, 2021. \u003ca href=\"#fnref:42\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:43\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://maliciousaireport.com/\"\u003eThe Malicious Use of Artificial Intelligence\u003c/a\u003e, Malicious \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Report, 2018. \u003ca href=\"#fnref:43\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:44\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.cambridge.org/core/books/constitutional-challenges-in-the-algorithmic-society/831B39F76C7870B330052D852D598F98\"\u003eConstitutional Challenges in the Algorithmic Society\u003c/a\u003e, Micklitz et al., 2022. \u003ca href=\"#fnref:44\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:45\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-its-first-series-of-three-snapshot-papers-ethical-issues-in-ai/snapshot-paper-smart-speakers-and-voice-assistants\"\u003eSmart Speakers and Voice Assistants\u003c/a\u003e, CDEI, 2019; \u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-its-first-series-of-three-snapshot-papers-ethical-issues-in-ai/snapshot-paper-deepfakes-and-audiovisual-disinformation\"\u003eDeepfakes and Audiovisual disinformation\u003c/a\u003e, CDEI, 2019. \u003ca href=\"#fnref:45\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:46\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.turing.ac.uk/sites/default/files/2021-03/cahai_feasibility_study_primer_final.pdf\"\u003eArtificial Intelligence, Human Rights, Democracy and the Rule of Law\u003c/a\u003e, Leslie et al., 2021. \u003ca href=\"#fnref:46\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:47\"\u003e\n      \u003cp\u003eGovernment has already committed to addressing some of these issues more broadly. See, for example, the \u003ca href=\"https://www.gov.uk/government/publications/inclusive-britain-action-plan-government-response-to-the-commission-on-race-and-ethnic-disparities/inclusive-britain-government-response-to-the-commission-on-race-and-ethnic-disparities\"\u003eInclusive Britain report\u003c/a\u003e, Race Disparity Unit, 2022. \u003ca href=\"#fnref:47\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:48\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://techcrunch.com/2023/01/11/chatgpt-cybersecurity-threat/?guccounter=1\u0026amp;guce_referrer=aHR0cHM6Ly93d3cuZ29vZ2xlLmNvbS8\u0026amp;guce_referrer_sig=AQAAAFztF-r2eW_zAxjt0v_rnED11-KLH5D1tr27pMb-xJwpH0yky8pGrLvLjlRloW03L-Adh7fEsTlJUGR32p00S09VfBlalHUC0Xl1YbV5JTZqCzXlEkFVKkCa7J33Y-A5we3JMmCIyNmK9UeDWE_Hoiz5p6rUFWBdReh8twfUQBiy#:~:text=One%20hacker%20on%20a%20dark,script%20they%20had%20ever%20created.\"\u003e‘Is ChatGPT a cybersecurity threat?\u003c/a\u003e’ TechCrunch, 2023. \u003ca href=\"#fnref:48\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:49\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://research.checkpoint.com/2023/opwnai-cybercriminals-starting-to-use-chatgpt/\"\u003eOPWNAI: Cybercriminals starting to use ChatGPT\u003c/a\u003e, Check Point Research, 2023. \u003ca href=\"#fnref:49\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:50\"\u003e\n      \u003cp\u003eThese are not intended to be legal definitions for the purposes of the framework. \u003ca href=\"#fnref:50\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:51\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.adalovelaceinstitute.org/blog/value-chain-general-purpose-ai/\"\u003eThe value chain of general-purpose \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Ada Lovelace Institute, 2023. \u003ca href=\"#fnref:51\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:52\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.globalinnovationindex.org/gii-2022-report\"\u003eGlobal Innovation Index 2022,\u003c/a\u003e GII 2022; \u003ca rel=\"external\" href=\"https://rulemaking.worldbank.org/en/data/explorecountries/united-kingdom\"\u003eGlobal Indicators of Regulatory Governance, World Bank, 2023\u003c/a\u003e. \u003ca href=\"#fnref:52\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:53\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.oecd-ilibrary.org/science-and-technology/demand-for-ai-skills-in-jobs_3ed32d94-en\"\u003eDemand for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e skills in jobs\u003c/a\u003e, \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e Science, Technology and Industry Working Papers, 2021. \u003ca href=\"#fnref:53\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:54\"\u003e\n      \u003cp\u003eThe protected characteristics are age, disability, gender reassignment, marriage and civil partnership, race, religion or belief, sex, and sexual orientation. \u003ca href=\"#fnref:54\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:55\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/eur/2016/679/article/5\"\u003eArticle 5(1)(a) Principles relating to processing of personal data\u003c/a\u003e, HM Government, 2016. \u003ca href=\"#fnref:55\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:56\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/uksi/2016/1101/contents\"\u003eElectrical Equipment (Safety) Regulations\u003c/a\u003e, HM Government, 2016. \u003ca href=\"#fnref:56\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:57\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/uksi/2002/618/contents/made\"\u003eMedical Devices Regulation\u003c/a\u003e, HM Government, 2002. \u003ca href=\"#fnref:57\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:58\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/uksi/2011/1881/contents\"\u003eToys (Safety) Regulations\u003c/a\u003e, HM Government, 2011. \u003ca href=\"#fnref:58\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:59\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2015/15/contents\"\u003eConsumer Rights Act 2015\u003c/a\u003e; \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/uksi/2008/1277/contents\"\u003eConsumer Protection from Unfair Trading Regulations\u003c/a\u003e, HM Government, 2008. \u003ca href=\"#fnref:59\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:60\"\u003e\n      \u003cp\u003eSuch as the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2000/8/contents\"\u003eFinancial Services and Markets Act\u003c/a\u003e, HM Government, 2000. \u003ca href=\"#fnref:60\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:61\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/evidence-to-support-the-analysis-of-impacts-for-artificial-intelligence-governance\"\u003eEvidence to support the analysis of impacts for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance\u003c/a\u003e, Frontier Economics, 2023. \u003ca href=\"#fnref:61\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:62\"\u003e\n      \u003cp\u003eIn 2019, 98.8% of businesses in the digital sector had less than 50 employees. \u003ca href=\"https://www.gov.uk/government/collections/dcms-sectors-economic-estimates\"\u003eDCMS Sectors Economic Estimates 2019: Business Demographics\u003c/a\u003e, ONS, 2022. \u003ca href=\"#fnref:62\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:63\"\u003e\n      \u003cp\u003eThe \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study found that almost 90% of businesses in the \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sector are small or micro in size. \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study 2022\u003c/a\u003e, DSIT, 2023 \u003ca href=\"#fnref:63\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:64\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.digitalregulations.innovation.nhs.uk/about-this-service/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and Digital Regulations Service\u003c/a\u003e, Care Quality Commission, Health Research Authority, Medicines and Healthcare Products Regulatory Agency, National Institute for Health and Care Excellence, 2023. \u003ca href=\"#fnref:64\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:65\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2000/8/contents\"\u003eFinancial Services and Markets Act\u003c/a\u003e, HM Government, 2000 \u003ca href=\"#fnref:65\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:66\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme/software-and-ai-as-a-medical-device-change-programme-roadmap\"\u003eSoftware and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as a Medical Device Change Programme – Roadmap\u003c/a\u003e, \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e, 2022. \u003ca href=\"#fnref:66\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:67\"\u003e\n      \u003cp\u003eThe exact relation between the concepts ‘interpretability’ and ‘explainability’ is the subject of ongoing academic debate. See \u003ca rel=\"external\" href=\"https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1493\"\u003eInterpretable and explainable machine learning: A methods-centric overview with concrete examples\u003c/a\u003e, Marcinkevics and Vogt, 2023. We use ‘explainability’ as the key term in our \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e principle in alignment with the \u003ca rel=\"external\" href=\"https://oecd.ai/en/ai-principles\"\u003e\u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e\u003c/a\u003e. \u003ca href=\"#fnref:67\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:68\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.nesta.org.uk/report/the-impact-of-regulation-on-innovation/\"\u003eThe impact of regulation on innovation\u003c/a\u003e, Nesta, 2012. \u003ca href=\"#fnref:68\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:69\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance\"\u003ePublic expectations for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance (transparency, fairness and accountability)\u003c/a\u003e, Centre for Data Ethics and Innovation, 2023. \u003ca href=\"#fnref:69\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:70\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/national-ai-strategy\"\u003eNational \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Strategy\u003c/a\u003e, Office for Artificial Intelligence, 2021. \u003ca href=\"#fnref:70\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:71\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/digital-regulation-driving-growth-and-unlocking-innovation\"\u003ePlan for Digital Regulation\u003c/a\u003e, DSIT (formerly DCMS), 2022. \u003ca href=\"#fnref:71\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:72\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Office for Artificial Intelligence, 2022. \u003ca href=\"#fnref:72\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:73\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.europarl.europa.eu/RegData/etudes/BRIE/2019/637967/EPRS_BRI(2019)637967_EN.pdf\"\u003eEconomic impacts of artificial intelligence\u003c/a\u003e, European Parliament, 2019. \u003ca href=\"#fnref:73\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:74\"\u003e\n      \u003cp\u003eThe UK is ranked near the top of the Global \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Index, third only to the US and China. \u003ca rel=\"external\" href=\"https://www.tortoisemedia.com/intelligence/global-ai/\"\u003eGlobal \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Index\u003c/a\u003e, Tortoise Media, 2022. \u003ca href=\"#fnref:74\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:75\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://kpmg.com/de/en/home/insights/2021/06/artificial-intelligence-five-country-study.html\"\u003eTrust in Artificial Intelligence: a five country study\u003c/a\u003e, KPMG and the University of Queensland, 2021; \u003ca href=\"https://www.gov.uk/government/publications/evidence-to-support-the-analysis-of-impacts-for-artificial-intelligence-governance\"\u003eEvidence to support the analysis of impacts for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance\u003c/a\u003e, Frontier Economics, 2023. \u003ca href=\"#fnref:75\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:76\"\u003e\n      \u003cp\u003e“Building on the UK’s strengths in the professional services and technology sectors, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance will also become a significant economic activity in its own right, with the potential for the UK to be a global leader in a new multi-billion pound industry.” See \u003ca href=\"https://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-ecosystem\"\u003eThe roadmap to an effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance ecosystem\u003c/a\u003e, Centre for Data Ethics and Innovation, 2021. \u003ca href=\"#fnref:76\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:77\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:77\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:78\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:78\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:79\"\u003e\n      \u003cp\u003eThese characteristics are aligned with existing principles set out in the \u003ca href=\"https://www.gov.uk/government/publications/digital-regulation-driving-growth-and-unlocking-innovation\"\u003ePlan for Digital Regulation\u003c/a\u003e, the \u003ca href=\"https://www.gov.uk/government/publications/taskforce-on-innovation-growth-and-regulatory-reform-independent-report\"\u003ereport of the independent Taskforce on Innovation, Growth and Regulatory Reform\u003c/a\u003e and with the findings of the \u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, published in March 2023, which called for a proportionate and agile regulatory approach and acknowledged the importance of achieving a “balance between providing clarity and building public trust, while also enabling development, experimentation, and deployment.” \u003ca href=\"#fnref:79\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:80\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://carnegieendowment.org/2022/10/06/one-of-biggest-problems-in-regulating-ai-is-agreeing-on-definition-pub-88100\"\u003eOne of the biggest problems in regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is agreeing on a definition\u003c/a\u003e, Carnegie Endowment for International Peace, 2022. \u003ca href=\"#fnref:80\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:81\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Office for Artificial Intelligence, 2022. \u003ca href=\"#fnref:81\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:82\"\u003e\n      \u003cp\u003eAs stated in government guidance on using \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in the public sector, we consider machine learning to be a subset of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e. While machine learning is the most widely-used form of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and will be captured within our framework, our adaptive and autonomous characteristics ensure any current or future \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system that meets this criteria will be within scope. See \u003ca href=\"https://www.gov.uk/government/collections/a-guide-to-using-artificial-intelligence-in-the-public-sector\"\u003eA guide to using artificial intelligence in the public sector\u003c/a\u003e, Government Digital Service and Office for Artificial Intelligence, 2019. \u003ca href=\"#fnref:82\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:83\"\u003e\n      \u003cp\u003eSee \u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Office for Artificial Intelligence, 2022. The context-based approach received wide support in feedback received following publication of this policy paper. \u003ca href=\"#fnref:83\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:84\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://smartwatermagazine.com/news/fido-tech/fido-direct-launched-end-end-solution-solve-water-loss\"\u003eFIDO Direct launched as end-to-end solution to solve water loss\u003c/a\u003e, Smart Water Magazine, 2023. \u003ca href=\"#fnref:84\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:85\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.axora.com/insights/how-ai-is-being-used-to-improve-health-and-safety-in-mining/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e on-side: how artificial intelligence is being used to improve health and safety in mining\u003c/a\u003e, Axora, 2023. \u003ca href=\"#fnref:85\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:86\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/digital-regulation-driving-growth-and-unlocking-innovation\"\u003ePlan for Digital Regulation\u003c/a\u003e, DSIT (formerly DCMS), 2021. \u003ca href=\"#fnref:86\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:87\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/taskforce-on-innovation-growth-and-regulatory-reform-independent-report\"\u003eThe Taskforce on Innovation, Growth and Regulatory Reform independent report\u003c/a\u003e, 10 Downing Street, 2021. The report argues for UK regulation that is: proportionate, forward-looking, outcome-focussed, collaborative, experimental, and responsive. \u003ca href=\"#fnref:87\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:88\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1083582/closing-the-gap-regulation-full-report.pdf\"\u003eClosing the gap: getting from principles to practices for innovation friendly regulation\u003c/a\u003e, Regulatory Horizons Council, 2022. \u003ca href=\"#fnref:88\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:89\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:89\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:90\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Office for Artificial Intelligence, 2022. \u003ca href=\"#fnref:90\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:91\"\u003e\n      \u003cp\u003eThe Centre for Data Ethics and Innovation (CDEI) has engaged with the public to understand their expectations for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance. This engagement has informed our policy development. Participants also referred to a privacy principle, which is embedded in the broader regulatory considerations as regulators and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e life cycle actors are expected to comply with the UK’s data protection framework. \u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance\"\u003ePublic expectations for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance (transparency, fairness and accountability)\u003c/a\u003e, Centre for Data Ethics and Innovation, 2023. \u003ca href=\"#fnref:91\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:92\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ncsc.gov.uk/collection/machine-learning\"\u003ePrinciples for the security of machine learning\u003c/a\u003e, National Cyber Security Centre, 2022 \u003ca href=\"#fnref:92\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:93\"\u003e\n      \u003cp\u003eFor example, digital security can affect the safety of connected products such as automobiles and home appliances if risks are not appropriately managed. See \u003ca rel=\"external\" href=\"https://oecd.ai/en/dashboards/ai-principles/P8\"\u003ePrinciple 1.4:Robustness, security and safety, \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, 2019\u003c/a\u003e. \u003ca href=\"#fnref:93\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:94\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://standards.ieee.org/ieee/7001/6929/\"\u003eAdapted from IEEE 7001-2021, Standard for Transparency of Autonomous Systems\u003c/a\u003e. \u003ca href=\"#fnref:94\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:95\"\u003e\n      \u003cp\u003eFor example \u003ca rel=\"external\" href=\"https://standards.ieee.org/ieee/7001/6929/\"\u003eIEEE 7001-2021\u003c/a\u003e (Active Standard) describes measurable, testable levels of transparency so that autonomous systems can be objectively assessed, and levels of compliance determined; \u003ca rel=\"external\" href=\"https://www.iso.org/standard/82148.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TS6254\u003c/a\u003e (Under development) will describe approaches and methods that can be used to achieve explainability objectives of stakeholders with regards to ML models and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e system’s behaviours, outputs, and results. \u003ca href=\"#fnref:95\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:96\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-commissioned-research-on-algorithmic-transparency-in-the-public-sector\"\u003eBritainThinks: Complete transparency, complete simplicity\u003c/a\u003e, CDEI and CDDO, 2021. \u003ca href=\"#fnref:96\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:97\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://home.kpmg/au/en/home/insights/2021/03/artificial-intelligence-five-country-study.html\"\u003eTrust in Artificial Intelligence: a five country study\u003c/a\u003e, KPMG and the University of Queensland, 2021; \u003ca href=\"https://www.gov.uk/government/publications/evidence-to-support-the-analysis-of-impacts-for-artificial-intelligence-governance\"\u003eEvidence to support the analysis of impacts for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance\u003c/a\u003e, Frontier Economics, 2023. \u003ca href=\"#fnref:97\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:98\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://hai.stanford.edu/news/should-ai-models-be-explainable-depends\"\u003eShould \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e models be explainable? That depends,\u003c/a\u003e Stanford Institute for Human-Centered Artificial Intelligence, 2021. \u003ca href=\"#fnref:98\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:99\"\u003e\n      \u003cp\u003eFor example, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/77607.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 24027:2021\u003c/a\u003e describes measurement techniques and methods for assessing bias in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems across their life cycle, especially in \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-aided decision-making. \u003ca href=\"#fnref:99\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:100\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://bills.parliament.uk/bills/3430\"\u003eThe Data Protection and Digital Information (No. 2) Bill\u003c/a\u003e reforms the UK’s data protection regime (\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2018/12/contents/enacted\"\u003eData Protection Act 2018\u003c/a\u003e and the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/eur/2016/679/contents\"\u003eUK \u003cabbr title=\"UK General Data Protection Regulation\"\u003eGDPR\u003c/abbr\u003e\u003c/a\u003e). \u003ca href=\"#fnref:100\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:101\"\u003e\n      \u003cp\u003eGuidance on vulnerability includes: \u003ca rel=\"external\" href=\"https://www.fca.org.uk/publication/guidance-consultation/gc19-03.pdf\"\u003e\u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e guidance on vulnerable consumers,\u003c/a\u003e \u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e, 2019; \u003ca rel=\"external\" href=\"https://www.ofgem.gov.uk/energy-policy-and-regulation/policy-and-regulatory-programmes/consumer-vulnerability-protections\"\u003eConsumer vulnerability protections\u003c/a\u003e, Ofgem, 2020; \u003ca href=\"https://www.gov.uk/government/publications/vulnerable-consumers\"\u003eVulnerable consumers\u003c/a\u003e, CMA, 2018. \u003ca href=\"#fnref:101\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:102\"\u003e\n      \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e has the potential to learn to solve problems without human intervention instructing it to do so, or cope with situations the systems have not encountered before, producing potentially different associated risks that require clear lines of accountability and governance mechanisms to be in place. For example, see ​​\u003ca rel=\"external\" href=\"https://www.technologyreview.com/2021/05/27/1025453/artificial-intelligence-learning-create-itself-agi/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e is learning how to create itself\u003c/a\u003e, MIT Technology Review, 2021. \u003ca href=\"#fnref:102\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:103\"\u003e\n      \u003cp\u003eFor example, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81230.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 42001\u003c/a\u003e (Under development) will provide guidance for establishing, implementing and maintaining an \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e management system within an organisation to develop or use \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e systems responsibly. \u003ca rel=\"external\" href=\"https://www.iso.org/obp/ui/#iso:std:iso-iec:23894:ed-1:v1:en\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 23894\u003c/a\u003e (Under development) will provide guidance for establishing \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e risk management principles and processes within an organisation. \u003ca href=\"#fnref:103\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:104\"\u003e\n      \u003cp\u003eWhile this activity is likely to be led centrally (see \u003ca href=\"#section331\"\u003esection 3.3.1\u003c/a\u003e), this will involve continuation of the existing collaboration across government to ensure alignment with (and appropriate leveraging of) existing work being undertaken in relation to the \u003ca href=\"https://www.gov.uk/government/publications/uk-national-cyber-strategy-2022\"\u003eNational Cyber Strategy\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://safeandtrustedai.org/\"\u003eUKRI work on Safe and Trusted \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, the work of the \u003ca href=\"https://www.gov.uk/government/organisations/centre-for-connected-and-autonomous-vehicles\"\u003eCentre for Connected and Autonomous Vehicles\u003c/a\u003e, the \u003ca rel=\"external\" href=\"https://transform.england.nhs.uk/ai-lab/\"\u003e\u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Lab\u003c/a\u003e and other examples. \u003ca href=\"#fnref:104\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:105\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/responsible-innovation-in-self-driving-vehicles\"\u003eResponsible Innovation in Self-Driving Vehicles\u003c/a\u003e, CDEI, 2022. \u003ca href=\"#fnref:105\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:106\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/guide-to-data-protection/key-dp-themes/explaining-decisions-made-with-ai/\"\u003eExplaining decisions made with \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, \u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e and the Alan Turing Institute, 2021. \u003ca href=\"#fnref:106\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:107\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme/software-and-ai-as-a-medical-device-change-programme-roadmap#wp-10-project-glass-box-ai-interpretability\"\u003eSoftware and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as a Medical Device Change Programme – Roadmap\u003c/a\u003e, \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e, 2022. \u003ca href=\"#fnref:107\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:108\"\u003e\n      \u003cp\u003eFollowing publication of our policy paper in July 2022. \u003ca href=\"#fnref:108\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:109\"\u003e\n      \u003cp\u003ePro-innovation, proportionate, adaptable, trustworthy, clear and collaborative – see \u003ca href=\"#paragraph37\"\u003eparagraph 37\u003c/a\u003e above \u003ca href=\"#fnref:109\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:110\"\u003e\n      \u003cp\u003eFor example, there are only 6 specific legal services activities that are overseen by regulators in the legal services sector. These ‘reserved legal activities’ are set out in the \u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/2007/29/contents\"\u003eLegal Services Act\u003c/a\u003e, HM Government, 2007 and can only be carried out by those who are authorised (or exempt). \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-driven systems could offer other services like writing wills or contracts (which many might consider to be legal services) without being subject to oversight from legal services regulators \u003ca href=\"#fnref:110\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:111\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/regulators-code\"\u003eRegulators’ Code\u003c/a\u003e, Office for Product Safety and Standards, 2014. \u003ca href=\"#fnref:111\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:112\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ucl.ac.uk/constitution-unit/explainers/what-uk-constitution\"\u003eWhat is the UK Constitution?\u003c/a\u003e, The Constitution Unit, University College London, 2023. \u003ca href=\"#fnref:112\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:113\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation regulation of technologies review: digital technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:113\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:114\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/news/uk-on-the-cusp-of-a-transport-revolution-as-self-driving-vehicles-set-to-be-worth-nearly-42-billion-by-2035\"\u003eUK on the cusp of a transport revolution\u003c/a\u003e, Department for Transport, 2021. \u003ca href=\"#fnref:114\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:115\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://assets.publishing.service.gov.uk/government/uploads/system/uploads/attachment_data/file/1099173/cam-2025-realising-benefits-self-driving-vehicles.pdf\"\u003eConnected \u0026amp; Automated Mobility 2025\u003c/a\u003e, Department for Transport, 2022. \u003ca href=\"#fnref:115\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:116\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/regulators-code\"\u003eRegulators’ Code\u003c/a\u003e, Office for Product Safety and Standards, 2014. \u003ca href=\"#fnref:116\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:117\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.legislation.gov.uk/ukpga/1998/42/contents\"\u003eHuman Rights Act\u003c/a\u003e, HM Government, 1998. \u003ca href=\"#fnref:117\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:118\"\u003e\n      \u003cp\u003e\u003ca href=\"#box3-3\"\u003eSee Box 3.3\u003c/a\u003e. \u003ca href=\"#fnref:118\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:119\"\u003e\n      \u003cp\u003e\u003ca href=\"#box-32-supporting-coherence-in-risk-assessment\"\u003eSee Box 3.2\u003c/a\u003e. \u003ca href=\"#fnref:119\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:120\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:120\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:121\"\u003e\n      \u003cp\u003eThe Centre for Data Ethics and Innovation (CDEI) Public attitudes report states that the public continue to have limited awareness of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, with knowledge mainly of low-risk use cases that are already in use but showing low familiarity with more complex \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e applications. \u003ca href=\"https://www.gov.uk/government/publications/cdei-publishes-research-on-ai-governance\"\u003ePublic expectations for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance (transparency, fairness and accountability)\u003c/a\u003e, Centre for Data Ethics and Innovation, 2023. \u003ca href=\"#fnref:121\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:122\"\u003e\n      \u003cp\u003eFor example, stakeholders have outlined proposals for governments’ roles in monitoring the wider \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e ecosystem as a means of addressing challenging policy issues. See \u003ca rel=\"external\" href=\"https://www.cser.ac.uk/resources/why-and-how-governments-should-monitor-ai-development/\"\u003eWhy and how governments should monitor \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e development\u003c/a\u003e, Whittlestone and Clark, 2021. \u003ca href=\"#fnref:122\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:123\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.digitalregulations.innovation.nhs.uk/about-this-service/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and Digital Regulations Service\u003c/a\u003e, Care Quality Commission, Health Research Authority, Medicines and Healthcare Products Regulatory Agency, National Institute for Health and Care Excellence, 2023. \u003ca href=\"#fnref:123\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:124\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/projects-selected-for-the-regulators-pioneer-fund/projects-selected-for-the-regulators-pioneer-fund-2022#project-led-by-the-information-commissioners-office\"\u003eEnabling innovation – piloting a multi-agency advice service for digital innovators\u003c/a\u003e, Regulators’ Pioneer Fund, 2022 (an \u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e-led project). \u003ca href=\"#fnref:124\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:125\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ucl.ac.uk/constitution-unit/explainers/what-uk-constitution\"\u003eWhat is the UK constitution?\u003c/a\u003e The Constitution Unit, University College London, 2023. \u003ca href=\"#fnref:125\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:126\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://arxiv.org/abs/2108.07258\"\u003eOn the opportunities and risks of foundation models\u003c/a\u003e, Bommasani et al., 2022; \u003ca rel=\"external\" href=\"https://www.adalovelaceinstitute.org/report/regulating-ai-in-europe/\"\u003eExpert opinion: Regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e in Europe\u003c/a\u003e, Edwards, 2022. \u003ca href=\"#fnref:126\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:127\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://dl.acm.org/doi/10.1145/3531146.3533088\"\u003eTaxonomy of Risks posed by Language Models\u003c/a\u003e, Weidinger et al., 2022. \u003ca href=\"#fnref:127\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:128\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.adalovelaceinstitute.org/blog/value-chain-general-purpose-ai/\"\u003eThe value chain of general-purpose \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Ada Lovelace Institute, 2023. \u003ca href=\"#fnref:128\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:129\"\u003e\n      \u003cp\u003eSee for example, \u003ca rel=\"external\" href=\"https://www.adalovelaceinstitute.org/blog/value-chain-general-purpose-ai/\"\u003eThe value chain of general-purpose \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Ada Lovelace Institute, 2023; \u003ca rel=\"external\" href=\"https://www.conjecture.dev/ai-alignment-overview\"\u003eAn overview of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e alignment\u003c/a\u003e, Conjecture, 2023; \u003ca rel=\"external\" href=\"https://www.anthropic.com/research\"\u003eMake safe systems and deploy them reliably\u003c/a\u003e, Anthropic, 2023. \u003ca href=\"#fnref:129\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:130\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/speeches/foreign-secretary-statement-to-parliament-on-the-integrated-review-refresh-2023\"\u003eIntegrated Review Refresh 2023\u003c/a\u003e, Prime Minister’s Office, 10 Downing Street, Foreign, Commonwealth and Development Office, Ministry of Defence 2023. \u003ca href=\"#fnref:130\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:131\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://arxiv.org/abs/2108.07258\"\u003eOn the opportunities and risks of foundation models\u003c/a\u003e, Bommasani et al., 2022. \u003ca href=\"#fnref:131\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:132\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://arxiv.org/pdf/2112.02969.pdf\"\u003eJigsaw: Large Language Models meet Program Synthesis\u003c/a\u003e, Jain et al., 2021. \u003ca href=\"#fnref:132\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:133\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.economist.com/interactive/briefing/2022/06/11/huge-foundation-models-are-turbo-charging-ai-progress\"\u003eHuge ‘foundation models’ are turbo-charging \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e progress\u003c/a\u003e, The Economist, 2022. \u003ca href=\"#fnref:133\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:134\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://openai.com/blog/gpt-3-apps/\"\u003eGPT-3 Powers the Next Generation of Apps\u003c/a\u003e, Open \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e, 2021. \u003ca href=\"#fnref:134\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:135\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://venturebeat.com/ai/large-language-models-broaden-ais-reach-in-industry-and-enterprises/\"\u003eLarge language models broaden \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e’s reach in industry and enterprise\u003c/a\u003e, Venture Beat, 2022. \u003ca href=\"#fnref:135\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:136\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://time.com/6252404/mira-murati-chatgpt-openai-interview/\"\u003eThe Creator of ChatGPT Thinks \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Should Be Regulated,\u003c/a\u003e Time, 2023. \u003ca href=\"#fnref:136\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:137\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://openai.com/blog/chatgpt/\"\u003eChatGPT\u003c/a\u003e by OpenAI; \u003ca rel=\"external\" href=\"https://www.newyorker.com/culture/cultural-comment/the-chatbot-problem\"\u003eThe Chatbot Problem\u003c/a\u003e, The New Yorker, 2023. \u003ca href=\"#fnref:137\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:138\"\u003e\n      \u003cp\u003eSee \u003ca rel=\"external\" href=\"https://www.longtermresilience.org/post/future-of-compute-review-submission-of-evidence\"\u003eFuture of Compute Review: Submission of Evidence\u003c/a\u003e, Centre for Long Term Resilience, 2022. \u003ca href=\"#fnref:138\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:139\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003eHM Government Response to Sir Patrick Vallance’s Pro-Innovation Regulation of Technologies Review\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:139\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:140\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/regulatory-sandbox/\"\u003eRegulatory Sandbox\u003c/a\u003e, \u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e, 2022; \u003ca rel=\"external\" href=\"https://www.fca.org.uk/firms/innovation/regulatory-sandbox\"\u003eRegulatory Sandbox\u003c/a\u003e, \u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e, 2022. \u003ca href=\"#fnref:140\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:141\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.fca.org.uk/data/innovation-market-insights\"\u003eInnovation Hub: Market Insights\u003c/a\u003e, \u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e. 2023; \u003ca rel=\"external\" href=\"https://www.cambridge.org/core/journals/european-journal-of-risk-regulation/article/sandbox-%20approach-to-regulating-highrisk-artificial-intelligence-%20applications/C350EADFB379465E7F4A95B973A4977D\"\u003eA Sandbox Approach to Regulating High-Risk Artificial Intelligence Applications\u003c/a\u003e, Tuby et al, 2021. \u003ca href=\"#fnref:141\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:142\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.bis.org/publ/work901.htm\"\u003eInside the regulatory sandbox: effects on fintech funding\u003c/a\u003e, Cornelli et al, 2020. \u003ca href=\"#fnref:142\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:143\"\u003e\n      \u003cp\u003eFor an existing example of this type of model see \u003ca rel=\"external\" href=\"https://www.fca.org.uk/firms/innovation/regulatory-sandbox\"\u003eRegulatory Sandbox\u003c/a\u003e, \u003cabbr title=\"Financial Conduct Authority\"\u003eFCA\u003c/abbr\u003e, 2022. \u003ca href=\"#fnref:143\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:144\"\u003e\n      \u003cp\u003eFor an existing example of this type of model see \u003ca rel=\"external\" href=\"https://ico.org.uk/for-organisations/regulatory-sandbox/\"\u003eRegulatory Sandbox\u003c/a\u003e, \u003cabbr title=\"Information Commissioner’s Office\"\u003eICO\u003c/abbr\u003e, 2023 \u003ca href=\"#fnref:144\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:145\"\u003e\n      \u003cp\u003eFor a report on a pilot of this type of model see: \u003ca rel=\"external\" href=\"https://www.cqc.org.uk/sites/default/files/20200324%20CQC%20sandbox%20report_machine%20learning%20in%20diagnostic%20services.pdf\"\u003eUsing machine learning in diagnostic services\u003c/a\u003e, CQC, 2020. \u003ca href=\"#fnref:145\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:146\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/projects-selected-for-the-regulators-pioneer-fund/projects-selected-for-the-regulators-pioneer-fund-2022#project-led-by-the-information-commissioners-office\"\u003eEnabling innovation – piloting a multi-agency advice service for digital innovators\u003c/a\u003e, Regulators’ Pioneer Fund, 2022. \u003ca href=\"#fnref:146\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:147\"\u003e\n      \u003cp\u003eThe \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e’s ‘airlock process’ is an example of this kind of service, designed for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e products meeting certain criteria. See: \u003ca href=\"https://www.gov.uk/government/publications/software-and-ai-as-a-medical-device-change-programme/software-and-ai-as-a-medical-device-change-programme-roadmap#wp2-02-secondary-legislation-and-process\"\u003eSoftware and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as a medical device change programme\u003c/a\u003e, \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e, 2022. \u003ca href=\"#fnref:147\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:148\"\u003e\n      \u003cp\u003eFor an example, see: \u003ca rel=\"external\" href=\"https://innovation.nhs.uk/\"\u003e\u003cabbr title=\"National Health Service\"\u003eNHS\u003c/abbr\u003e Innovation Service\u003c/a\u003e, Accelerated Access Collaborative, 2023. For \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e projects, see: \u003ca rel=\"external\" href=\"https://www.digitalregulations.innovation.nhs.uk/about-this-service/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and Digital Regulations Service\u003c/a\u003e, Care Quality Commission, Health Research Authority, Medicines and Healthcare Products Regulatory Agency, National Institute for Health and Care Excellence, 2023. \u003ca href=\"#fnref:148\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:149\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/pro-innovation-regulation-of-technologies-review-digital-technologies\"\u003ePro-innovation Regulation of Technologies Review: Digital Technologies\u003c/a\u003e, HM Treasury, 2023. \u003ca href=\"#fnref:149\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:150\"\u003e\n      \u003cp\u003eAny attempt by a regulator to enforce a principle beyond its existing remit and powers may be legally challenged on the basis of going beyond its legal authority. \u003ca href=\"#fnref:150\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:151\"\u003e\n      \u003cp\u003eIncluding but not limited to \u003ca rel=\"external\" href=\"https://www.turing.ac.uk/news/publications/common-regulatory-capacity-ai\"\u003eCommon Regulatory Capacity for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, The Alan Turing Institute, 2022. \u003ca href=\"#fnref:151\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:152\"\u003e\n      \u003cp\u003eThere is evidence that this is predominantly a recruitment problem. Regulators are trying to recruit but often cannot find the right candidates as they are competing for a limited supply of suitable candidates. \u003ca href=\"#fnref:152\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:153\"\u003e\n      \u003cp\u003eEvidence showed that technical standards expertise varies across regulators. \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e regularly uses and designates standards to clarify legal requirements, provide presumptive conformity and demonstrate the state of the art. Other regulators recognise their potential to support regulatory guidance but their thinking is nascent. \u003ca href=\"#fnref:153\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:154\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/the-roadmap-to-an-effective-ai-assurance-ecosystem\"\u003eRoadmap to an effective \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e assurance ecosystem in the UK\u003c/a\u003e, DSIT (formerly DCMS), 2021. \u003ca href=\"#fnref:154\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:155\"\u003e\n      \u003cp\u003eThe \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Standards Hub is led by The Alan Turing Institute in partnership with the British Standards Institution (BSI) and the National Physical Laboratory (NPL) and supported by the UK Government. \u003ca href=\"#fnref:155\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:156\"\u003e\n      \u003cp\u003eTechnical standards are generally voluntary and developed through an industry-led process in global standards development organisations (SDOs), based on the principles of consensus, openness, and transparency, and benefiting from global technical expertise and best practice. In this paper, when referring to ‘technical standards’, we are referring to standards developed in standards development organisations. \u003ca href=\"#fnref:156\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:157\"\u003e\n      \u003cp\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e-specific standards addressing trustworthiness characteristics such as safety, transparency and robustness, amongst others, have been developed or are currently being developed (‘\u003cem\u003e’ indicates standards which are under development at the time of writing) in SDOs such as \u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e and IEEE (for example, \u003ca rel=\"external\" href=\"https://standards.ieee.org/ieee/7001/6929/\"\u003eIEEE 7001\u003c/a\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/82148.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TS 6254\u003c/a\u003e\u003c/em\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81283.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 5469\u003c/a\u003e\u003cem\u003e, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/79804.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 24029-2\u003c/a\u003e\u003c/em\u003e). \u003ca href=\"#fnref:157\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:158\"\u003e\n      \u003cp\u003eTechnical standards can be updated as good practices and the technology develop, allowing flexibility for requirements to adapt to technological change. \u003ca href=\"#fnref:158\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:159\"\u003e\n      \u003cp\u003eStandards help organisations to manage and mitigate risks, as well as helping to unlock and scale the benefits of their products and services. In doing so, standards play a role in responsible innovation both as tools supporting good governance and as mechanisms for enabling and accelerating innovation. \u003ca href=\"#fnref:159\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:160\"\u003e\n      \u003cp\u003eThe UK government established a \u003ca rel=\"external\" href=\"https://www.npl.co.uk/news/unlocking-standards-for-4th-industrial-revolution\"\u003estrategic coordination initiative\u003c/a\u003e with the British Standards Institution (BSI) and the National Physical Laboratory (NPL) to step up UK’s engagement in the global development of standards. \u003ca href=\"#fnref:160\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:161\"\u003e\n      \u003cp\u003eFor example, these include \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81230.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e DIS 42001\u003c/a\u003e* , \u003ca rel=\"external\" href=\"https://www.iso.org/standard/77304.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e FDIS 23894\u003c/a\u003e* and \u003ca rel=\"external\" href=\"https://www.iso.org/standard/80655.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e DIS 25059\u003c/a\u003e. \u003ca href=\"#fnref:161\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:162\"\u003e\n      \u003cp\u003eFor example, transparency standards include \u003ca rel=\"external\" href=\"https://www.iso.org/standard/84111.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e AWI 12792\u003c/a\u003e\u003cem\u003e, \u003ca rel=\"external\" href=\"https://standards.ieee.org/ieee/7001/6929/\"\u003eIEEE P7001-2021\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://www.iso.org/standard/82148.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e AWI TS 6254\u003c/a\u003e\u003c/em\u003e. Bias mitigation standards include \u003ca rel=\"external\" href=\"https://www.iso.org/standard/77607.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e TR 24027:2021\u003c/a\u003e and \u003ca rel=\"external\" href=\"https://www.iso.org/standard/84110.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e AWI TS 12791\u003c/a\u003e*. \u003ca href=\"#fnref:162\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:163\"\u003e\n      \u003cp\u003eFor example, safety in healthcare can be addressed by the joint application of management system, risk management and quality standards along with horizontal thematic safety standards (such as, \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81283.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e 5469\u003c/a\u003e\u003cem\u003e) and sector specific standards (such as, \u003ca rel=\"external\" href=\"https://standardsdevelopment.bsigroup.com/projects/2021-00605#/section\"\u003eBS 30440\u003c/a\u003e\u003c/em\u003e). Accordingly, regulators such as \u003cabbr title=\"Medicines and Healthcare products Regulatory Agency\"\u003eMHRA\u003c/abbr\u003e might decide to reference sector-specific standards in their regulatory guidance as tools for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e providers to demonstrate compliance with regulatory requirements for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e as a medical device. \u003ca href=\"#fnref:163\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:164\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.tortoisemedia.com/intelligence/global-ai/\"\u003eGlobal \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Index\u003c/a\u003e, Tortoise Media, 2022; \u003ca rel=\"external\" href=\"https://airankings.org/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e rankings by country\u003c/a\u003e, \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Rankings, 2023. \u003ca href=\"#fnref:164\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:165\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://oecd.ai/en/network-of-experts\"\u003e\u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e Working Party and Network of Experts on Artificial Intelligence Governance (AIGO)\u003c/a\u003e, \u003cabbr title=\"Organisation for Economic Co-operation and Development\"\u003eOECD\u003c/abbr\u003e, 2023. \u003ca href=\"#fnref:165\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:166\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://gpai.ai/\"\u003eGlobal Partnership on Artificial Intelligence\u003c/a\u003e, \u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e, 2023. \u003ca href=\"#fnref:166\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:167\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.gpai.ai/projects/climate-change-and-ai.pdf\"\u003eClimate Change and \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e: Recommendations for Government Action, Global Partnership on \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e \u003cabbr title=\"Global Partnership on AI\"\u003eGPAI\u003c/abbr\u003e, Climate Change \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e and the Centre for \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e \u0026amp; Climate, 2021. \u003ca href=\"#fnref:167\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:168\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.coe.int/en/web/artificial-intelligence/cai\"\u003eCommittee on Artificial Intelligence (\u003cabbr title=\"Council of Europe Committee on AI\"\u003eCAI\u003c/abbr\u003e)\u003c/a\u003e, Council of Europe, 2023. \u003ca href=\"#fnref:168\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:169\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://en.unesco.org/artificial-intelligence\"\u003eArtificial Intelligence\u003c/a\u003e, UNESCO, 2023. \u003ca href=\"#fnref:169\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:170\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://aistandardshub.org/\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Standards Hub\u003c/a\u003e, 2023 \u003ca href=\"#fnref:170\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:171\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.iso.org/home.html\"\u003eInternational Organisation for Standardisation and International Electrotechnical Commission\u003c/a\u003e, \u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e, 2023. \u003ca href=\"#fnref:171\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:172\"\u003e\n      \u003cp\u003eThe \u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e work programme, which the UK is contributing to alongside our partners, includes the development of an \u003ca rel=\"external\" href=\"https://www.iso.org/management-system-standards-list.html\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Management System Standard\u003c/a\u003e (MSS), which intends to help solve some of the implementation challenges relating to \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e governance. This standard will be known as \u003ca rel=\"external\" href=\"https://www.iso.org/standard/81230.html\"\u003e\u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e 42001\u003c/a\u003e and will help organisations develop or use artificial intelligence responsibly when pursuing their objectives, and fulfil their obligations to interested parties. Additionally, through BSI, the UK is leading the development of \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e international standards in concepts and terminology at \u003cabbr title=\"International Organisation for Standardisation\"\u003eISO\u003c/abbr\u003e/\u003cabbr title=\"International Electrotechnical Commission\"\u003eIEC\u003c/abbr\u003e, including those on data, bias, governance implications, and data life cycles. At the European Telecommunications Standards Institute (ETSI) we have led the creation of documents including the \u003ca rel=\"external\" href=\"https://www.etsi.org/deliver/etsi_gr/SAI/001_099/002/01.01.01_60/gr_SAI002v010101p.pdf\"\u003eETSI GR SAI 002 on Data Supply Chain Security\u003c/a\u003e, out of the UK’s National Cyber Security Centre. \u003ca href=\"#fnref:172\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:173\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.bsigroup.com/en-GB/\"\u003eBritish Standards Institution\u003c/a\u003e, 2023. \u003ca href=\"#fnref:173\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:174\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://ethicsstandards.org/\"\u003eThe Open Community for Ethics in Autonomous and Intelligent Systems (\u003cabbr title=\"Open Community for Ethics in Autonomous and Intelligent Systems\"\u003eOCEANIS\u003c/abbr\u003e),\u003c/a\u003e 2023. \u003ca href=\"#fnref:174\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:175\"\u003e\n      \u003cp\u003eThe \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e sector is estimated to contribute £3.7 billion in GVA (Gross Value Added) to the UK economy. \u003ca href=\"https://www.gov.uk/government/publications/artificial-intelligence-sector-study-2022\"\u003e\u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e Sector Study 2022\u003c/a\u003e, DSIT, 2023. \u003ca href=\"#fnref:175\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:176\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ucl.ac.uk/constitution-unit/explainers/what-uk-constitution\"\u003eWhat is the UK Constitution?\u003c/a\u003e, The Constitution Unit, Uiniversity College London, 2023. \u003ca href=\"#fnref:176\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:177\"\u003e\n      \u003cp\u003e\u003ca rel=\"external\" href=\"https://www.ncsc.gov.uk/collection/machine-learning\"\u003ePrinciples for the security of machine learning\u003c/a\u003e, National Cyber Security Centre, 2022. \u003ca href=\"#fnref:177\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:178\"\u003e\n      \u003cp\u003eTechnical standards marked with ‘*’ are under development. \u003ca href=\"#fnref:178\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n    \u003cli id=\"fn:179\"\u003e\n      \u003cp\u003e\u003ca href=\"https://www.gov.uk/government/publications/establishing-a-pro-innovation-approach-to-regulating-ai/establishing-a-pro-innovation-approach-to-regulating-ai-policy-statement\"\u003eEstablishing a pro-innovation approach to regulating \u003cabbr title=\"Artificial Intelligence\"\u003eAI\u003c/abbr\u003e\u003c/a\u003e, Office for Artificial Intelligence, 2022. \u003ca href=\"#fnref:179\" class=\"reversefootnote\" role=\"doc-backlink\" aria-label=\"go to where this is referenced\"\u003e↩\u003c/a\u003e\u003c/p\u003e\n    \u003c/li\u003e\n  \u003c/ol\u003e\n\u003c/div\u003e"
      }
    }
  ]
}
</script><script type="application/ld+json">
  {
  "@context": "http://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [
    {
      "@type": "ListItem",
      "position": 1,
      "item": {
        "name": "Home",
        "@id": "https://www.gov.uk/"
      }
    },
    {
      "@type": "ListItem",
      "position": 2,
      "item": {
        "name": "Business and industry",
        "@id": "https://www.gov.uk/business-and-industry"
      }
    },
    {
      "@type": "ListItem",
      "position": 3,
      "item": {
        "name": "Science and innovation",
        "@id": "https://www.gov.uk/business-and-industry/science-and-innovation"
      }
    },
    {
      "@type": "ListItem",
      "position": 4,
      "item": {
        "name": "Artificial intelligence",
        "@id": "https://www.gov.uk/business-and-industry/artificial-intelligence"
      }
    },
    {
      "@type": "ListItem",
      "position": 5,
      "item": {
        "name": "AI regulation: a pro-innovation approach",
        "@id": "https://www.gov.uk/government/publications/ai-regulation-a-pro-innovation-approach"
      }
    }
  ]
}
</script>


</body></html>