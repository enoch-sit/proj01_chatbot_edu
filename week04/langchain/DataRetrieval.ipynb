{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "f164e26d",
      "metadata": {
        "id": "f164e26d"
      },
      "source": [
        "# Chatbot via Data Retrieval with LangChain\n",
        "\n",
        "## RAG Workflow Overview\n",
        "```\n",
        "Documents → Loading → Splitting → Embeddings → Vector Store → Retrieval → LLM → Response\n",
        "```\n",
        "\n",
        "## Structure:\n",
        "1. **Document Loading** - Ingest from PDFs, web, YouTube, Notion\n",
        "2. **Document Splitting** - Chunk documents with overlap and metadata preservation\n",
        "3. **Vector Stores and Embeddings** - Convert text to vectors, store in Chroma\n",
        "4. **Advanced Retrieval** - MMR, metadata filtering, compression, self-query\n",
        "5. **Question Answering** - RetrievalQA with custom prompts and chain types\n",
        "6. **Conversational Chat** - Memory-enabled chatbot with GUI\n",
        "\n",
        "Each section includes multiple techniques and addresses common failure modes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36e2ad32",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install all required packages\n",
        "!pip install langchain langchain-community langchain-openai langchain-chroma \\\n",
        "             langchain-huggingface langchain-aws langchain-text-splitters \\\n",
        "             beautifulsoup4 chromadb sentence-transformers pypdf yt-dlp pydub \\\n",
        "             panel param docarray tiktoken lark-parser"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b1c192e",
      "metadata": {},
      "source": [
        "## Setup and Configuration\n",
        "\n",
        "Configure Azure OpenAI and AWS credentials for comprehensive embedding options."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdab98c2",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import datetime\n",
        "from google.colab import userdata\n",
        "from langchain_openai import AzureChatOpenAI\n",
        "import numpy as np\n",
        "\n",
        "# Set Azure OpenAI credentials\n",
        "os.environ[\"AZURE_OPENAI_API_KEY\"] = userdata.get('eduhkkey')\n",
        "os.environ['AWS_ACCESS_KEY_ID'] = userdata.get('awsid')\n",
        "os.environ['AWS_SECRET_ACCESS_KEY'] = userdata.get('awssecret')\n",
        "os.environ['AWS_DEFAULT_REGION'] = 'us-east-1'\n",
        "\n",
        "# Configure LLM\n",
        "llm = AzureChatOpenAI(\n",
        "    azure_endpoint=\"https://aai02.eduhk.hk/openai/deployments/gpt-4o-mini/chat/completions?Hello=\",\n",
        "    api_version=\"2024-02-15-preview\",\n",
        "    deployment_name=\"gpt-4o-mini\",\n",
        "    temperature=0,\n",
        "    streaming=False,\n",
        ")\n",
        "\n",
        "print(f\"LLM configured: {llm.deployment_name}\")\n",
        "print(f\"Base URL: {llm.client._client._base_url}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bdd68a13",
      "metadata": {},
      "source": [
        "## 1. Document Loading\n",
        "\n",
        "### Comprehensive Loading from Multiple Sources\n",
        "\n",
        "Document loading is the first step in RAG, involving:\n",
        "- Converting raw data to Document objects\n",
        "- Preserving metadata (source, page numbers, etc.)\n",
        "- Handling multiple formats and sources"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33df99a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.document_loaders import (\n",
        "    WebBaseLoader, PyPDFLoader, NotionDirectoryLoader, TextLoader\n",
        ")\n",
        "from langchain_community.document_loaders.generic import GenericLoader\n",
        "from langchain_community.document_loaders.parsers import OpenAIWhisperParser\n",
        "from langchain_community.document_loaders.blob_loaders.youtube_audio import YoutubeAudioLoader\n",
        "import bs4\n",
        "\n",
        "# 1.1 Web Loading with HTML filtering\n",
        "bs4_strainer = bs4.SoupStrainer(class_=(\"post-title\", \"post-header\", \"post-content\"))\n",
        "web_loader = WebBaseLoader(\n",
        "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
        "    bs_kwargs={\"parse_only\": bs4_strainer},\n",
        ")\n",
        "web_docs = web_loader.load()\n",
        "\n",
        "print(f\"Loaded {len(web_docs)} web documents\")\n",
        "print(f\"First 200 chars: {web_docs[0].page_content[:200]}\")\n",
        "print(f\"Metadata: {web_docs[0].metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1b2c3d4",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1.2 PDF Loading with metadata preservation\n",
        "# Create sample PDF documents for demonstration\n",
        "sample_pdfs = [\n",
        "    # You can add your own PDF paths here\n",
        "    # \"path/to/your/document1.pdf\",\n",
        "    # \"path/to/your/document2.pdf\",\n",
        "]\n",
        "\n",
        "pdf_docs = []\n",
        "for pdf_path in sample_pdfs:\n",
        "    if os.path.exists(pdf_path):\n",
        "        loader = PyPDFLoader(pdf_path)\n",
        "        docs = loader.load()\n",
        "        pdf_docs.extend(docs)\n",
        "        print(f\"Loaded {len(docs)} pages from {pdf_path}\")\n",
        "\n",
        "# Demonstrate with web content if no PDFs available\n",
        "if not pdf_docs:\n",
        "    print(\"No PDFs found, using web content for demonstration\")\n",
        "    all_docs = web_docs\n",
        "else:\n",
        "    all_docs = pdf_docs + web_docs\n",
        "\n",
        "print(f\"Total documents loaded: {len(all_docs)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b221807a",
      "metadata": {},
      "source": [
        "## 2. Document Splitting\n",
        "\n",
        "### Advanced Text Splitting Strategies\n",
        "\n",
        "Splitting is crucial for:\n",
        "- Fitting within LLM context windows\n",
        "- Maintaining semantic coherence\n",
        "- Preserving document structure and metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0c04ef4",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_text_splitters import (\n",
        "    RecursiveCharacterTextSplitter, \n",
        "    CharacterTextSplitter,\n",
        "    TokenTextSplitter,\n",
        "    MarkdownHeaderTextSplitter\n",
        ")\n",
        "\n",
        "# 2.1 Recursive Character Text Splitter (Recommended)\n",
        "recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200,\n",
        "    add_start_index=True,  # Adds start index metadata\n",
        "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Hierarchical separators\n",
        ")\n",
        "\n",
        "# 2.2 Token-based splitter for precise token control\n",
        "token_splitter = TokenTextSplitter(\n",
        "    chunk_size=500,\n",
        "    chunk_overlap=50\n",
        ")\n",
        "\n",
        "# 2.3 Structure-aware markdown splitter\n",
        "markdown_splitter = MarkdownHeaderTextSplitter(\n",
        "    headers_to_split_on=[\n",
        "        (\"#\", \"Header 1\"),\n",
        "        (\"##\", \"Header 2\"),\n",
        "        (\"###\", \"Header 3\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# Split documents using recursive splitter\n",
        "splits = recursive_splitter.split_documents(all_docs)\n",
        "\n",
        "print(f\"Original documents: {len(all_docs)}\")\n",
        "print(f\"Split chunks: {len(splits)}\")\n",
        "print(f\"Sample chunk metadata: {splits[0].metadata}\")\n",
        "print(f\"Sample chunk content: {splits[0].page_content[:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "splitting_demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 2.4 Demonstrate different splitting strategies\n",
        "sample_text = \"\"\"# Introduction to AI\n",
        "\n",
        "Artificial Intelligence (AI) is transforming our world. It encompasses machine learning, deep learning, and natural language processing.\n",
        "\n",
        "## Machine Learning\n",
        "\n",
        "Machine learning enables computers to learn without explicit programming. Key algorithms include linear regression, decision trees, and neural networks.\n",
        "\n",
        "### Supervised Learning\n",
        "\n",
        "Supervised learning uses labeled training data to make predictions.\n",
        "\"\"\"\n",
        "\n",
        "# Compare splitting methods\n",
        "char_splits = CharacterTextSplitter(chunk_size=100, chunk_overlap=20).split_text(sample_text)\n",
        "recursive_splits = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20).split_text(sample_text)\n",
        "markdown_splits = markdown_splitter.split_text(sample_text)\n",
        "\n",
        "print(\"Character Splitter Results:\")\n",
        "for i, chunk in enumerate(char_splits):\n",
        "    print(f\"Chunk {i}: {chunk[:50]}...\")\n",
        "\n",
        "print(\"\\nMarkdown Header Splitter Results:\")\n",
        "for i, doc in enumerate(markdown_splits):\n",
        "    print(f\"Chunk {i}: {doc.page_content[:50]}...\")\n",
        "    print(f\"Metadata: {doc.metadata}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b397d65b",
      "metadata": {},
      "source": [
        "## 3. Vector Stores and Embeddings\n",
        "\n",
        "### Multiple Embedding Providers and Vector Storage\n",
        "\n",
        "Embeddings convert text to vectors that capture semantic meaning. Different providers offer various capabilities and pricing models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff412f5",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_aws import BedrockEmbeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 3.1 Multiple Embedding Options\n",
        "\n",
        "# AWS Bedrock Embeddings (requires AWS setup)\n",
        "try:\n",
        "    bedrock_embeddings = BedrockEmbeddings(model_id=\"amazon.titan-embed-text-v1\")\n",
        "    print(\"AWS Bedrock embeddings configured\")\n",
        "except Exception as e:\n",
        "    print(f\"AWS Bedrock not available: {e}\")\n",
        "    bedrock_embeddings = None\n",
        "\n",
        "# Hugging Face Embeddings (free, local)\n",
        "hf_embeddings = HuggingFaceEmbeddings(\n",
        "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
        "    model_kwargs={'device': 'cpu'},\n",
        "    encode_kwargs={'normalize_embeddings': True}\n",
        ")\n",
        "\n",
        "print(\"Hugging Face embeddings loaded\")\n",
        "\n",
        "# 3.2 Demonstrate embedding similarity\n",
        "sentences = [\n",
        "    \"I love machine learning and AI\",\n",
        "    \"Artificial intelligence and ML are fascinating\",\n",
        "    \"The weather is beautiful today\"\n",
        "]\n",
        "\n",
        "embeddings_list = [hf_embeddings.embed_query(sent) for sent in sentences]\n",
        "\n",
        "# Calculate similarity between embeddings\n",
        "similarity_1_2 = np.dot(embeddings_list[0], embeddings_list[1])\n",
        "similarity_1_3 = np.dot(embeddings_list[0], embeddings_list[2])\n",
        "\n",
        "print(f\"\\nSimilarity between sentences 1 and 2 (related): {similarity_1_2:.4f}\")\n",
        "print(f\"Similarity between sentences 1 and 3 (unrelated): {similarity_1_3:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vector_store_setup",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 3.3 Create Vector Store with Chroma\n",
        "vectorstore = Chroma.from_documents(\n",
        "    documents=splits,\n",
        "    embedding=hf_embeddings,\n",
        "    persist_directory=\"./chroma_db\"\n",
        ")\n",
        "\n",
        "print(f\"Vector store created with {vectorstore._collection.count()} documents\")\n",
        "\n",
        "# 3.4 Add documents with rich metadata\n",
        "enhanced_docs = [\n",
        "    Document(\n",
        "        page_content=\"Machine learning is a subset of AI focused on algorithms that learn from data.\",\n",
        "        metadata={\"topic\": \"machine_learning\", \"difficulty\": \"beginner\", \"year\": 2024}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Deep learning uses neural networks with multiple layers to model complex patterns.\",\n",
        "        metadata={\"topic\": \"deep_learning\", \"difficulty\": \"advanced\", \"year\": 2024}\n",
        "    ),\n",
        "    Document(\n",
        "        page_content=\"Natural language processing enables computers to understand human language.\",\n",
        "        metadata={\"topic\": \"nlp\", \"difficulty\": \"intermediate\", \"year\": 2024}\n",
        "    )\n",
        "]\n",
        "\n",
        "vectorstore.add_documents(enhanced_docs)\n",
        "print(f\"Added {len(enhanced_docs)} documents with enhanced metadata\")\n",
        "\n",
        "# 3.5 Basic similarity search\n",
        "query = \"What is artificial intelligence?\"\n",
        "basic_results = vectorstore.similarity_search(query, k=3)\n",
        "\n",
        "print(f\"\\nBasic similarity search for: '{query}'\")\n",
        "for i, doc in enumerate(basic_results):\n",
        "    print(f\"Result {i+1}: {doc.page_content[:100]}...\")\n",
        "    print(f\"Metadata: {doc.metadata}\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74d17f53",
      "metadata": {},
      "source": [
        "## 4. Advanced Retrieval Techniques\n",
        "\n",
        "### Addressing Common Retrieval Problems\n",
        "\n",
        "Basic similarity search has limitations:\n",
        "- **Diversity**: Results may be too similar\n",
        "- **Specificity**: Metadata filtering needed\n",
        "- **Relevance**: Context compression required\n",
        "- **Query Understanding**: Self-query for complex requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6a84b93",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
        "from langchain.chains.query_constructor.base import AttributeInfo\n",
        "from langchain.retrievers import ContextualCompressionRetriever\n",
        "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
        "\n",
        "# 4.1 Maximum Marginal Relevance (MMR) - Balances relevance and diversity\n",
        "mmr_retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",\n",
        "    search_kwargs={\"k\": 5, \"fetch_k\": 10, \"lambda_mult\": 0.5}\n",
        ")\n",
        "\n",
        "print(\"MMR Retrieval Results:\")\n",
        "mmr_results = mmr_retriever.invoke(\"machine learning algorithms\")\n",
        "for i, doc in enumerate(mmr_results):\n",
        "    print(f\"MMR Result {i+1}: {doc.page_content[:80]}...\")\n",
        "\n",
        "# 4.2 Metadata Filtering\n",
        "filtered_results = vectorstore.similarity_search(\n",
        "    \"learning algorithms\",\n",
        "    k=3,\n",
        "    filter={\"topic\": \"machine_learning\"}\n",
        ")\n",
        "\n",
        "print(\"\\nFiltered Results (topic=machine_learning):\")\n",
        "for i, doc in enumerate(filtered_results):\n",
        "    print(f\"Filtered Result {i+1}: {doc.page_content[:80]}...\")\n",
        "    print(f\"Metadata: {doc.metadata}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "self_query_demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.3 Self-Query Retriever - LLM parses query for content and metadata\n",
        "metadata_field_info = [\n",
        "    AttributeInfo(\n",
        "        name=\"topic\",\n",
        "        description=\"The topic of the document (machine_learning, deep_learning, nlp)\",\n",
        "        type=\"string\"\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"difficulty\",\n",
        "        description=\"The difficulty level (beginner, intermediate, advanced)\",\n",
        "        type=\"string\"\n",
        "    ),\n",
        "    AttributeInfo(\n",
        "        name=\"year\",\n",
        "        description=\"The year the content was created\",\n",
        "        type=\"integer\"\n",
        "    )\n",
        "]\n",
        "\n",
        "try:\n",
        "    self_query_retriever = SelfQueryRetriever.from_llm(\n",
        "        llm=llm,\n",
        "        vectorstore=vectorstore,\n",
        "        document_contents=\"Technical documents about AI and machine learning\",\n",
        "        metadata_field_info=metadata_field_info,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Test self-query with natural language\n",
        "    self_query_results = self_query_retriever.invoke(\n",
        "        \"Find beginner-friendly content about machine learning from 2024\"\n",
        "    )\n",
        "    \n",
        "    print(\"Self-Query Results:\")\n",
        "    for i, doc in enumerate(self_query_results):\n",
        "        print(f\"Self-Query Result {i+1}: {doc.page_content}\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Self-query retriever error: {e}\")\n",
        "    print(\"Falling back to regular similarity search\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compression_demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.4 Contextual Compression - Extract relevant portions\n",
        "try:\n",
        "    compressor = LLMChainExtractor.from_llm(llm)\n",
        "    compression_retriever = ContextualCompressionRetriever(\n",
        "        base_compressor=compressor,\n",
        "        base_retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
        "    )\n",
        "    \n",
        "    compressed_results = compression_retriever.invoke(\n",
        "        \"What are the key concepts in machine learning?\"\n",
        "    )\n",
        "    \n",
        "    print(\"Compressed Retrieval Results:\")\n",
        "    for i, doc in enumerate(compressed_results):\n",
        "        print(f\"Compressed Result {i+1}: {doc.page_content}\")\n",
        "        print(f\"Metadata: {doc.metadata}\")\n",
        "        print()\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"Compression retriever error: {e}\")\n",
        "    print(\"This feature requires a compatible LLM\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alternative_retrievers",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 4.5 Alternative Retrieval Methods\n",
        "from langchain.retrievers import SVMRetriever, TFIDFRetriever\n",
        "\n",
        "# Prepare text data for alternative retrievers\n",
        "texts = [doc.page_content for doc in splits[:10]]  # Use first 10 splits\n",
        "\n",
        "# SVM Retriever\n",
        "svm_retriever = SVMRetriever.from_texts(texts, hf_embeddings)\n",
        "svm_results = svm_retriever.get_relevant_documents(\"machine learning\")\n",
        "\n",
        "print(\"SVM Retriever Results:\")\n",
        "for i, doc in enumerate(svm_results[:2]):\n",
        "    print(f\"SVM Result {i+1}: {doc.page_content[:100]}...\")\n",
        "\n",
        "# TF-IDF Retriever\n",
        "tfidf_retriever = TFIDFRetriever.from_texts(texts)\n",
        "tfidf_results = tfidf_retriever.get_relevant_documents(\"machine learning\")\n",
        "\n",
        "print(\"\\nTF-IDF Retriever Results:\")\n",
        "for i, doc in enumerate(tfidf_results[:2]):\n",
        "    print(f\"TF-IDF Result {i+1}: {doc.page_content[:100]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d079c419",
      "metadata": {},
      "source": [
        "## 5. Question Answering with RetrievalQA\n",
        "\n",
        "### Multiple Chain Types and Custom Prompts\n",
        "\n",
        "RetrievalQA combines document retrieval with LLM generation using different strategies:\n",
        "- **Stuff**: Concatenate all documents (default)\n",
        "- **Map-Reduce**: Process documents separately, then combine\n",
        "- **Refine**: Iteratively refine answer with each document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3e405383",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain import hub\n",
        "\n",
        "# 5.1 Basic RAG Chain\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Custom prompt template\n",
        "custom_prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"You are a helpful AI assistant specializing in machine learning and AI.\n",
        "    \n",
        "    Use the following context to answer the question. If you don't know the answer based on the context, \n",
        "    say \"I don't have enough information in the provided context to answer that question.\"\n",
        "    \n",
        "    Always cite which part of the context you used for your answer.\n",
        "    \n",
        "    Context: {context}\n",
        "    \n",
        "    Question: {question}\n",
        "    \n",
        "    Answer:\"\"\"\n",
        ")\n",
        "\n",
        "# Create RAG chain\n",
        "rag_chain = (\n",
        "    {\"context\": mmr_retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | custom_prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# Test the chain\n",
        "questions = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How does deep learning differ from traditional machine learning?\",\n",
        "    \"What are the applications of natural language processing?\"\n",
        "]\n",
        "\n",
        "print(\"RAG Chain Responses:\")\n",
        "for question in questions:\n",
        "    try:\n",
        "        response = rag_chain.invoke(question)\n",
        "        print(f\"\\nQ: {question}\")\n",
        "        print(f\"A: {response}\")\n",
        "        print(\"-\" * 80)\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing question '{question}': {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "retrieval_qa_demo",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 5.2 RetrievalQA with Different Chain Types\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "# Custom prompt for RetrievalQA\n",
        "qa_prompt = PromptTemplate(\n",
        "    input_variables=[\"context\", \"question\"],\n",
        "    template=\"\"\"Use the following pieces of context to answer the question at the end. \n",
        "    If you don't know the answer, just say that you don't know, don't try to make up an answer. \n",
        "    Use three sentences maximum. Keep the answer as concise as possible. \n",
        "    Always say \"Thanks for asking!\" at the end of the answer.\n",
        "    \n",
        "    {context}\n",
        "    \n",
        "    Question: {question}\n",
        "    Helpful Answer:\"\"\"\n",
        ")\n",
        "\n",
        "# Test different chain types\n",
        "chain_types = [\"stuff\", \"map_reduce\", \"refine\"]\n",
        "test_question = \"What are the main types of machine learning?\"\n",
        "\n",
        "for chain_type in chain_types:\n",
        "    try:\n",
        "        qa_chain = RetrievalQA.from_chain_type(\n",
        "            llm=llm,\n",
        "            chain_type=chain_type,\n",
        "            retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "            return_source_documents=True,\n",
        "            chain_type_kwargs={\"prompt\": qa_prompt} if chain_type == \"stuff\" else {}\n",
        "        )\n",
        "        \n",
        "        result = qa_chain.invoke({\"query\": test_question})\n",
        "        \n",
        "        print(f\"\\n{chain_type.upper()} Chain Type:\")\n",
        "        print(f\"Answer: {result['result']}\")\n",
        "        print(f\"Source documents: {len(result['source_documents'])}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error with {chain_type} chain: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27bc440b",
      "metadata": {},
      "source": [
        "## 6. Conversational Chat with Memory\n",
        "\n",
        "### Building a Stateful Chatbot\n",
        "\n",
        "Conversational AI requires:\n",
        "- **Memory**: Tracking conversation history\n",
        "- **Context**: Understanding references to previous messages\n",
        "- **Persistence**: Maintaining state across interactions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a41d4c70",
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_core.chat_history import InMemoryChatMessageHistory\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain.memory import ConversationBufferMemory\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "\n",
        "# 6.1 Simple Chat with Memory\n",
        "chat_prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful AI assistant specializing in machine learning and artificial intelligence. \"\n",
        "               \"Use the conversation history to provide context-aware responses.\"),\n",
        "    MessagesPlaceholder(variable_name=\"messages\"),\n",
        "])\n",
        "\n",
        "chat_chain = chat_prompt | llm\n",
        "\n",
        "# Memory management\n",
        "store = {}\n",
        "def get_session_history(session_id: str) -> InMemoryChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "chat_with_history = RunnableWithMessageHistory(\n",
        "    chat_chain, \n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        "    history_messages_key=\"messages\"\n",
        ")\n",
        "\n",
        "# Test conversation\n",
        "config = {\"configurable\": {\"session_id\": \"user123\"}}\n",
        "\n",
        "conversation = [\n",
        "    \"Hi, I'm Alice! I'm new to machine learning.\",\n",
        "    \"What's my name?\",\n",
        "    \"Can you recommend some beginner-friendly ML topics?\",\n",
        "    \"What did you just recommend?\"\n",
        "]\n",
        "\n",
        "print(\"Conversational Chat Demo:\")\n",
        "for message in conversation:\n",
        "    try:\n",
        "        response = chat_with_history.invoke({\"messages\": message}, config)\n",
        "        print(f\"User: {message}\")\n",
        "        print(f\"AI: {response.content}\")\n",
        "        print(\"-\" * 50)\n",
        "    except Exception as e:\n",
        "        print(f\"Error in conversation: {e}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "conversational_rag",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 6.2 Conversational RAG with Document Retrieval\n",
        "# Set up memory for RAG\n",
        "memory = ConversationBufferMemory(\n",
        "    memory_key=\"chat_history\",\n",
        "    return_messages=True,\n",
        "    output_key=\"answer\"\n",
        ")\n",
        "\n",
        "# Create conversational retrieval chain\n",
        "conversational_rag = ConversationalRetrievalChain.from_llm(\n",
        "    llm=llm,\n",
        "    retriever=vectorstore.as_retriever(search_kwargs={\"k\": 3}),\n",
        "    memory=memory,\n",
        "    return_source_documents=True,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "# Test conversational RAG\n",
        "rag_conversation = [\n",
        "    \"What is machine learning?\",\n",
        "    \"Can you tell me more about the types you mentioned?\",\n",
        "    \"How does it relate to artificial intelligence?\",\n",
        "    \"What are some real-world applications?\"\n",
        "]\n",
        "\n",
        "print(\"\\nConversational RAG Demo:\")\n",
        "for question in rag_conversation:\n",
        "    try:\n",
        "        result = conversational_rag.invoke({\"question\": question})\n",
        "        \n",
        "        print(f\"User: {question}\")\n",
        "        print(f\"AI: {result['answer']}\")\n",
        "        \n",
        "        if result.get('source_documents'):\n",
        "            print(f\"Sources: {len(result['source_documents'])} documents\")\n",
        "        \n",
        "        print(\"-\" * 50)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error in conversational RAG: {e}\")\n",
        "        break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "advanced_features",
      "metadata": {},
      "source": [
        "## 7. Advanced Features and Best Practices\n",
        "\n",
        "### Production-Ready Enhancements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "evaluation_metrics",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.1 Evaluation and Metrics\n",
        "from datetime import datetime\n",
        "import json\n",
        "\n",
        "def evaluate_retrieval(query, ground_truth_docs, retrieved_docs, k=3):\n",
        "    \"\"\"Simple evaluation metrics for retrieval quality\"\"\"\n",
        "    \n",
        "    # Precision@K\n",
        "    relevant_retrieved = len(set(ground_truth_docs) & set(retrieved_docs[:k]))\n",
        "    precision_at_k = relevant_retrieved / min(k, len(retrieved_docs))\n",
        "    \n",
        "    # Recall@K\n",
        "    recall_at_k = relevant_retrieved / len(ground_truth_docs) if ground_truth_docs else 0\n",
        "    \n",
        "    return {\n",
        "        \"precision_at_k\": precision_at_k,\n",
        "        \"recall_at_k\": recall_at_k,\n",
        "        \"retrieved_count\": len(retrieved_docs),\n",
        "        \"relevant_count\": relevant_retrieved\n",
        "    }\n",
        "\n",
        "# 7.2 Response Quality Assessment\n",
        "def assess_response_quality(question, answer, context_docs):\n",
        "    \"\"\"Assess response quality based on various criteria\"\"\"\n",
        "    \n",
        "    metrics = {\n",
        "        \"timestamp\": datetime.now().isoformat(),\n",
        "        \"question\": question,\n",
        "        \"answer_length\": len(answer),\n",
        "        \"context_docs_count\": len(context_docs),\n",
        "        \"has_citations\": \"according to\" in answer.lower() or \"based on\" in answer.lower(),\n",
        "        \"confidence_indicators\": {\n",
        "            \"uncertain\": any(phrase in answer.lower() for phrase in [\"i don't know\", \"unclear\", \"uncertain\"]),\n",
        "            \"confident\": any(phrase in answer.lower() for phrase in [\"definitely\", \"certainly\", \"clearly\"])\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "# Example evaluation\n",
        "test_query = \"What is machine learning?\"\n",
        "test_results = vectorstore.similarity_search(test_query, k=5)\n",
        "test_response = rag_chain.invoke(test_query)\n",
        "\n",
        "quality_metrics = assess_response_quality(\n",
        "    test_query, \n",
        "    test_response, \n",
        "    test_results\n",
        ")\n",
        "\n",
        "print(\"Response Quality Assessment:\")\n",
        "print(json.dumps(quality_metrics, indent=2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "optimization_techniques",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.3 Performance Optimization\n",
        "import time\n",
        "\n",
        "def benchmark_retrieval_methods():\n",
        "    \"\"\"Compare performance of different retrieval methods\"\"\"\n",
        "    \n",
        "    test_query = \"machine learning algorithms\"\n",
        "    methods = {\n",
        "        \"Basic Similarity\": lambda: vectorstore.similarity_search(test_query, k=3),\n",
        "        \"MMR\": lambda: vectorstore.max_marginal_relevance_search(test_query, k=3),\n",
        "        \"Similarity with Score\": lambda: vectorstore.similarity_search_with_score(test_query, k=3)\n",
        "    }\n",
        "    \n",
        "    results = {}\n",
        "    \n",
        "    for method_name, method_func in methods.items():\n",
        "        start_time = time.time()\n",
        "        try:\n",
        "            result = method_func()\n",
        "            end_time = time.time()\n",
        "            \n",
        "            results[method_name] = {\n",
        "                \"time_ms\": (end_time - start_time) * 1000,\n",
        "                \"result_count\": len(result),\n",
        "                \"success\": True\n",
        "            }\n",
        "        except Exception as e:\n",
        "            results[method_name] = {\n",
        "                \"time_ms\": None,\n",
        "                \"error\": str(e),\n",
        "                \"success\": False\n",
        "            }\n",
        "    \n",
        "    return results\n",
        "\n",
        "# Run benchmark\n",
        "benchmark_results = benchmark_retrieval_methods()\n",
        "\n",
        "print(\"Retrieval Method Benchmark:\")\n",
        "for method, metrics in benchmark_results.items():\n",
        "    if metrics[\"success\"]:\n",
        "        print(f\"{method}: {metrics['time_ms']:.2f}ms, {metrics['result_count']} results\")\n",
        "    else:\n",
        "        print(f\"{method}: Error - {metrics['error']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "error_handling",
      "metadata": {},
      "outputs": [],
      "source": [
        "# 7.4 Robust Error Handling and Fallbacks\n",
        "class RobustRAGChain:\n",
        "    def __init__(self, primary_retriever, fallback_retriever, llm):\n",
        "        self.primary_retriever = primary_retriever\n",
        "        self.fallback_retriever = fallback_retriever\n",
        "        self.llm = llm\n",
        "        \n",
        "    def retrieve_with_fallback(self, query, k=3):\n",
        "        \"\"\"Try primary retriever, fall back to secondary if needed\"\"\"\n",
        "        try:\n",
        "            docs = self.primary_retriever.invoke(query)\n",
        "            if len(docs) >= k:\n",
        "                return docs[:k], \"primary\"\n",
        "        except Exception as e:\n",
        "            print(f\"Primary retriever failed: {e}\")\n",
        "        \n",
        "        try:\n",
        "            docs = self.fallback_retriever.get_relevant_documents(query)\n",
        "            return docs[:k], \"fallback\"\n",
        "        except Exception as e:\n",
        "            print(f\"Fallback retriever failed: {e}\")\n",
        "            return [], \"failed\"\n",
        "    \n",
        "    def generate_response(self, query):\n",
        "        \"\"\"Generate response with error handling\"\"\"\n",
        "        docs, retriever_used = self.retrieve_with_fallback(query)\n",
        "        \n",
        "        if not docs:\n",
        "            return \"I apologize, but I couldn't find relevant information to answer your question.\"\n",
        "        \n",
        "        context = \"\\n\\n\".join([doc.page_content for doc in docs])\n",
        "        \n",
        "        prompt = f\"\"\"Based on the following context, answer the question. If the context doesn't contain enough information, say so.\n",
        "        \n",
        "        Context: {context}\n",
        "        \n",
        "        Question: {query}\n",
        "        \n",
        "        Answer:\"\"\"\n",
        "        \n",
        "        try:\n",
        "            response = self.llm.invoke(prompt)\n",
        "            return f\"{response.content}\\n\\n[Retrieved using: {retriever_used}]\"\n",
        "        except Exception as e:\n",
        "            return f\"I encountered an error while generating the response: {e}\"\n",
        "\n",
        "# Create robust RAG chain\n",
        "robust_rag = RobustRAGChain(\n",
        "    primary_retriever=mmr_retriever,\n",
        "    fallback_retriever=tfidf_retriever,\n",
        "    llm=llm\n",
        ")\n",
        "\n",
        "# Test robust retrieval\n",
        "test_queries = [\n",
        "    \"What is machine learning?\",\n",
        "    \"How do neural networks work?\",\n",
        "    \"What is the meaning of life?\"  # Question likely not in our documents\n",
        "]\n",
        "\n",
        "print(\"Robust RAG Chain Results:\")\n",
        "for query in test_queries:\n",
        "    response = robust_rag.generate_response(query)\n",
        "    print(f\"\\nQ: {query}\")\n",
        "    print(f\"A: {response}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99e3c12f",
      "metadata": {},
      "source": [
        "## 8. Summary and Best Practices\n",
        "\n",
        "### Key Takeaways for Production RAG Systems\n",
        "\n",
        "**Document Processing:**\n",
        "- Use appropriate loaders for different formats\n",
        "- Preserve metadata for filtering and traceability\n",
        "- Choose chunk sizes based on your domain and use case\n",
        "\n",
        "**Retrieval Strategy:**\n",
        "- Start with basic similarity search, add MMR for diversity\n",
        "- Use metadata filtering for domain-specific queries\n",
        "- Consider compression for long documents\n",
        "- Implement fallback retrievers for robustness\n",
        "\n",
        "**Generation Quality:**\n",
        "- Custom prompts improve response quality\n",
        "- Test different chain types (stuff, map-reduce, refine)\n",
        "- Add conversation memory for interactive applications\n",
        "- Implement proper error handling\n",
        "\n",
        "**Evaluation and Monitoring:**\n",
        "- Track retrieval precision and recall\n",
        "- Monitor response quality and user satisfaction\n",
        "- Benchmark different approaches\n",
        "- Log queries and responses for analysis\n",
        "\n",
        "**Scalability Considerations:**\n",
        "- Use persistent vector stores for large datasets\n",
        "- Consider distributed retrieval for high throughput\n",
        "- Cache frequent queries\n",
        "- Monitor latency and costs\n",
        "\n",
        "This notebook provides a comprehensive foundation for building production-ready RAG systems. Experiment with different combinations of techniques based on your specific use case and requirements."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
