{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set up Pinecone API key (assuming you have stored it as 'pinecone_api_key' in Colab secrets)\n",
    "os.environ[\"EDUHK_API_KEY\"] = userdata.get('eduhkkey')\n",
    "print(os.environ[\"EDUHK_API_KEY\"])\n",
    "\n",
    "os.environ[\"HUGGING_API_KEY\"] = userdata.get('hugging')\n",
    "print(os.environ[\"HUGGING_API_KEY\"])\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = userdata.get('pinecone')\n",
    "print(os.environ[\"PINECONE_API_KEY\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install -q langchain langchain-community langchain-huggingface langchain-pinecone pinecone-client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Tutorial: Document Loading, Splitting, Vectorstores, Embeddings, and Retrieval\n",
    "\n",
    "This notebook provides a step-by-step tutorial on key LangChain concepts using a simple example. We'll use a sample text document to demonstrate:\n",
    "\n",
    "1. Document Loading\n",
    "2. Document Splitting with RecursiveCharacterTextSplitter\n",
    "3. Vectorstores and Embeddings\n",
    "4. Retrieval\n",
    "\n",
    "We'll use Hugging Face embeddings for free, open-source models, and Pinecone as the vectorstore.\n",
    "\n",
    "**Note:** Before running this notebook, you need to:\n",
    "- Sign up for a free Pinecone account at https://www.pinecone.io/.\n",
    "- Create a new index in your Pinecone dashboard named \"tutorial-index\" with dimension 384 (matching the Hugging Face model's output dimension) and metric \"cosine\".\n",
    "- Store your Pinecone API key in Colab secrets under the name 'pinecone_api_key'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Document Loading\n",
    "\n",
    "LangChain provides various loaders to ingest documents from different sources. Here, we'll create a sample text file and load it using `TextLoader`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a sample text file\n",
    "sample_text = \"\"\"\n",
    "LangChain is a framework for developing applications powered by large language models (LLMs).\n",
    "\n",
    "It enables building applications that are:\n",
    "\n",
    "- Data-aware: Connect a language model to other sources of data.\n",
    "\n",
    "- Agentic: Allow a language model to interact with its environment.\n",
    "\n",
    "The main value props of LangChain are:\n",
    "\n",
    "1. Components: Abstractions for working with language models, along with a collection of implementations for each abstraction.\n",
    "\n",
    "2. Off-the-shelf chains: A structured assembly of components for accomplishing specific higher-level tasks.\n",
    "\n",
    "Off-the-shelf chains make it easy to get started. For more complex applications and nuanced use-cases, components make it easy to customize existing chains or build new ones.\n",
    "\"\"\"\n",
    "\n",
    "with open(\"sample_document.txt\", \"w\") as f:\n",
    "    f.write(sample_text)\n",
    "\n",
    "# Now load the document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"sample_document.txt\")\n",
    "documents = loader.load()\n",
    "\n",
    "print(f\"Loaded {len(documents)} document(s).\")\n",
    "print(\"Sample content:\")\n",
    "print(documents[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Document Splitting\n",
    "\n",
    "Large documents need to be split into smaller chunks for efficient embedding and retrieval. LangChain's `RecursiveCharacterTextSplitter` splits text recursively on characters like `\\n\\n`, `\\n`, etc., trying to keep chunks coherent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=200,  # Maximum size of each chunk\n",
    "    chunk_overlap=50,  # Overlap between chunks for context\n",
    "    length_function=len,  # Function to measure chunk size\n",
    ")\n",
    "\n",
    "split_docs = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Split into {len(split_docs)} chunks.\")\n",
    "for i, doc in enumerate(split_docs):\n",
    "    print(f\"Chunk {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Vectorstores and Embeddings\n",
    "\n",
    "Embeddings convert text into vector representations. We'll use Hugging Face's `sentence-transformers` for embeddings.\n",
    "\n",
    "A vectorstore (like Pinecone) stores these embeddings and allows efficient similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Initialize embeddings\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# Create vectorstore from split documents\n",
    "# Note: Ensure the index 'tutorial-index' exists in Pinecone with dimension 384 and cosine metric\n",
    "index_name = \"tutorial-index\"\n",
    "vectorstore = PineconeVectorStore.from_documents(\n",
    "    documents=split_docs,\n",
    "    embedding=embeddings,\n",
    "    index_name=index_name\n",
    ")\n",
    "\n",
    "print(\"Vectorstore created with embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Retrieval\n",
    "\n",
    "Retrieval involves querying the vectorstore to find relevant documents based on semantic similarity.\n",
    "\n",
    "We'll use the vectorstore as a retriever and query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vectorstore\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",  # Use cosine similarity\n",
    "    search_kwargs={\"k\": 2}  # Return top 2 results\n",
    ")\n",
    "\n",
    "# Query the retriever\n",
    "query = \"What are the main value props of LangChain?\"\n",
    "retrieved_docs = retriever.invoke(query)\n",
    "\n",
    "print(f\"Retrieved {len(retrieved_docs)} documents for query: '{query}'\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"Result {i+1}:\")\n",
    "    print(doc.page_content)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers the basics! You can expand this by using different loaders (e.g., PDFLoader), splitters, embeddings (e.g., OpenAI), or vectorstores (e.g., Chroma). Experiment with the parameters to see how they affect the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
