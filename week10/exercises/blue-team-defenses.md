# Blue Team Exercise - Defense and Mitigation

**Time Limit:** 15 minutes  
**Objective:** Secure the vulnerable chatbot by implementing defensive measures

---

## üõ°Ô∏è Mission Briefing

You are the security team responsible for protecting the chatbot application. The Red Team has just demonstrated several critical vulnerabilities. Your mission is to patch these security holes and implement defense-in-depth strategies.

**Rules:**

- ‚úÖ Fix vulnerabilities in the code
- ‚úÖ Test your fixes against Red Team attacks
- ‚úÖ Document all changes
- ‚úÖ Balance security with usability
- ‚ùå Don't break legitimate functionality

---

## üìã Defense Checklist

### Priority 1: Critical Vulnerabilities (Must Fix)

#### ‚úÖ Task 1: Implement Authentication (20 points)

**Problem:** No authentication - anyone can use the chatbot and access any user's data

**Your Mission:**

- [ ] Add a login system
- [ ] Implement session management
- [ ] Use server-side session tokens (not client-side userId)

**Code Location:** `server.js` - Create new login endpoint

**Hints:**

```javascript
// Add session storage
const sessions = new Map();

// Login endpoint
app.post('/api/login', (req, res) => {
    // Verify username/password
    // Generate session token
    // Store session server-side
});

// Middleware to validate sessions
function validateSession(req) {
    const token = req.headers.authorization;
    return sessions.get(token);
}
```

**Testing:** After implementing, try IDOR attacks - they should fail!

---

#### ‚úÖ Task 2: Input Validation & Sanitization (20 points)

**Problem:** No input validation - prompt injection works easily

**Your Mission:**

- [ ] Create input sanitization function
- [ ] Block dangerous keywords/patterns
- [ ] Limit input length

**Code Location:** `server.js` - Add before processing messages

**Implementation Example:**

```javascript
function sanitizeInput(input) {
    // Remove HTML tags
    let clean = input.replace(/<[^>]*>/g, '');
    
    // Limit length
    if (clean.length > 500) {
        clean = clean.substring(0, 500);
    }
    
    // Remove dangerous characters
    clean = clean.replace(/javascript:/gi, '');
    
    return clean.trim();
}

function validateInput(input) {
    // Block injection patterns
    const dangerousPatterns = [
        /ignore.*previous/i,
        /forget.*instructions/i,
        /system.*prompt/i,
    ];
    
    for (const pattern of dangerousPatterns) {
        if (pattern.test(input)) {
            return false;
        }
    }
    return true;
}

// In your chat endpoint:
let message = sanitizeInput(req.body.message);
if (!validateInput(message)) {
    return res.json({ 
        response: 'Invalid input detected. Please rephrase.' 
    });
}
```

**Testing:** Try "Ignore instructions..." - should be blocked!

---

#### ‚úÖ Task 3: Fix IDOR Vulnerability (20 points)

**Problem:** Client controls userId - can access any user's data

**Your Mission:**

- [ ] Remove userId from client requests
- [ ] Get userId from server-side session
- [ ] Add authorization checks

**Code Location:** `server.js` - Chat endpoint

**Before (Vulnerable):**

```javascript
const userId = req.body.userId; // CLIENT CONTROLLED!
```

**After (Secure):**

```javascript
const session = validateSession(req);
if (!session) {
    return res.status(401).json({ error: 'Unauthorized' });
}
const userId = session.userId; // SERVER CONTROLLED!
```

**Testing:** changeUserId(2) in console should no longer work!

---

#### ‚úÖ Task 4: Output Sanitization (15 points)

**Problem:** Bot responses are not sanitized - XSS vulnerability

**Your Mission:**

- [ ] Sanitize all bot responses
- [ ] Remove sensitive data patterns
- [ ] Escape HTML in frontend

**Code Location:** `server.js` and `index.html`

**Server-side:**

```javascript
function sanitizeOutput(text) {
    // Redact sensitive information
    let safe = text.replace(/password[:\s]*\w+/gi, 'password: [REDACTED]');
    safe = safe.replace(/secret[:\s]*\w+/gi, 'secret: [REDACTED]');
    safe = safe.replace(/api[_\s]?key[:\s]*[\w-]+/gi, 'api_key: [REDACTED]');
    return safe;
}

// Before sending response:
response = sanitizeOutput(response);
```

**Client-side (index.html):**

```javascript
function escapeHtml(text) {
    const div = document.createElement('div');
    div.textContent = text; // Use textContent, not innerHTML
    return div.innerHTML;
}

// When displaying bot message:
messageDiv.innerHTML = `<b>Bot:</b> ${escapeHtml(data.response)}`;
```

**Testing:** Send `<script>alert('XSS')</script>` - should display as text, not execute!

---

### Priority 2: Important Security Measures

#### ‚úÖ Task 5: Rate Limiting (10 points)

**Problem:** No rate limiting - vulnerable to DoS attacks

**Your Mission:**

- [ ] Implement per-user rate limiting
- [ ] Track request counts
- [ ] Return 429 status when limit exceeded

**Implementation:**

```javascript
const rateLimits = new Map();

function checkRateLimit(userId) {
    const now = Date.now();
    const userLimit = rateLimits.get(userId) || { count: 0, resetAt: now + 60000 };
    
    if (now > userLimit.resetAt) {
        userLimit.count = 0;
        userLimit.resetAt = now + 60000;
    }
    
    if (userLimit.count >= 10) { // 10 requests per minute
        return false;
    }
    
    userLimit.count++;
    rateLimits.set(userId, userLimit);
    return true;
}

// In chat endpoint:
if (!checkRateLimit(session.userId)) {
    return res.status(429).json({ error: 'Rate limit exceeded' });
}
```

---

#### ‚úÖ Task 6: Secure Error Handling (10 points)

**Problem:** Error messages leak sensitive information

**Your Mission:**

- [ ] Catch all errors
- [ ] Log details server-side
- [ ] Return generic messages to client

**Implementation:**

```javascript
app.post('/chat', (req, res) => {
    try {
        // ... your code ...
    } catch (error) {
        console.error('[ERROR]', error); // Log server-side
        return res.status(500).json({ 
            error: 'An error occurred' // Generic message
        });
    }
});
```

---

#### ‚úÖ Task 7: Remove Debug Endpoints (5 points)

**Problem:** `/api/users` and `/api/debug` expose sensitive data

**Your Mission:**

- [ ] Delete or protect debug endpoints
- [ ] Add authentication if needed

**Solution:**

```javascript
// Option 1: Delete completely
// Just remove these endpoints!

// Option 2: Protect with authentication
app.get('/api/users', (req, res) => {
    const session = validateSession(req);
    if (!session || session.role !== 'admin') {
        return res.status(403).json({ error: 'Forbidden' });
    }
    // ... return data only for admins
});
```

---

### Priority 3: Defense in Depth

#### ‚úÖ Task 8: Security Headers (5 points)

**Your Mission:** Add security headers to all responses

**Implementation:**

```javascript
app.use((req, res, next) => {
    res.setHeader('X-Content-Type-Options', 'nosniff');
    res.setHeader('X-Frame-Options', 'DENY');
    res.setHeader('X-XSS-Protection', '1; mode=block');
    res.setHeader('Content-Security-Policy', "default-src 'self'");
    next();
});
```

---

#### ‚úÖ Task 9: Logging & Monitoring (5 points)

**Your Mission:** Add security event logging

**Implementation:**

```javascript
function logSecurityEvent(event, details) {
    console.log(`[SECURITY] ${new Date().toISOString()} - ${event}:`, details);
}

// Use throughout code:
if (!validateInput(message)) {
    logSecurityEvent('Suspicious Input', { userId, message });
    // ...
}
```

---

## üß™ Testing Your Defenses

### Test Suite

Run through each Red Team attack and verify it's blocked:

| Attack | Expected Result | Status |
|--------|----------------|---------|
| Prompt Injection | ‚ùå Blocked | [ ] |
| System Prompt Leak | ‚ùå Blocked | [ ] |
| IDOR (changeUserId) | ‚ùå No effect | [ ] |
| XSS Injection | ‚ùå Displayed as text | [ ] |
| /api/users endpoint | ‚ùå 403 Forbidden | [ ] |
| /api/debug endpoint | ‚ùå 403 Forbidden | [ ] |
| Rate limit test | ‚ùå Blocked after limit | [ ] |
| Unauthenticated access | ‚ùå 401 Unauthorized | [ ] |

---

## üìù Change Documentation Template

Document each fix you implement:

```markdown
### Fix: [Vulnerability Name]

**What was vulnerable:**
[Describe the problem]

**What we changed:**
[List code changes]

**How it prevents the attack:**
[Explain the defense]

**Trade-offs:**
[Any usability impacts?]

**Code snippet:**
```javascript
// Your fix here
```

```

---

## üéØ Success Criteria

Your implementation is successful when:

- ‚úÖ All Priority 1 tasks are complete
- ‚úÖ Red Team attacks no longer work
- ‚úÖ Legitimate users can still use the chatbot
- ‚úÖ All inputs are validated
- ‚úÖ All outputs are sanitized
- ‚úÖ Authentication is required
- ‚úÖ Sessions are managed server-side
- ‚úÖ Rate limiting is in place

---

## üí° Best Practices Checklist

- [ ] **Input Validation:** Never trust user input
- [ ] **Output Encoding:** Always escape output
- [ ] **Authentication:** Verify user identity
- [ ] **Authorization:** Check user permissions
- [ ] **Least Privilege:** Users only access their own data
- [ ] **Defense in Depth:** Multiple layers of security
- [ ] **Fail Securely:** Default to deny access
- [ ] **Logging:** Monitor for attacks
- [ ] **Keep it Simple:** Complex code = more vulnerabilities

---

## üîß Quick Reference: Code Locations

| File | What to Change |
|------|----------------|
| `server.js` | Add authentication, input validation, rate limiting |
| `index.html` | Add login UI, remove debug panel, escape output |
| Both | Test all changes together |

---

## üèÜ Scoring

| Points | Security Level |
|--------|----------------|
| 0-30   | Still Vulnerable |
| 31-60  | Basic Security |
| 61-80  | Good Security |
| 81-100 | Excellent Security |

---

## üöÄ Advanced Challenges (Bonus)

If you finish early, try these:

### Bonus 1: Password Hashing (10 points)
- Implement bcrypt for password hashing
- Never store plaintext passwords

### Bonus 2: HTTPS/TLS (10 points)
- Add HTTPS support
- Generate self-signed certificate for testing

### Bonus 3: Content Security Policy (10 points)
- Implement strict CSP headers
- Test with CSP validator

### Bonus 4: AI Guardrails (15 points)
- Add a moderation layer
- Filter inappropriate content
- Use allowlists for safe topics

---

## üìä Team Progress Tracker

| Task | Assigned To | Status | Tested | Notes |
|------|-------------|---------|---------|-------|
| Authentication | | ‚¨ú | ‚¨ú | |
| Input Validation | | ‚¨ú | ‚¨ú | |
| IDOR Fix | | ‚¨ú | ‚¨ú | |
| Output Sanitization | | ‚¨ú | ‚¨ú | |
| Rate Limiting | | ‚¨ú | ‚¨ú | |
| Error Handling | | ‚¨ú | ‚¨ú | |
| Remove Debug | | ‚¨ú | ‚¨ú | |
| Security Headers | | ‚¨ú | ‚¨ú | |
| Logging | | ‚¨ú | ‚¨ú | |

---

## üéì Learning Objectives

After completing this exercise, you will:
- ‚úÖ Understand how to implement authentication
- ‚úÖ Know how to validate and sanitize inputs
- ‚úÖ Prevent IDOR vulnerabilities
- ‚úÖ Implement rate limiting
- ‚úÖ Apply defense-in-depth principles
- ‚úÖ Balance security with usability

---

## üí¨ Discussion Questions

1. Which vulnerability was hardest to fix? Why?
2. What trade-offs did you make between security and usability?
3. How would you test these fixes in production?
4. What additional security measures would you add?
5. How do chatbots differ from traditional web apps in terms of security?

---

## ÔøΩ Advanced Defense Tasks (Bonus)

### Priority 4: LLM-Specific Security Controls

#### ‚úÖ Task 7: System Prompt Protection (20 points)

**Problem:** System prompts containing API keys or instructions can be leaked through prompt injection

**Your Mission:**

- [ ] Never include secrets directly in system prompts
- [ ] Implement prompt injection detection
- [ ] Add output filtering to prevent system prompt leakage

**Code Location:** `server.js` - System prompt handling

**Implementation Example:**

```javascript
// BAD - Never do this
const systemPrompt = "You are a helpful assistant. API_KEY=sk-12345...";

// GOOD - Separate secrets from prompts
const systemPrompt = "You are a helpful assistant with restricted knowledge.";
const apiKey = process.env.API_KEY; // Store separately

// Add output filter
function filterSystemPromptLeakage(output) {
    const dangerousPatterns = [
        /api[_-]?key/gi,
        /system prompt/gi,
        /your instructions/gi,
        /you were told/gi
    ];
    
    for (const pattern of dangerousPatterns) {
        if (pattern.test(output)) {
            return "I cannot reveal that information.";
        }
    }
    return output;
}

// Use in response pipeline
app.post('/api/chat', (req, res) => {
    let response = generateLLMResponse(req.body.message);
    response = filterSystemPromptLeakage(response);
    res.json({ response });
});
```

**Testing:** Try extracting system prompts - they should be blocked!

---

#### ‚úÖ Task 8: Implement LLM Guardrails (25 points)

**Problem:** No protection against jailbreaking, malicious prompts, or harmful outputs

**Your Mission:**

- [ ] Add pre-processing input filters
- [ ] Implement output content moderation
- [ ] Create jailbreak detection

**Implementation Example:**

```javascript
// Input Guardrail - Check before sending to LLM
function checkInputSafety(userInput) {
    const jailbreakPatterns = [
        /ignore.*previous.*instructions/i,
        /you are now.*DAN/i,
        /do anything now/i,
        /pretend you.*no rules/i,
        /roleplay.*unrestricted/i,
        /forget.*constraints/i
    ];
    
    for (const pattern of jailbreakPatterns) {
        if (pattern.test(userInput)) {
            return {
                safe: false,
                reason: "Potential jailbreak attempt detected"
            };
        }
    }
    
    return { safe: true };
}

// Output Guardrail - Check before returning to user
function checkOutputSafety(llmOutput) {
    const unsafeContent = [
        /password.*is.*[a-zA-Z0-9]+/i,
        /api[_-]?key.*[:=]/i,
        /secret.*[:=]/i,
        /<script>/i,
        /eval\(/i
    ];
    
    for (const pattern of unsafeContent) {
        if (pattern.test(llmOutput)) {
            return {
                safe: false,
                filtered: "I cannot provide that information for security reasons."
            };
        }
    }
    
    return { safe: true, filtered: llmOutput };
}

// Use both guardrails
app.post('/api/chat', async (req, res) => {
    const inputCheck = checkInputSafety(req.body.message);
    if (!inputCheck.safe) {
        return res.status(400).json({ 
            error: inputCheck.reason 
        });
    }
    
    let response = await generateLLMResponse(req.body.message);
    const outputCheck = checkOutputSafety(response);
    
    res.json({ response: outputCheck.filtered });
});
```

---

#### ‚úÖ Task 9: Excessive Agency Prevention (20 points)

**Problem:** LLM can perform unauthorized actions or access restricted resources

**Your Mission:**

- [ ] Implement role-based access control (RBAC)
- [ ] Restrict LLM function calling to allowed operations
- [ ] Add human-in-the-loop for sensitive actions

**Implementation Example:**

```javascript
// Define allowed actions per role
const rolePermissions = {
    user: ['readPublicFiles', 'chat'],
    admin: ['readPublicFiles', 'readPrivateFiles', 'chat', 'manageUsers']
};

// Check permissions before allowing LLM actions
function checkPermission(userRole, action) {
    return rolePermissions[userRole]?.includes(action) || false;
}

// Restrict LLM function calls
class SecureLLMAgent {
    constructor(userRole) {
        this.userRole = userRole;
        this.allowedFunctions = rolePermissions[userRole];
    }
    
    async executeAction(action, params) {
        // CRITICAL: Validate with non-LLM logic
        if (!this.allowedFunctions.includes(action)) {
            throw new Error(`Action ${action} not permitted for role ${this.userRole}`);
        }
        
        // Log all actions
        console.log(`[AUDIT] ${this.userRole} executing ${action}`, params);
        
        // Execute with least privilege
        return await this[action](params);
    }
    
    async readPublicFiles(filename) {
        // Restrict to public directory only
        const safePath = path.join(__dirname, 'public', filename);
        if (!safePath.startsWith(path.join(__dirname, 'public'))) {
            throw new Error('Path traversal attempt detected');
        }
        return fs.readFileSync(safePath, 'utf8');
    }
}

// Use in LLM integration
app.post('/api/agent-action', authenticate, async (req, res) => {
    const agent = new SecureLLMAgent(req.user.role);
    
    try {
        const result = await agent.executeAction(
            req.body.action,
            req.body.params
        );
        res.json({ result });
    } catch (error) {
        res.status(403).json({ error: error.message });
    }
});
```

---

#### ‚úÖ Task 10: DoS Protection for LLM Endpoints (15 points)

**Problem:** Malicious prompts can cause resource exhaustion

**Your Mission:**

- [ ] Implement request timeouts
- [ ] Limit response token count
- [ ] Add rate limiting per user

**Implementation Example:**

```javascript
const rateLimit = require('express-rate-limit');

// Global rate limiter
const llmRateLimiter = rateLimit({
    windowMs: 60 * 1000, // 1 minute
    max: 10, // Max 10 requests per minute
    message: 'Too many requests. Please try again later.',
    standardHeaders: true,
    legacyHeaders: false,
});

// Per-user token bucket
const userTokenBuckets = new Map();

function checkUserQuota(userId) {
    const now = Date.now();
    const bucket = userTokenBuckets.get(userId) || {
        tokens: 1000,
        lastRefill: now
    };
    
    // Refill tokens (100 per minute)
    const elapsedMinutes = (now - bucket.lastRefill) / 60000;
    bucket.tokens = Math.min(1000, bucket.tokens + (elapsedMinutes * 100));
    bucket.lastRefill = now;
    
    userTokenBuckets.set(userId, bucket);
    return bucket.tokens > 0;
}

// Apply protections
app.post('/api/chat', 
    llmRateLimiter,
    authenticate,
    async (req, res) => {
        // Check quota
        if (!checkUserQuota(req.user.id)) {
            return res.status(429).json({ 
                error: 'Token quota exceeded' 
            });
        }
        
        // Input length limit
        if (req.body.message.length > 2000) {
            return res.status(400).json({ 
                error: 'Message too long (max 2000 chars)' 
            });
        }
        
        // Timeout protection
        const controller = new AbortController();
        const timeout = setTimeout(() => controller.abort(), 30000); // 30s
        
        try {
            const response = await generateLLMResponse(
                req.body.message,
                { 
                    signal: controller.signal,
                    maxTokens: 500 // Limit output tokens
                }
            );
            
            clearTimeout(timeout);
            
            // Deduct tokens from quota
            const bucket = userTokenBuckets.get(req.user.id);
            bucket.tokens -= response.tokenCount;
            
            res.json({ response: response.text });
            
        } catch (error) {
            clearTimeout(timeout);
            if (error.name === 'AbortError') {
                res.status(408).json({ error: 'Request timeout' });
            } else {
                res.status(500).json({ error: 'Generation failed' });
            }
        }
    }
);
```

---

#### ‚úÖ Task 11: RAG Security - Vector Database Protection (25 points)

**Problem:** Retrieval systems can leak unintended documents through semantic search manipulation

**Your Mission:**

- [ ] Implement document-level access control
- [ ] Add similarity threshold validation
- [ ] Filter retrieved documents before sending to LLM

**Implementation Example:**

```javascript
// Document with access control metadata
class SecureDocument {
    constructor(content, metadata) {
        this.content = content;
        this.metadata = {
            accessLevel: metadata.accessLevel || 'public',
            owner: metadata.owner,
            tags: metadata.tags || []
        };
    }
}

// Secure RAG retrieval
class SecureRAGSystem {
    constructor(vectorDB) {
        this.vectorDB = vectorDB;
    }
    
    async retrieve(query, userRole, options = {}) {
        const {
            maxResults = 3,
            minSimilarity = 0.7
        } = options;
        
        // Get candidate documents
        const candidates = await this.vectorDB.similaritySearch(
            query, 
            maxResults * 2 // Retrieve more to filter
        );
        
        // Filter by access control
        const authorizedDocs = candidates.filter(doc => {
            // Check similarity threshold
            if (doc.similarity < minSimilarity) {
                return false;
            }
            
            // Check access level
            if (doc.metadata.accessLevel === 'public') {
                return true;
            }
            if (doc.metadata.accessLevel === 'admin' && userRole === 'admin') {
                return true;
            }
            
            return false;
        });
        
        // Limit results
        return authorizedDocs.slice(0, maxResults);
    }
    
    // Prevent semantic collision attacks
    detectCollisionAttempt(query, retrievedDocs) {
        // Check if query is suspiciously similar to restricted content
        const restrictedKeywords = ['admin', 'secret', 'password', 'api_key'];
        const queryLower = query.toLowerCase();
        
        const hasRestrictedKeyword = restrictedKeywords.some(
            keyword => queryLower.includes(keyword)
        );
        
        if (hasRestrictedKeyword && retrievedDocs.length === 0) {
            console.warn('[SECURITY] Possible collision attack attempt:', query);
            return true;
        }
        
        return false;
    }
}

// Use in chat endpoint
app.post('/api/rag-chat', authenticate, async (req, res) => {
    const ragSystem = new SecureRAGSystem(vectorDB);
    
    const documents = await ragSystem.retrieve(
        req.body.query,
        req.user.role,
        { minSimilarity: 0.75 }
    );
    
    if (ragSystem.detectCollisionAttempt(req.body.query, documents)) {
        return res.status(400).json({ 
            error: 'Query blocked for security reasons' 
        });
    }
    
    // Build context only from authorized documents
    const context = documents.map(d => d.content).join('\n\n');
    
    const response = await generateLLMResponse(
        `Context: ${context}\n\nQuestion: ${req.body.query}`
    );
    
    res.json({ response });
});
```

---

#### ‚úÖ Task 12: Comprehensive Logging and Monitoring (15 points)

**Problem:** No visibility into attacks or suspicious behavior

**Your Mission:**

- [ ] Log all LLM interactions
- [ ] Detect and alert on attack patterns
- [ ] Create security dashboard

**Implementation Example:**

```javascript
const fs = require('fs');
const path = require('path');

// Security event logger
class SecurityLogger {
    constructor(logPath) {
        this.logPath = logPath;
        this.alertThresholds = {
            failedAuthAttempts: 5,
            blockedPrompts: 10,
            rateLimitHits: 20
        };
        this.metrics = new Map();
    }
    
    log(event) {
        const entry = {
            timestamp: new Date().toISOString(),
            ...event
        };
        
        // Write to log file
        fs.appendFileSync(
            this.logPath,
            JSON.stringify(entry) + '\n'
        );
        
        // Update metrics
        this.updateMetrics(event);
        
        // Check for alerts
        this.checkAlerts(event);
    }
    
    updateMetrics(event) {
        const key = `${event.userId}:${event.type}`;
        const count = (this.metrics.get(key) || 0) + 1;
        this.metrics.set(key, count);
    }
    
    checkAlerts(event) {
        if (event.type === 'blocked_prompt') {
            const count = this.metrics.get(`${event.userId}:blocked_prompt`) || 0;
            if (count >= this.alertThresholds.blockedPrompts) {
                this.sendAlert({
                    severity: 'HIGH',
                    message: `User ${event.userId} has ${count} blocked prompts`,
                    recommendation: 'Possible jailbreak attempt - review user activity'
                });
            }
        }
    }
    
    sendAlert(alert) {
        console.error('[SECURITY ALERT]', alert);
        // In production: send to SIEM, Slack, PagerDuty, etc.
    }
}

const securityLogger = new SecurityLogger('./logs/security.log');

// Log all security events
app.use((req, res, next) => {
    const originalJson = res.json;
    res.json = function(data) {
        if (data.error || res.statusCode >= 400) {
            securityLogger.log({
                type: 'error',
                userId: req.user?.id || 'anonymous',
                endpoint: req.path,
                statusCode: res.statusCode,
                error: data.error,
                ip: req.ip
            });
        }
        return originalJson.call(this, data);
    };
    next();
});

// Log blocked prompts
function logBlockedPrompt(userId, prompt, reason) {
    securityLogger.log({
        type: 'blocked_prompt',
        userId,
        prompt: prompt.substring(0, 100), // Truncate for privacy
        reason,
        severity: 'MEDIUM'
    });
}

// Log successful attacks (for testing)
function logSuccessfulAttack(userId, attackType, details) {
    securityLogger.log({
        type: 'successful_attack',
        userId,
        attackType,
        details,
        severity: 'CRITICAL'
    });
}
```

---

## üìä Defense Implementation Checklist

### Basic Security (Must Have)

- [ ] Authentication & Authorization
- [ ] Input Validation & Sanitization
- [ ] Rate Limiting
- [ ] HTTPS/TLS
- [ ] Error Handling
- [ ] Security Headers

### LLM-Specific Security (Highly Recommended)

- [ ] System Prompt Protection
- [ ] Input/Output Guardrails
- [ ] Jailbreak Detection
- [ ] Excessive Agency Prevention
- [ ] DoS Protection
- [ ] Token Limits

### Advanced Security (Best Practice)

- [ ] RAG Access Control
- [ ] Vector DB Security
- [ ] Comprehensive Logging
- [ ] Security Monitoring
- [ ] Incident Response Plan
- [ ] Regular Security Audits

---

## ÔøΩüîÑ Next Steps

After completing your fixes:

1. Test against all Red Team attacks
2. Share your implementation with the group
3. Compare approaches with other teams
4. Discuss lessons learned
5. Review the "secure-demo" folder for complete implementation

**Remember: Security is not a feature, it's a process! üõ°Ô∏è**
