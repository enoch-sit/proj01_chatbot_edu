{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f353006f",
   "metadata": {},
   "source": [
    "# LangSmith Evaluation Tutorial: Complete Guide\n",
    "\n",
    "This comprehensive notebook demonstrates LangSmith's evaluation capabilities with AWS Bedrock Nova Lite model. We'll cover:\n",
    "\n",
    "1. **Core Concepts**: Datasets, Evaluators, Experiments, Trajectory Evaluation\n",
    "2. **Dataset Creation**: Building test collections for evaluation\n",
    "3. **Multiple Evaluators**: String matching, LLM-as-judge, custom metrics\n",
    "4. **A/B Testing**: Comparing different system prompts\n",
    "5. **Tool Usage Evaluation**: Validating agent tool calls\n",
    "6. **Comparative Analysis**: Side-by-side experiment comparison\n",
    "\n",
    "**Prerequisites:**\n",
    "- AWS Bedrock access with Nova Lite model\n",
    "- LangSmith API key\n",
    "- Store credentials in Google Colab secrets as 'awskey', 'awssecret', 'langsmithkey'\n",
    "\n",
    "**Environment Setup:**\n",
    "```bash\n",
    "!pip install langchain langchain-aws langsmith langchain-core -q\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac41689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install langchain langchain-aws langsmith langchain-core -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b328470c",
   "metadata": {},
   "source": [
    "## Setup: AWS Bedrock Nova Lite Model\n",
    "\n",
    "This cell configures:\n",
    "- AWS Bedrock with Nova Lite model\n",
    "- LangSmith tracing for evaluation\n",
    "- Environment variables from Google Colab secrets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ab3031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from google.colab import userdata\n",
    "\n",
    "# Set AWS credentials\n",
    "os.environ[\"AWS_ACCESS_KEY_ID\"] = userdata.get('awskey')\n",
    "os.environ[\"AWS_SECRET_ACCESS_KEY\"] = userdata.get('awssecret')\n",
    "os.environ[\"AWS_REGION\"] = \"us-east-1\"\n",
    "\n",
    "# Set LangSmith credentials for tracing and evaluation\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('langsmithkey')\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"tutorial-langsmith-evaluation\"\n",
    "\n",
    "print(\"âœ… Environment configured successfully!\")\n",
    "print(f\"   AWS Region: {os.environ['AWS_REGION']}\")\n",
    "print(f\"   LangSmith Project: {os.environ['LANGCHAIN_PROJECT']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d99605",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_aws import ChatBedrock\n",
    "from langchain_core.messages import HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langsmith import Client\n",
    "from langsmith.evaluation import evaluate, evaluate_comparative\n",
    "import json\n",
    "\n",
    "# Initialize AWS Bedrock Nova Lite model\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"amazon.nova-lite-v1:0\",\n",
    "    region_name=os.environ[\"AWS_REGION\"],\n",
    "    model_kwargs={\"temperature\": 0.0, \"max_tokens\": 300}\n",
    ")\n",
    "\n",
    "# Initialize LangSmith client\n",
    "client = Client()\n",
    "\n",
    "print(\"âœ… Models initialized:\")\n",
    "print(f\"   LLM: amazon.nova-lite-v1:0\")\n",
    "print(f\"   Region: {os.environ['AWS_REGION']}\")\n",
    "print(f\"   LangSmith Client: Connected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b03209dc",
   "metadata": {},
   "source": [
    "## Section 1: Understanding LangSmith Evaluation Concepts\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Traces**: Logs of agent runs (inputs, outputs, latencies, errors)\n",
    "2. **Datasets**: Collections of test examples with inputs and expected outputs\n",
    "3. **Evaluators**: Functions that score outputs (string match, LLM-judge, custom)\n",
    "4. **Experiments**: Tracked runs of agents on datasets with metrics\n",
    "5. **Trajectory Evaluation**: Validates sequence of agent actions (tool calls)\n",
    "\n",
    "Let's see each in action!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f6afde",
   "metadata": {},
   "source": [
    "## Section 2: Creating Calculator Tools\n",
    "\n",
    "We'll build a simple calculator agent with three tools to demonstrate tool usage evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define calculator tools\n",
    "@tool\n",
    "def add(a: float, b: float) -> float:\n",
    "    \"\"\"Add two numbers together.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: float, b: float) -> float:\n",
    "    \"\"\"Multiply two numbers together.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "@tool\n",
    "def divide(a: float, b: float) -> float:\n",
    "    \"\"\"Divide first number by second number.\"\"\"\n",
    "    if b == 0:\n",
    "        return \"Error: Division by zero\"\n",
    "    return a / b\n",
    "\n",
    "tools = [add, multiply, divide]\n",
    "tools_by_name = {t.name: t for t in tools}\n",
    "\n",
    "# Bind tools to LLM\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "print(\"âœ… Created 3 calculator tools:\")\n",
    "for tool in tools:\n",
    "    print(f\"   - {tool.name}: {tool.description}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c09e8a",
   "metadata": {},
   "source": [
    "## Section 3: Creating a Dataset\n",
    "\n",
    "Datasets are the foundation of systematic evaluation. We'll create a math Q&A dataset with:\n",
    "- Inputs (questions)\n",
    "- Expected outputs (answers)\n",
    "- Metadata (whether tool should be used, which tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21527fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"Math Calculator QA - Tutorial\"\n",
    "\n",
    "# Check if dataset exists\n",
    "if client.has_dataset(dataset_name=dataset_name):\n",
    "    print(f\"âš ï¸  Dataset '{dataset_name}' already exists. Deleting and recreating...\")\n",
    "    dataset = client.read_dataset(dataset_name=dataset_name)\n",
    "    client.delete_dataset(dataset_id=dataset.id)\n",
    "\n",
    "# Create new dataset\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=dataset_name,\n",
    "    description=\"Math questions requiring calculator tool usage for LangSmith tutorial\"\n",
    ")\n",
    "\n",
    "# Define examples with inputs and expected outputs\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is 15 plus 27?\"},\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"42\",\n",
    "            \"should_use_tool\": True,\n",
    "            \"expected_tool\": \"add\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Calculate 8 times 7\"},\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"56\",\n",
    "            \"should_use_tool\": True,\n",
    "            \"expected_tool\": \"multiply\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What is 100 divided by 4?\"},\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"25\",\n",
    "            \"should_use_tool\": True,\n",
    "            \"expected_tool\": \"divide\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"Hello, how are you today?\"},\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"greeting response\",\n",
    "            \"should_use_tool\": False,\n",
    "            \"expected_tool\": None\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"question\": \"What's 12 + 8?\"},\n",
    "        \"outputs\": {\n",
    "            \"answer\": \"20\",\n",
    "            \"should_use_tool\": True,\n",
    "            \"expected_tool\": \"add\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Batch create examples\n",
    "client.create_examples(\n",
    "    inputs=[ex[\"inputs\"] for ex in examples],\n",
    "    outputs=[ex[\"outputs\"] for ex in examples],\n",
    "    dataset_id=dataset.id,\n",
    ")\n",
    "\n",
    "print(f\"âœ… Dataset created: '{dataset_name}'\")\n",
    "print(f\"   ID: {dataset.id}\")\n",
    "print(f\"   Examples: {len(examples)}\")\n",
    "print(f\"\\nðŸ“Š Sample examples:\")\n",
    "for i, ex in enumerate(examples[:2], 1):\n",
    "    print(f\"   {i}. Q: {ex['inputs']['question']}\")\n",
    "    print(f\"      Expected: {ex['outputs']['answer']} (Tool: {ex['outputs']['expected_tool']})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11d37c9",
   "metadata": {},
   "source": [
    "## Section 4: Building Agent Variants for A/B Testing\n",
    "\n",
    "We'll create two agent variants with different system prompts:\n",
    "- **Variant A (Formal)**: Precise and concise mathematical assistant\n",
    "- **Variant B (Friendly)**: Warm and encouraging math tutor\n",
    "\n",
    "This demonstrates how to compare different approaches systematically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357fe448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# System prompts for A/B testing\n",
    "SYSTEM_PROMPT_A = \"\"\"You are a precise mathematical assistant. \n",
    "When asked to perform calculations, you MUST use the available calculator tools.\n",
    "Always use tools for arithmetic operations. Be formal and concise.\"\"\"\n",
    "\n",
    "SYSTEM_PROMPT_B = \"\"\"You are a friendly and helpful math tutor! \n",
    "When someone asks you to calculate something, use your calculator tools to help them out.\n",
    "Use tools for math operations and explain your steps in a warm, encouraging way.\"\"\"\n",
    "\n",
    "def create_agent(system_prompt: str):\n",
    "    \"\"\"Factory function to create agents with different system prompts.\"\"\"\n",
    "    \n",
    "    def agent_with_tools(inputs: dict) -> dict:\n",
    "        \"\"\"Agent that can use calculator tools.\"\"\"\n",
    "        question = inputs[\"question\"]\n",
    "        trajectory = []  # Track tool calls\n",
    "        \n",
    "        # Initial LLM call with system prompt\n",
    "        messages = [\n",
    "            SystemMessage(content=system_prompt),\n",
    "            HumanMessage(content=question)\n",
    "        ]\n",
    "        \n",
    "        response = llm_with_tools.invoke(messages)\n",
    "        trajectory.append({\"step\": \"llm_call\", \"message\": response})\n",
    "        \n",
    "        # Check if tools were called\n",
    "        if hasattr(response, 'tool_calls') and response.tool_calls:\n",
    "            # Process tool calls\n",
    "            for tool_call in response.tool_calls:\n",
    "                tool_name = tool_call[\"name\"]\n",
    "                tool_args = tool_call[\"args\"]\n",
    "                \n",
    "                # Execute tool\n",
    "                if tool_name in tools_by_name:\n",
    "                    tool_result = tools_by_name[tool_name].invoke(tool_args)\n",
    "                    trajectory.append({\n",
    "                        \"step\": \"tool_call\",\n",
    "                        \"tool\": tool_name,\n",
    "                        \"args\": tool_args,\n",
    "                        \"result\": tool_result\n",
    "                    })\n",
    "                    \n",
    "                    # Add tool result to messages\n",
    "                    messages.append(response)\n",
    "                    messages.append(ToolMessage(\n",
    "                        content=str(tool_result),\n",
    "                        tool_call_id=tool_call[\"id\"]\n",
    "                    ))\n",
    "            \n",
    "            # Get final response after tool execution\n",
    "            final_response = llm.invoke(messages)\n",
    "            answer = final_response.content\n",
    "        else:\n",
    "            # No tools used\n",
    "            answer = response.content\n",
    "        \n",
    "        # Extract tool calls for evaluation\n",
    "        tool_calls = [\n",
    "            {\"tool\": t[\"tool\"], \"args\": t[\"args\"]}\n",
    "            for t in trajectory if t[\"step\"] == \"tool_call\"\n",
    "        ]\n",
    "        \n",
    "        return {\n",
    "            \"answer\": answer,\n",
    "            \"tool_calls\": tool_calls,\n",
    "            \"trajectory\": trajectory\n",
    "        }\n",
    "    \n",
    "    return agent_with_tools\n",
    "\n",
    "# Create both agent variants\n",
    "agent_a = create_agent(SYSTEM_PROMPT_A)\n",
    "agent_b = create_agent(SYSTEM_PROMPT_B)\n",
    "\n",
    "print(\"âœ… Created 2 agent variants:\")\n",
    "print(f\"   Agent A (Formal): {len(SYSTEM_PROMPT_A)} chars\")\n",
    "print(f\"   Agent B (Friendly): {len(SYSTEM_PROMPT_B)} chars\")\n",
    "print(\"\\nðŸ§ª Testing Agent A:\")\n",
    "test_result = agent_a({\"question\": \"What is 5 + 3?\"})\n",
    "print(f\"   Answer: {test_result['answer']}\")\n",
    "print(f\"   Tools used: {test_result['tool_calls']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa547874",
   "metadata": {},
   "source": [
    "## Section 5: Defining Evaluators\n",
    "\n",
    "Evaluators score your agent's outputs. We'll implement four types:\n",
    "\n",
    "1. **Correctness**: Does the answer contain the expected result?\n",
    "2. **Tool Usage**: Did the agent use the correct tool when needed?\n",
    "3. **LLM-as-Judge**: How helpful is the response?\n",
    "4. **Response Length**: Is the response appropriately concise?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79fe277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluator 1: Correctness (String matching)\n",
    "def correctness_evaluator(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Check if answer contains the expected numerical result.\"\"\"\n",
    "    answer = str(outputs.get(\"answer\", \"\")).lower()\n",
    "    expected = str(reference_outputs[\"answer\"]).lower()\n",
    "    \n",
    "    # Check if expected answer is in the response\n",
    "    score = 1 if expected in answer else 0\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"correctness\",\n",
    "        \"score\": score,\n",
    "        \"comment\": f\"Expected '{expected}' in answer\"\n",
    "    }\n",
    "\n",
    "# Evaluator 2: Tool Usage (Trajectory evaluation)\n",
    "def tool_usage_evaluator(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Check if correct tool was used when needed.\"\"\"\n",
    "    should_use_tool = reference_outputs.get(\"should_use_tool\", False)\n",
    "    expected_tool = reference_outputs.get(\"expected_tool\")\n",
    "    \n",
    "    tool_calls = outputs.get(\"tool_calls\", [])\n",
    "    tools_used = [tc[\"tool\"] for tc in tool_calls]\n",
    "    \n",
    "    if should_use_tool:\n",
    "        # Should use a tool\n",
    "        if expected_tool in tools_used:\n",
    "            score = 1\n",
    "            comment = f\"âœ… Correctly used {expected_tool}\"\n",
    "        elif len(tools_used) > 0:\n",
    "            score = 0.5\n",
    "            comment = f\"âš ï¸ Used {tools_used} instead of {expected_tool}\"\n",
    "        else:\n",
    "            score = 0\n",
    "            comment = f\"âŒ Should have used {expected_tool} but used no tools\"\n",
    "    else:\n",
    "        # Should NOT use a tool\n",
    "        if len(tools_used) == 0:\n",
    "            score = 1\n",
    "            comment = \"âœ… Correctly used no tools\"\n",
    "        else:\n",
    "            score = 0\n",
    "            comment = f\"âŒ Should not use tools but used {tools_used}\"\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"tool_usage\",\n",
    "        \"score\": score,\n",
    "        \"comment\": comment\n",
    "    }\n",
    "\n",
    "# Evaluator 3: LLM-as-Judge for Helpfulness\n",
    "def llm_judge_helpfulness(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"Use LLM to judge the helpfulness of the response.\"\"\"\n",
    "    \n",
    "    judge_prompt = f\"\"\"You are evaluating an AI assistant's response for helpfulness.\n",
    "\n",
    "Question: {reference_outputs.get('question', 'N/A')}\n",
    "Response: {outputs.get('answer', '')}\n",
    "\n",
    "Rate the helpfulness on a scale of 0-1:\n",
    "- 1.0: Very helpful, clear, and answers the question well\n",
    "- 0.5: Somewhat helpful but could be clearer\n",
    "- 0.0: Not helpful or confusing\n",
    "\n",
    "Respond with ONLY a number between 0 and 1 (e.g., 0.8)\"\"\"\n",
    "    \n",
    "    try:\n",
    "        judge_response = llm.invoke([HumanMessage(content=judge_prompt)])\n",
    "        score_text = judge_response.content.strip()\n",
    "        \n",
    "        # Extract numeric score\n",
    "        score = float(score_text)\n",
    "        score = max(0.0, min(1.0, score))  # Clamp to [0, 1]\n",
    "        \n",
    "        return {\n",
    "            \"key\": \"helpfulness\",\n",
    "            \"score\": score,\n",
    "            \"comment\": f\"LLM judge rated {score}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            \"key\": \"helpfulness\",\n",
    "            \"score\": 0.5,\n",
    "            \"comment\": f\"Judge error: {str(e)}\"\n",
    "        }\n",
    "\n",
    "# Evaluator 4: Response Length\n",
    "def response_length_evaluator(outputs: dict) -> dict:\n",
    "    \"\"\"Check if response is appropriately concise.\"\"\"\n",
    "    answer = outputs.get(\"answer\", \"\")\n",
    "    length = len(answer)\n",
    "    \n",
    "    # Good range: 10-300 characters\n",
    "    if 10 <= length <= 300:\n",
    "        score = 1.0\n",
    "    elif length < 10:\n",
    "        score = 0.3\n",
    "    else:\n",
    "        score = 0.7\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"response_length\",\n",
    "        \"score\": score,\n",
    "        \"comment\": f\"Length: {length} chars\"\n",
    "    }\n",
    "\n",
    "# Combine all evaluators\n",
    "evaluators = [\n",
    "    correctness_evaluator,\n",
    "    tool_usage_evaluator,\n",
    "    llm_judge_helpfulness,\n",
    "    response_length_evaluator\n",
    "]\n",
    "\n",
    "print(\"âœ… Defined 4 evaluators:\")\n",
    "print(\"   1. Correctness (string match)\")\n",
    "print(\"   2. Tool Usage (trajectory)\")\n",
    "print(\"   3. Helpfulness (LLM-as-judge)\")\n",
    "print(\"   4. Response Length (conciseness)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28845bcc",
   "metadata": {},
   "source": [
    "## Section 6: Running Experiment A (Formal Agent)\n",
    "\n",
    "Now we'll run our first experiment! LangSmith will:\n",
    "1. Run the agent on each example in the dataset\n",
    "2. Apply all evaluators to score each output\n",
    "3. Track metrics and traces\n",
    "4. Store results for comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4cac22",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§ª Running Experiment A: Formal System Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_a = evaluate(\n",
    "    agent_a,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"tutorial-math-agent-formal\",\n",
    "    description=\"Agent with formal, precise system prompt using AWS Nova Lite\",\n",
    "    metadata={\n",
    "        \"model\": \"amazon.nova-lite-v1:0\",\n",
    "        \"system_prompt\": \"formal\",\n",
    "        \"variant\": \"A\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"tutorial\": \"langsmith-evaluation\"\n",
    "    },\n",
    "    max_concurrency=2,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Experiment A Complete!\")\n",
    "print(f\"   Experiment Name: {results_a.experiment_name}\")\n",
    "print(f\"   Dataset: {dataset_name}\")\n",
    "print(f\"   Examples Evaluated: {len(examples)}\")\n",
    "print(f\"   Evaluators Applied: {len(evaluators)}\")\n",
    "print(f\"\\nðŸ“Š View results at: https://smith.langchain.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d3a955",
   "metadata": {},
   "source": [
    "## Section 7: Running Experiment B (Friendly Agent)\n",
    "\n",
    "Let's run the second experiment with the friendly system prompt for comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369df419",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ§ª Running Experiment B: Friendly System Prompt\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "results_b = evaluate(\n",
    "    agent_b,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    experiment_prefix=\"tutorial-math-agent-friendly\",\n",
    "    description=\"Agent with friendly, encouraging system prompt using AWS Nova Lite\",\n",
    "    metadata={\n",
    "        \"model\": \"amazon.nova-lite-v1:0\",\n",
    "        \"system_prompt\": \"friendly\",\n",
    "        \"variant\": \"B\",\n",
    "        \"temperature\": 0.0,\n",
    "        \"tutorial\": \"langsmith-evaluation\"\n",
    "    },\n",
    "    max_concurrency=2,\n",
    ")\n",
    "\n",
    "print(\"\\nâœ… Experiment B Complete!\")\n",
    "print(f\"   Experiment Name: {results_b.experiment_name}\")\n",
    "print(f\"   Dataset: {dataset_name}\")\n",
    "print(f\"   Examples Evaluated: {len(examples)}\")\n",
    "print(f\"   Evaluators Applied: {len(evaluators)}\")\n",
    "print(f\"\\nðŸ“Š View results at: https://smith.langchain.com/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53655da",
   "metadata": {},
   "source": [
    "## Section 8: Analyzing Results\n",
    "\n",
    "Let's retrieve and display aggregate metrics from both experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ccfff0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸ“Š EVALUATION RESULTS SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\nðŸ”¬ Experiment A (Formal):\")\n",
    "print(f\"   Name: {results_a.experiment_name}\")\n",
    "print(f\"   Project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "\n",
    "print(f\"\\nðŸ”¬ Experiment B (Friendly):\")\n",
    "print(f\"   Name: {results_b.experiment_name}\")\n",
    "print(f\"   Project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "\n",
    "print(f\"\\nðŸ’¡ Next Steps:\")\n",
    "print(f\"   1. Visit LangSmith UI: https://smith.langchain.com/\")\n",
    "print(f\"   2. Navigate to project: {os.environ['LANGCHAIN_PROJECT']}\")\n",
    "print(f\"   3. Compare experiments A and B side-by-side\")\n",
    "print(f\"   4. Examine individual traces and tool calls\")\n",
    "print(f\"   5. View aggregate metrics across all evaluators\")\n",
    "\n",
    "print(f\"\\nðŸ“ˆ What to look for in the UI:\")\n",
    "print(f\"   âœ“ Correctness scores: Did agents answer correctly?\")\n",
    "print(f\"   âœ“ Tool usage: Did they use the right tools?\")\n",
    "print(f\"   âœ“ Helpfulness: Which prompt style is clearer?\")\n",
    "print(f\"   âœ“ Response length: Which is more concise?\")\n",
    "print(f\"   âœ“ Trajectory view: See step-by-step tool calls\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2a5965",
   "metadata": {},
   "source": [
    "## Section 9: Comparative Evaluation Concepts\n",
    "\n",
    "### How to Compare Experiments\n",
    "\n",
    "In LangSmith UI, you can:\n",
    "\n",
    "1. **Side-by-Side View**: Compare outputs for the same input\n",
    "2. **Aggregate Metrics**: See overall performance differences\n",
    "3. **Per-Example Breakdown**: Identify where each variant excels\n",
    "4. **Trajectory Comparison**: Compare tool usage patterns\n",
    "\n",
    "### Programmatic Comparison\n",
    "\n",
    "While `evaluate_comparative()` is available, the UI provides richer insights. Here's the conceptual approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec44e565",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrating comparative evaluation concepts\n",
    "print(\"âš–ï¸  COMPARATIVE EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“‹ Comparison Framework:\")\n",
    "print(f\"   Experiment A: {results_a.experiment_name}\")\n",
    "print(f\"   Experiment B: {results_b.experiment_name}\")\n",
    "\n",
    "print(\"\\nðŸ” Comparison Dimensions:\")\n",
    "print(\"   1. Correctness: Which gets more answers right?\")\n",
    "print(\"   2. Tool Usage: Which uses tools more appropriately?\")\n",
    "print(\"   3. Helpfulness: Which provides better explanations?\")\n",
    "print(\"   4. Conciseness: Which is more efficient?\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Best Practices:\")\n",
    "print(\"   âœ“ Run experiments on same dataset\")\n",
    "print(\"   âœ“ Use consistent evaluators\")\n",
    "print(\"   âœ“ Tag with metadata for filtering\")\n",
    "print(\"   âœ“ Run multiple times for statistical significance\")\n",
    "print(\"   âœ“ Compare in LangSmith UI for detailed insights\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Decision Making:\")\n",
    "print(\"   - If Agent A scores higher on correctness â†’ Choose A\")\n",
    "print(\"   - If Agent B is more helpful but slower â†’ Trade-off decision\")\n",
    "print(\"   - Use metadata to track what changed between experiments\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbdd58a",
   "metadata": {},
   "source": [
    "## Section 10: Advanced Evaluation Patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c2ce1f",
   "metadata": {},
   "source": [
    "### Custom Evaluator Example\n",
    "\n",
    "Let's create a domain-specific evaluator for mathematical accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961019cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def mathematical_accuracy_evaluator(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    \"\"\"\n",
    "    Advanced evaluator that extracts and compares numerical values.\n",
    "    More robust than simple string matching.\n",
    "    \"\"\"\n",
    "    answer = outputs.get(\"answer\", \"\")\n",
    "    expected = str(reference_outputs.get(\"answer\", \"\"))\n",
    "    \n",
    "    # Extract all numbers from both answer and expected\n",
    "    def extract_numbers(text):\n",
    "        return re.findall(r'-?\\d+\\.?\\d*', str(text))\n",
    "    \n",
    "    answer_nums = extract_numbers(answer)\n",
    "    expected_nums = extract_numbers(expected)\n",
    "    \n",
    "    # Check if expected number appears in answer\n",
    "    if expected_nums and expected_nums[0] in answer_nums:\n",
    "        score = 1.0\n",
    "        comment = f\"âœ… Found expected value {expected_nums[0]}\"\n",
    "    elif len(answer_nums) > 0:\n",
    "        score = 0.0\n",
    "        comment = f\"âŒ Got {answer_nums[0]}, expected {expected_nums[0] if expected_nums else 'N/A'}\"\n",
    "    else:\n",
    "        score = 0.0\n",
    "        comment = \"âŒ No numerical answer found\"\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"mathematical_accuracy\",\n",
    "        \"score\": score,\n",
    "        \"comment\": comment\n",
    "    }\n",
    "\n",
    "# Test the evaluator\n",
    "test_outputs = {\"answer\": \"The result is 42\"}\n",
    "test_reference = {\"answer\": \"42\"}\n",
    "result = mathematical_accuracy_evaluator(test_outputs, test_reference)\n",
    "\n",
    "print(\"âœ… Custom Evaluator: Mathematical Accuracy\")\n",
    "print(f\"   Test Input: {test_outputs['answer']}\")\n",
    "print(f\"   Expected: {test_reference['answer']}\")\n",
    "print(f\"   Score: {result['score']}\")\n",
    "print(f\"   Comment: {result['comment']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74cf219",
   "metadata": {},
   "source": [
    "### 3. Repetitions for Statistical Significance\n",
    "\n",
    "Run each example multiple times to measure variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7289684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of running with repetitions (commented to save time)\n",
    "print(\"ðŸ”„ REPETITIONS FOR STATISTICAL SIGNIFICANCE\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nðŸ“ Concept:\")\n",
    "print(\"   Run each example N times to measure:\")\n",
    "print(\"   - Variance in responses\")\n",
    "print(\"   - Consistency of tool usage\")\n",
    "print(\"   - Average performance\")\n",
    "\n",
    "print(\"\\nðŸ’» Code Example:\")\n",
    "\n",
    "results_with_repetitions = evaluate(\n",
    "    agent_a,\n",
    "    data=dataset_name,\n",
    "    evaluators=evaluators,\n",
    "    num_repetitions=3,  # Run each example 3 times\n",
    "    experiment_prefix=\"tutorial-with-repetitions\"\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nðŸ“Š Benefits:\")\n",
    "print(\"   âœ“ Understand performance variance\")\n",
    "print(\"   âœ“ Detect non-deterministic behavior\")\n",
    "print(\"   âœ“ Build confidence in metrics\")\n",
    "print(\"   âœ“ Identify edge cases\")\n",
    "\n",
    "print(\"\\nâš ï¸  Note: Repetitions increase evaluation time and cost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce1c8b2",
   "metadata": {},
   "source": [
    "## Section 11: Best Practices Summary\n",
    "\n",
    "### âœ… Do's\n",
    "\n",
    "1. **Tag experiments** with comprehensive metadata\n",
    "2. **Use multiple evaluators** for different aspects\n",
    "3. **Version your datasets** to track changes\n",
    "4. **Run baselines** before optimizing\n",
    "5. **Compare systematically** using A/B tests\n",
    "6. **Track tool usage** for agent evaluation\n",
    "7. **Use LLM-as-judge** for subjective metrics\n",
    "\n",
    "### âŒ Don'ts\n",
    "\n",
    "1. Don't evaluate without a baseline\n",
    "2. Don't use only one type of evaluator\n",
    "3. Don't ignore edge cases in datasets\n",
    "4. Don't skip metadata tracking\n",
    "5. Don't compare experiments on different datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca1538f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸŽ“ LANGSMITH EVALUATION - KEY TAKEAWAYS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(\"\\nâœ… What We Covered:\")\n",
    "print(\"   1. Dataset Creation: Building test collections\")\n",
    "print(\"   2. Multiple Evaluators: String match, LLM-judge, custom\")\n",
    "print(\"   3. Agent Variants: A/B testing different prompts\")\n",
    "print(\"   4. Tool Evaluation: Validating agent tool usage\")\n",
    "print(\"   5. Experiments: Systematic tracking and comparison\")\n",
    "print(\"   6. Best Practices: Versioning, metadata, repetitions\")\n",
    "\n",
    "print(\"\\nðŸ“Š LangSmith Evaluation Framework:\")\n",
    "print(\"   Traces â†’ Logs of runs (inputs, outputs, latencies)\")\n",
    "print(\"   Datasets â†’ Test examples for repeatable evaluation\")\n",
    "print(\"   Evaluators â†’ Scoring functions (metrics)\")\n",
    "print(\"   Experiments â†’ Tracked runs on datasets\")\n",
    "print(\"   Trajectory â†’ Sequence of agent actions\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"   1. Explore LangSmith UI for your experiments\")\n",
    "print(\"   2. Create domain-specific datasets\")\n",
    "print(\"   3. Build custom evaluators for your use case\")\n",
    "print(\"   4. Compare different models or prompts\")\n",
    "print(\"   5. Iterate based on evaluation insights\")\n",
    "\n",
    "print(\"\\nðŸ”— Resources:\")\n",
    "print(\"   - LangSmith UI: https://smith.langchain.com/\")\n",
    "print(\"   - LangSmith Docs: https://docs.smith.langchain.com/\")\n",
    "print(\"   - LangChain Evaluators: https://python.langchain.com/docs/guides/evaluation/\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"ðŸŽ‰ Tutorial Complete! Happy Evaluating!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ccb981",
   "metadata": {},
   "source": [
    "## Appendix: Quick Reference\n",
    "\n",
    "### Create Dataset\n",
    "```python\n",
    "dataset = client.create_dataset(\n",
    "    dataset_name=\"My Dataset\",\n",
    "    description=\"Description\"\n",
    ")\n",
    "\n",
    "client.create_examples(\n",
    "    inputs=[{\"question\": \"Q1\"}, {\"question\": \"Q2\"}],\n",
    "    outputs=[{\"answer\": \"A1\"}, {\"answer\": \"A2\"}],\n",
    "    dataset_id=dataset.id\n",
    ")\n",
    "```\n",
    "\n",
    "### Define Evaluator\n",
    "```python\n",
    "def my_evaluator(outputs: dict, reference_outputs: dict) -> dict:\n",
    "    score = 1 if outputs[\"answer\"] == reference_outputs[\"answer\"] else 0\n",
    "    return {\"key\": \"exact_match\", \"score\": score}\n",
    "```\n",
    "\n",
    "### Run Evaluation\n",
    "```python\n",
    "results = evaluate(\n",
    "    my_agent_function,\n",
    "    data=\"My Dataset\",\n",
    "    evaluators=[my_evaluator],\n",
    "    experiment_prefix=\"my-experiment\",\n",
    "    metadata={\"version\": \"1.0\"}\n",
    ")\n",
    "```\n",
    "\n",
    "### Compare in UI\n",
    "1. Go to https://smith.langchain.com/\n",
    "2. Select your project\n",
    "3. Click on experiments to compare\n",
    "4. View side-by-side results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
